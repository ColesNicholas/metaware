---
title             : "A meta-analysis of the effects of demand characteristics"
shorttitle        : "Demand characteristics meta-analysis"
author: 
  - name          : "Nicholas A. Coles"
    affiliation   : "1"
    corresponding : yes
    address       : "Cordura Hall, 210 Panama St, Stanford, CA 94305" 
    email         : "ncoles@stanford.edu"
  - name          : "Morgan H. Wyatt"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Center for the Study of Language and Information, Stanford University"
authornote: |
  All materials, data, and code are available at <insert OSF link>. 
abstract: |
  MARS guidelines: The problem of relation(s) under investigation; Study eligibility criteria; Type(s) of participants included in primary studies; Meta‐analysis methods (indicating whether a fixed‐effects or random‐effects model was used); Main results (including the more important effect sizes and any important moderators of these effect sizes); Conclusion (including limitations); Implications for theory, policy, and/or practice
  
keywords          : "demand characteristics, hypothesis awareness, placebo effect, research methods, meta-analysis"
wordcount         : "TBD"
bibliography      : "r-references.bib"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include = F}
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")

# identify refs
r_refs("r-references.bib")
```

Imagine that one day a mysterious person approaches you and begins telling you about a new method for understanding humans: Colesology.

The person explains that Colesology in a new approach for estimating causal relationships. However, they add that the method can sometimes be thrown off by a *methodological artifact.* You ask the Colesologist about this artifact. They explain that, sometimes, it causes researchers to detect an effect that's not real; other times, it causes researchers to miss an effect that *is* real. They add that sometimes the artifact causes estimates of relationships to be biased upward; other times, it causes the estimates to be biased downward. And, in general, they explain, it means that the things researchers observe using Colesology don't necessarily capture real-world human behavior.

At this point, you might become skeptical and ask the Colesologist, "How does this methodological artifact work?" Their response? They don't know, because sometimes the artifact seems to matter and other times it doesn't.

If this scenario were real, you would reasonably question whether Colesology is a valid method of scientific inquiry. But here's the thing: we're not describing Colesology, we're describing experimental psychology.

## Demand characteristics as a methodological artifact

In 1962, Martin Orne published a seminal paper highlighting a view that challenged deeply-ingrained beliefs about experimental psychology. Orne argued that research participants are not passive responders to the experimental context. Instead, he suggested that participants actively try to make sense of the situation based on their assumptions, beliefs, and motivations. One factor that Orne believed played a particularly powerful role was *demand characteristics*: "cues which convey an experimental hypothesis to the subject" (p. 779). This idea was met with some controversy, as not everybody agreed about the importance of these demand characteristics [e.g., @berkowitz1971weapons; @milgram1972interpreting; @kruglanski1975human]. Nonetheless, over the next 60 years, demand characteristics become recognized as a literal textbook methodological concern in experimental psychology [@sharpe2016frightened].

Orne initially focused on evidence that demand characteristics can lead to false positives---such as patients exhibiting sham symptoms of hypnosis [@orne1959nature]. Follow-up research, though, indicated that demand characteristics can also lead to false negatives. For example, @hayes1967two demonstrated that participants will ignore visual cues of depth when they believe that doing so is the purpose of the experiment. In addition to creating inferential errors, demand characteristics can also bias estimates of causal relationships. For example, @coles2022fact found that the estimated effect of facial poses on self-reported emotion could be amplified *or* attenuated based on the communicated purpose of the study. Yet, it's still not clear when demand characteristics matter. For example, in large replications of classic studies in behavioral economics, @mummolo2019demand consistently failed to find that manipulations of their communicated hypothesis impacted participants' responses.

After over 60 years, experimental psychologists are left with an uncomfortable truth: demand characteristics are a literal textbook methodological concern---but it is not clear when and how their effects emerge. The goal of the current paper is to use meta-analysis to take stock of what we know (and don't know) about demand characteristics.

We first provide an overview of a framework designed to accommodate the potentially heterogeneous effects of demand characteristics [@rosnow1997people]. We also review a complementary framework that attempts to bridge the gap between research on demand characteristics and research on placebo effects [@coles2022fact]. We then use meta-analysis to conduct the first quantitative synthesis of strict experimental tests of the effects of demand characteristics. Through this meta-analysis, we not only estimate the overall impact of demand characteristics, but also use moderator analyses to provide preliminary tests of predictions made by @rosnow1997people and @coles2022fact. We end with a discussion of the steps we believe are required to transform vague frameworks about demand characteristics into formal theories---theories we believe might help distinguish a valid experimental psychology from the invalid methods of Colesology.

## Rosnow and Rosenthal's (1997) demand characteristics framework

@rosnow1997people proposed that there are three key moderators of the effects of demand characteristic: (1) receptivity to cues, (2) motivation to provide hypothesis-consistent responses, and (3) opportunity to alter their responses (Figure \@ref(fig:framework)).

```{r framework, fig.cap = "Rosnow and Rosenthal’s (1997) and Coles et al.’s (2022) frameworks for conceptualizing the impact of demand characteristics on participants’ responses."}
knitr::include_graphics("images/metaware_framework.png")
```

### Receptivity to the cues

@rosnow1997people argued that participants must be perceptive to demand characteristics in order for them to impact downstream responses [see also @rosnow1973mediation; @strohmetz2008research]. As an extreme example, imagine that a researcher hands an infant participant a sheet of paper that precisely explains the researcher's hypothesis. Demand characteristics are certainly present---but they are not predicted to have an impact because the infant is not receptive to the cues (i.e., cannot read).

### Motivation to provide hypothesis-consistent responses

Early in the history of research on demand characteristics, researchers debated which motivational forces typically underlie the effects of demand characteristics [for a review, see @weber1972subject]. @orne1962social originally characterized participants as "good subjects" who change their responses because they are altruistically motivated to help the researcher confirm their hypothesis. Others characterized participants as "apprehensive subjects" who are motivated to respond in a manner that will lead them to be evaluated positively [@riecken1962program; @rosenberg1969conditions; @sigall1970cooperative]. @masling1966role argued that participants sometimes interfere with the purpose of the study ["negativistic subjects", see also @cook1970demand], whereas @fillenbaun1970more argued that participants attempt to respond as naturally as possible ("faithful subjects"). Although seemingly divided, these early theorists actually agreed on one overarching principle: that participants' motivation to provide hypothesis-consistent responses is a key driver of the effects of demand characteristics.

Because early demand characteristic theorists often focused on a single predominant subject goal--such as the goal to help the experimenter, be evaluated positively, or respond faithfully--less attention was paid to the notion that participants may have multiple, sometimes competing motivations [@barbuto1998motivation; @boudreaux2013goal]. Indeed, when the idea of multiple motivations was explored, it was often done so to highlight the more prominent role of a single goal [e.g., evaluation apprehension vs. motivation to help the experimenter, @sigall1970cooperative]. However, @rosnow1997people found that people have multiple goals in mind when they conceptualize their role as research participants. Participants describe their role as being similar to situations where one is being altruistic (e.g., giving to charity), being evaluated (e.g., being interviewed for a job), *and* obeying authority (e.g., obeying a no-smoking sign). All these goals may impact the extent to which participants are motivated to provide hypothesis-consistent responses. Furthermore, these goals can sometimes conflict. For example, imagine that an experimenter is friendly towards the participant--and that the participant is thus motivated to help the experimenter. Now imagine that the participant learns that the experimenter hypothesizes that they will show a race-based preference for job applicants. In this scenario, the motivation to help the experimenter may conflict with the participant's desire to respond in a socially desirable manner.

Based on the above observations and reasoning, @rosnow1997people suggested that participants can be characterized as being motivated to either (a) non-acquiesce (i.e., not change their responses), (b) acquiesce (i.e., provide hypothesis-consistent responses), or (c) counter-acquiesce (i.e., provide hypothesis-inconsistent responses). Of course, as we later discuss, motivation can also be conceptualized on a continuum ranging from highly motivated counter-acquiesce to highly motivated to acquiesce.

### Opportunity to alter responses

No matter how motivated they are to confirm the hypothesis, @rosnow1997people suggested that there is variability in the extent to which participants have the opportunity to alter the outcome-of-interest. Thus, they posited that demand characteristics can only impact outcomes that participants can readily alter.

To summarize, @rosnow1997people posited that demand characteristics only bias participants responses when they (1) notice the cues, (2) are motivated to adjust their responses, and (3) are capable of adjusting their responses.

## Coles et al.'s (2022) framework

Researchers have generally conceptualized the effects of demand characteristics on participants' responses as a *response bias* [@orne1962social; @rosnow1973mediation; @strohmetz2008research]. For example, demand characteristics that indicate the researcher expects an intervention to boost mood is *not* posited to impact participants' actual mood; Instead, the demand characteristics are posited to merely impact participants' mood *reports.*

@coles2022fact argued that demand characteristics not only have the potential to lead to response biases--but also placebo biases (Figure \@ref(fig:framework)). They defined (a) response biases as changes mediated by relatively deliberate changes that participants make to their responses, and (b) placebo effects as changes that are mediated by relatively automatic activation of beliefs or pre-existing conditioned responses [@zion2018mindsets]. Thus, unlike @rosnow1997people, @coles2022fact argued that demand characteristics can impact responses even when participants have neither the motivation nor opportunity to adjust their responses. Preliminary evidence for this assertion comes from @coles2022fact observation that participants' beliefs did not always match the demand characteristics manipulation. For example, some participants disclosed that they (a) did not personally believe that posed expressions impacted emotion, but (b) recognized that the experimenter did. In their studies, both the manipulations of demand characteristics and measures of participants' beliefs independently moderated the effects of posed expressions on emotion---providing preliminary evidence of distinct psychological mechanisms.

# Methodology

The present meta-analysis was designed to (a) provide the first quantitative synthesis of strict experimental tests of demand effects, and (b) test predictions made by @rosnow1997people and @coles2022fact.

We defined the scope of the meta-analysis using the Population, Intervention, Comparison, Outcome framework [@schardt2007utilization]. Our population-of-interest was human subjects participating in non-clinical research studies. We excluded clinical research studies so that we could focus on research that better isolated the mechanism most often discussed in the demand characteristics literature: response biases (as opposed to placebo effects). Given that there is a sizable literature on placebo effects, excluding clinical tests of demand characteristics also helped us improve the feasibility of the project.

The intervention-of-interest was explicit manipulations of the hypothesis communicated to participants--i.e., scenarios where a researcher tells participants about the effect of an independent variable on a dependent variable. We focused on this intervention because it provides a relatively overt test of the impact of demand characteristics.

Our comparison-of-interest were conditions where either no hypothesis or a different hypothesis was communicated to participants. Our outcome-of-interest was the dependent variable described in the communicated hypothesis. For example, in a study that manipulated whether the intervention is described as "mood-boosting" or "mood-dampening", the outcome-of-interest would be any measure of mood.

## Literature search

```{r literature search, include = F}
# open and process literature search data
DF.s <- 
  # open data
  read_xlsx(path = "data/metaware_EsData_raw.xlsx",
            sheet = "records.screening") %>% 
  
  # identify unpublished dissertations by identifying links that contain the word 'dissertation'
  mutate(dissertation = if_else(grepl("dissertation", link),
                                1,
                                0)
         )

# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>% 
  filter(!is.na(Database)) %>% 
  nrow()

# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>% 
  filter(dissertation == 1) %>% 
  nrow()
```

Our literature search strategy was developed in consultation with a librarian at Stanford University. Given the broad nature of the demand characteristics construct, we determined that a truly comprehensive strategy was infeasible (see Limitations section). Thus, we sought to design a strategy that best balanced comprehensiveness and feasibility.

We searched APA PsycInfo using broad search terms: "demand characteristics" OR "hypothesis awareness". This yielded `r r.pi` records. We additionally released a call for unpublished studies on the Society for Personality and Social Psychology Open Forum; Twitter; Facebook Psychological Methods Discussion Group and PsychMAP groups. This yielded `r nrow(DF.s) - r.pi` additional records. In total, `r r.unp` of the records were unpublished.

## Screening

```{r final.df, include = F}
# open effect size data
DF.es <- 
  read_csv(file = "data/metaware_data_clean.csv")

# specify number of studies (denoted by id column)
num.s <- DF.es$id %>% 
  unique() %>% 
  length()

# specify number of papers (denoted by name column)
num.p <- DF.es$name %>% 
  unique() %>% 
  length()

# identify outlier es
outlier.es <- DF.es %>% 
  filter(id == 18) %>% 
  summarise(min.es = min(es)) %>% 
  round(2)
```

To be eligible for inclusion in the meta-analysis, the following criteria must have been met:

-   The researcher manipulated what participants were told about the effect of an independent variable on a dependent variable. This included both *positive demand* (participants told that the dependent variable will increase), *negative demand* (participants told that the dependent variable will decrease) and *nil demand* (participants told the dependent variable will be unaffected) conditions.

    We excluded scenarios where the researcher described a non-nil effect that was *non-directional*. We did so because participants in these scenarios could not readily infer how to adjust their responses. For example, if participants were told that an independent variable would "impact mood", it is not clear if participants should infer that the mood will be boosted (positive demand) or dampened (negative demand).

-   The demand characteristics manipulation was not strongly confounded. For example, a study by @sigall1970cooperative was excluded because the manipulation of the stated hypothesis was confounded with a disclosure about the meaning of the behavior. Specifically, participants were either informed or not informed that the researcher expected them to copy a large quantity of numbers. When participants were informed about this hypothesis, they were also told that such behavior would be indicative of an undesirable personality trait.

-   Information necessary for computing at least one effect size was included.

N. C. and M. W. screened records independently, reviewed potentially relevant records together, and coded the information for moderator analyses and effect size computations. Disagreements were resolved through discussion. It total, `r num.s` studies from `r num.p` records were eligible for inclusion. However, one record [@allen2012demand] was removed because the information reported led to implausibly large effect size estimates (e.g., $d$ = `r outlier.es`).

```{r clean.env.1, include = F}
# remove outlier
DF.es <- DF.es %>% 
  filter(id != 18)

# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
```

## Effect size index

We used standardized mean difference scores (Cohen's $d_{s}$ and $d_{rm}$) as our effect size index [@borenstein2009effect; @cohen1988statistical].

In some scenarios, we estimated the main effect of demand characteristics. For example, @coles2022fact manipulated whether participants were told that smiling would increase happiness. Here, the main effect of demand characteristics can be computed by comparing happiness ratings from smiling participants who were either informed or not informed about its expected effect.

In other scenarios, we estimated the interactive effect of demand characteristics. For example, in the same @coles2022fact study, participants reported happiness both after smiling and scowling. Participants' mood generally improved when smiling vs. scowling (i.e., there was a main effect of facial pose). However, the difference was more pronounced when participants were told about the mood-boosting effects of smiling. In other words, there was an interaction between facial pose and demand characteristics. In this scenario, the interactive effect of demand characteristics was computed by calculating a difference-in-differences score.

Effect sizes were calculated so that positive values indicated an effect consistent with the demand characteristics manipulation. For example, if participants were told that an intervention should increase mood, an increase in mood would be coded as a positive effect. If participants were told that an intervention should decrease mood, an increase in mood would be coded as a negative effect.

```{r corr.sens, include = F}
# examine how assumed repeated measures correlation impacts general pattern of results

# create list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")

# (1) open dataframe, (2) compute intercept-only model
sens.res <- 
  sapply(X = sens.df.list, 
         FUN = function(i){
           # open data
           DF.es <- read.csv(paste0("data/r_sensitivity/",
                                    i)
                             ) 
           # fit model
           mod <- rma.uni(yi = es,
                          vi = es.var,
                          data = DF.es,
                          method = "REML") %>% 
             robust(cluster = DF.es$id)
           
           # return overall es as a number
           mod$b %>% 
             as.numeric() %>% 
             return()
           }
         )

# compute range of es values
sens.range <- max(sens.res) - min(sens.res)

# delete vestigial
rm(sens.df.list, sens.res)

```

For repeated-measure comparisons, the correlation between the repeated measures is needed to calculate Cohen's $d_{rm}$. This correlation is rarely reported, so we followed a recommendation by @borenstein2009effect and performed sensitivity analyses on an assumed correlation. We preregistered a default correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses only produced a `r sens.range` range in overall effect size estimates---so we do not discuss them further.

```{r mult.eff, include = F}
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>% 
  # identify number of effect sizes for each study (id)
  group_by(id) %>% 
  count() %>% 
  
  # code whether each study has more than one effect size
  mutate(dep = if_else(n > 1,
                       true = 1,
                       false = 0)
         ) %>% 
  
  # calculate proportion of studies with more than one effect size 
  ungroup() %>% 
  summarise(mult.eff = mean(dep)) %>% 
  
  # export as numeric and turn into percentage
  as.numeric() %>% 
  round(digits = 2) * 100
```

`r mult.eff.per` of studies contained multiple effect sizes of interest. For example, Coles et al. (2022) had a positive demand, nil demand, and control condition. Participants also completed several facial expression poses (happy, angry, and neutral) and self-reported several emotions (happiness and anger). To be comprehensive, we recorded all reported effect sizes and account for dependencies in our models (described later).

## Types of demand characteristic comparisons

Cohen's $d$ represents a standardized mean difference between two groups. Often, this comparison involved a single demand characteristic condition (positive, negative, or nil demand) compared to a control group. Sometimes, however, this comparison involved two demand characteristic comparisons (e.g., positive demand vs. negative demand). We thus coded whether the comparison involved one vs. two types of demand characteristics. In addition, we coded each type of comparison: positive vs. control, nil vs. control, negative vs. control, positive vs. nil, positive vs. negative, nil vs. negative demand.

## Post-hoc measures of motivation, opportunity, and belief

```{r vig.desc, include = F}
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>% 
  nrow()
```

Both @rosnow1997people and @coles2022fact posited that the effects of demand characteristics are moderated by participants' (1) motivation to provide hypothesis-consistent responses and (2) opportunity to adjust their responses (Figure \@ref(fig:framework)). @coles2022fact additionally predicted a third moderator: (3) participants' belief in the hypothesized effect. Unfortunately, these variables were rarely measured in the studies included in the meta-analysis.

As an indirect measure of these three moderators-of-interest, we estimated their values through a new set of participants. (See SI for construct validity analyses.) For each demand characteristic condition and dependent variable combination, we created vignettes that described key study details. For example, @standing2008demonstration had two demand characteristics manipulations (positive demand and nil demand) and two dependent variables (measures of verbal and spatial reasoning). Thus, we created four vignettes for this study (see Figure \@ref(fig:vig)).

In total, there were `r vig.n` vignettes. We did not create vignettes for control conditions because participants were not given information about the experimenter's hypothesis. Because there were no explicit demand characteristics to act upon, we left motivation, belief, and opportunity values blank for this condition.

```{r vig, fig.cap = "Vignettes for Standing et al., 2008."}
knitr::include_graphics("images/metaware_vigs.png")
```

```{r survey.n, include = F}
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>% 
  nrow()
```

`r survey.n` undergraduates from Stanford University reviewed 20 randomly-selected vignettes in exchange for course credit. For each vignette, raters were asked to first identify the researcher's hypothesis. Here, participants chose between four options that described a filler effect (usually involving a different dependent variable) or a positive, negative, or nil effect of the independent variable on the dependent variable. Afterwards, they rated the extent to which they would (1) be motivated to provide hypothesis-consistent responses (-3 = "extremely motivated to adjust responses to be inconsistent" to 3 = "extremely motivated to adjust responses to be consistent"), (2) be able to adjust their responses on the outcome-of-interest (0 = "extremely incapable" to 7 = "extremely capable), and (3) believe the experimenter's hypothesis (0 ="strong disbelief" to 7 = "strong belief"). (Raters also indicated whether they believed the actual participants would confirm the hypothesis, which we discuss later.) These questions were presented in random order.

For each vignette, ratings were removed if the rater did not correctly identify the communicated hypothesis. The remaining scores were averaged across raters to provide mean estimates of motivation, opportunity, and belief.

### Accounting for different demand comparisons

```{r mods, fig.cap = "Hypothetical data from a study where a procedure is either described as mood-boosting (positive demand), described as mood-dampening (negative demand), or not described at all (control). Data provides examples of how the effects of demand characteristics (d) on self-reported mood are moderating by participants' reports of their motivation to confirm the stated hypothesis (m, Panel A), belief in the stated hypothesis (b, Panel B), and opportunity to adjust responses (c, Panel C). In each panel, separate examples are provided for scenarios where motivation is invariant (Column 1) and variant (Column 2) across levels of demand characteristics"}
knitr::include_graphics("images/metaware_mods.png")
```

Cohen's $d$ represents the standardized difference between *two* groups. Thus, for each effect size, we summed the motivation, opportunity, and belief scores for the two groups being compared. Doing so allowed us to accommodate the fact that some comparisons involved two demand characteristics conditions. For example, imagine a study where a procedure is either described as mood-boosting (positive demand), described as mood-dampening (negative demand), or not described at all (control). Compared to a control condition, participants who are motivated to confirm the hypothesis will have upward-biased responses in the positive demand condition and downward-biased responses in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 1). When comparing the two demand conditions, the size of the demand effect should be doubled because the motivational forces in the two conditions produce an additive effect. Similarly, these motivational forces could hypothetically cancel each other out. This would happen if participants were (a) motivated to confirm the hypothesis in the positive demand condition, and (b) motivated to disconfirm the hypothesis in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 2). We used a similar approach for belief (Panel B) and opportunity scores (Figure \@ref(fig:mods), Panel C).

We did not include nil-hypothesis comparisons in our analyses because our coding strategy could not accommodate the potential moderating role of motivation and belief in this condition. For example, imagine that a participant is (a) told that an intervention will not impact mood (nil demand), and (b) is extremely motivated to disconfirm the hypothesis. Relative to a control condition, this participant could disconfirm the hypothesis by either increasing *or* decreasing their mood report. Nonetheless, we discuss potential strategies in the Limitations sections for addressing this question in future research.

## Rater forecasts of demand effects

Even if researchers cannot explain how demand characteristics work, it might be valuable to be able to predict their effects [@yarkoni2017choosing]. Orne suggested that one group that may be particularly good at predicting these effects is participants themselves [@orne1969demand]. To examine this, raters who reviewed the vignettes also predicted whether other participants would confirm vs. disconfirm the researcher's hypothesis (-3 = "extremely likely to adjust responses to be inconsistent" to 3 = "extremely likely to adjust responses to be consistent"). We processed these data using the same approach as the motivation, opportunity, and belief scores (e.g., summed the scores when comparing two demand conditions).

## Other moderators

We also coded several moderators for which we had no a-priori hypotheses. This included: (1) whether the study was published vs. unpublished, (2) whether the sample was student or non-student (e.g., MTurk), (3) whether participants were paid or unpaid (4) whether the study was conducted online or in-person , and (5) whether demand characteristics were manipulated within- vs. between-subjects.

## **Meta-analytic approach**

For estimating overall effects and moderators, we used random-effects meta-analysis with robust variance estimates [@hedges2010robust]. We included random-effects because @rosnow1997people and @coles2022fact posited that moderators create a distribution of true effects---as opposed to a single fixed-effect [@hedges1998fixed]. We used robust variance estimates because `r mult.eff.per`% of studies provided multiple effect sizes of interest, which violates the statistical assumption that effect sizes are independent. Like many meta-analysis procedures, meta-analysis with robust variance estimates uses an inverse-variance weighting scheme that adjusts for dependencies among effect sizes. For the present work, we used a hierarchical effects weighting scheme [@tanner2014robust].

To estimate the overall effect size, we fit an intercept-only model. The intercept of this model can be interpreted as the precision-weighted overall effect size, adjusted for correlated-effect dependencies. We used the same approach to estimate subgroup overall effect sizes by subsetting the data by each level of each moderator. For moderator analyses, continuous and dummy-coded categorical moderators were separately entered into the meta-regression model.

```{r clean.env.2}
# delete vestigial
rm(mult.eff.per, vig.n, survey.n)
```

### **Publication bias analyses**

The most common way to assess publication bias with dependent structures is to aggregate the dependent effect sizes and perform standard publication bias analyses. Following this approach, we used the MAd R package to aggregate dependent effect sizes. We then used precision effect tests [i.e., PET-PEESE, @stanley2014meta] and weight-function modeling [@vevea1995general] to test for the presence of publication bias and estimate the bias-corrected overall effect size. As a further sensitivity analysis, we used an approach by @mathur2020sensitivity to estimate the maximal publication bias required to shift the observed overall effect size estimate to zero.

# Results

```{r}
# estimate overall effect size
overall <-
  rma.uni(yi = es,
          vi = es.var,
          data = DF.es,
          method = "REML") %>% 
  robust(cluster = DF.es$id)
```

Results indicated that, overall, explicit manipulations of demand characteristics create a small-to-medium acquiescence effect, $d$ = `r overall$b` , 95% CI [`r overall$ci.lb`, `r overall$ci.ub`], $t$ = `r overall$zval`, $p$ `r printp(overall$pval)`. In other words, when explicitly told a hypothesis, participants responses tend to shift in a manner consistent with that hypothesis.

That being said, the observed effects of demand characteristics were highly heterogeneous. The standard deviation of the distribution of effects $\tau$ = `r sqrt(overall$tau2)` was larger than the overall effect---and `r overall$I2`% of variability appeared to be driven by heterogeneity (i.e., $I^2$ = `r overall$I2`).

### Moderator analyses

#### Motivation, opportunity, and belief

Run these without the z demand comparisons

#### Other moderators

### Forecasts

### Publication bias analyses

# Discussion

# Thoughts

Compare average effect of demand to average effect in psychology. It suggest that it's plausible that these are driven by demand. When looking at the distribution, it's clear that no effect is too big to rule out concerns about demand.

Might want to plot es distribution and identify proportion of (a) non-negligible acquiesence, (b) non-negligible counter-acquiesence, and (c) non-acquiesence.

# To-do

[] Clean up folder structure

[] Have Mike review again?

[] Update pre-registration

[] Have M.W. add refs

[] Have M.W. build moderator table

[] Have M.W. build codebooks

# References

::: {#refs custom-style="Bibliography"}
:::
