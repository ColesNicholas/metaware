---
title             : "NEED TYPE 3 SS A meta-analysis on demand characteristics"
shorttitle        : "Demand characteristics meta-analysis"
author: 
  - name          : "Nicholas A. Coles"
    affiliation   : "1"
    corresponding : yes
    address       : "Cordura Hall, 210 Panama St, Stanford, CA 94305" 
    email         : "ncoles@stanford.edu"
  - name          : "Michael C. Frank"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Center for the Study of Language and Information, Stanford University"
authornote: |
  All materials, data, and code are available at <insert OSF link>. 
abstract: |
  Demand characteristics are a fundamental methodological concern in experimental psychology. Yet, little is known about the direction, magnitude, consistency, and mechanisms underlying their effects. In the first quantitative synthesis on the topic, we conducted a three-level meta-analysis of 195 effect sizes from 40 studies that provided strict experimental tests of demand effects by manipulating the hypothesis communicated to participants. Results indicated that these demand characteristics tend to produce small increases in hypothesis-consistent responding. However, these effects were extremely heterogeneous. The estimated distribution of effects ranged from d = 1.91 (a massive increase in hypothesis-consistent responding) to d = -1.67 (a massive increases in hypothesis-*in*consistent responding). This range covers the span of almost every conceivable effect in experimental psychology. Contrary to conventional views, we found little evidence that demand effects were driven by participants’ motivation and opportunity to adjust their responses---i.e., a response bias. Instead, such effects seemed to be more so driven by participants’ beliefs---i.e., placebo effects. Similar findings emerged in a direct replication of a study included in the meta-analysis. Taken together, results challenge conventional distinctions between demand characteristics and placebo effects. More importantly, they highlight a pressing need to understand the mysterious but potentially massive impact of demand characteristics.
  
keywords          : "demand characteristics, hypothesis awareness, placebo effect, research methods, meta-analysis"
wordcount         : "TBD"
bibliography      : "r-references.bib"
floatsintext      : no
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include = F}
# load packages writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")

# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")

# load mixed-effect regression packages
library(lme4)
library(lmerTest)
library(emmeans)

# identify refs
r_refs("r-references.bib")

# turn scientific notation off
options(scipen = 999)

# set theme
theme_set(theme_classic())
```

Imagine that one day a mysterious person approaches you and begins telling you about a new method for understanding humans: Crankology[^1]. The person explains that Crankology is useful for estimating causal relationships---but adds that it can sometimes be thrown off by a *methodological artifact.* When you ask the Crankologist about this artifact, they explain that it sometimes causes researchers to detect an effect that's not real, and other times causes them to miss an effect that is real. They add that it sometimes causes relationships to be biased upward and other times causes them to be biased downward. And then they offer a confession: they don't understand how the artifact works. Sometimes the artifact seems to matter, other times it doesn't---and its underlying mechanism is poorly understood.

[^1]: "Crankology" is a portmanteau. It is a combination of the authors' last names and the name of the discipline they study: psychology.

If this scenario were real, you would reasonably question whether Crankology is a valid method of scientific inquiry. However, perhaps we should not be so quick to judge. Because, like Crankology, experimental psychologists deal with a difficult-to-understand methodological artifact: *demand characteristics*.

## Demand characteristics as a methodological artifact

In 1962, Martin Orne published a seminal paper highlighting a view that challenged deeply-ingrained beliefs about the role of human subjects in experiments. Contrary to popular views at the time, Orne argued that research participants are not passive responders to the experimental context. Instead, he suggested that participants are perceptive to demand characteristics---"cues which convey an experimental hypothesis to the subject"---and are motivated to use these cues to help the experimenter confirm their hypothesis [@orne1962social, p. 779]. This idea was controversial at first, with some researchers suggesting that the concern was vague and/or overblown [e.g., @berkowitz1971weapons; @milgram1972interpreting; @kruglanski1975human]. Nonetheless, over the next 60 years, demand characteristics would become recognized as a literal textbook methodological concern in experimental psychology [@sharpe2016frightened].

Orne initially focused on evidence that demand characteristics can lead to false positives---such as patients exhibiting sham symptoms of hypnosis [@orne1959nature]. Follow-up research, though, indicated that demand characteristics can also lead to false negatives. For example, @hayes1967two demonstrated that participants will ignore visual cues of depth when they believe that doing so is the purpose of the experiment. Of course, in addition to creating inferential errors, demand characteristics can bias estimates of causal relationships. For example, @coles2022fact found that the estimated effect of facial poses on self-reported emotion could be amplified *or* attenuated depending on whether the experimenter communicates expectations of positive or nil effects. However, not all researchers have found that demand characteristics matter. For example, in large replications of classic studies in behavioral economics, @mummolo2019demand consistently failed to find that manipulations of the communicated hypothesis impacted participants' responses.

After over 60 years, experimental psychologists are left with an uncomfortable state of affairs. Demand characteristics are a literal textbook methodological concern. However, like Crankology, the magnitude, direction, consistency, and mechanisms underlying this artifact remain mysterious.

## How do demand characteristics bias participant responses?

Historically, theorists have conceptualized the effects of demand characteristics as *response biases* mediated by relatively deliberate changes that participants make to their responses [@orne1962social; @rosnow1973mediation; @strohmetz2008research]. In doing so, these theorists distinguished their ideas from conceptually similar work on *placebo effects*: changes in participants' responses that are mediated by the relatively automatic activation of beliefs and/or conditioned responses [@zion2018mindsets]. As an example of this distinction, imagine that a participant knows that a researcher expects an intervention to boost mood. Response bias---the historical focus of the demand characteristics literature---would involve a change in participants' self-reported mood without a concomitant change in actual mood. Placebo effects, on the other hand, would entail an actual change in mood.

As we review below, the most comprehensive demand characteristics framework follows this historic trend, conceptualizing the artifact as a response bias [@rosnow1997people]. However, this conceptualization was recently challenged by @coles2022fact and @corneille2022sixty, who argued that demand characteristics can lead to both response biases *and* placebo effects (Figure \@ref(fig:framework)). For example, after inferring that a researcher expects an intervention to boost mood, a participant may both (a) deliberately adjust their mood ratings (a response bias), and (b) unintentionally experience a placebo-induced change in mood. We discuss these two mechanisms in greater detail below.

### Response bias

To date, the most influential framework for conceptualizing the effects of demand characteristics has been developed by @rosnow1997people. Like most researchers, @rosnow1997people suggested that demand characteristics produce response biases. As such, they proposed three key moderators: (1) receptivity to cues, (2) motivation to provide hypothesis-consistent responses, and (3) opportunity to alter responses.

```{r framework, fig.cap = "Rosnow and Rosenthal’s (1997) and Coles, Gaertner, et al.’s (2022) frameworks for conceptualizing how demand characteristics can lead to increases (green), decreases (red), or no shift (light grey) in hypothesis-consistent responding. Rosnow and Rosenthal conceptualized demand effects as response biases moderated by receptivity to cues (not pictured), motivation, and opportunity to adjust responses. Coles, Gaertner, et al. proposed that demand characteristics can also produce placebo biases (dotted boxes) that are driven by the activation of or change in participants’ beliefs."}
knitr::include_graphics("images/metaware_framework.png")
```

#### Receptivity to cues

To start, @rosnow1997people reasoned that participants must be perceptive to demand characteristics in order for there to be a response bias [see also @rosnow1973mediation; @strohmetz2008research]. As an extreme example, imagine that a researcher hands an infant participant a sheet of paper that precisely explains the study hypothesis. Demand characteristics are certainly present, but they are not predicted to have an impact because the infant is not receptive to the cues (i.e., cannot read). In the present work, we will focus on scenarios where participants are likely to be highly receptive to cues. Thus, we will pay less attention to receptivity as a moderator (but see the General Discussion for more information).

#### Motivation to provide hypothesis-consistent responses

Early in the history of research on demand characteristics, researchers debated which motivational forces typically underlie its subsequent response bias [for a review, see @weber1972subject]. @orne1962social originally characterized participants as "good subjects" who change their responses because they are motivated to help the researcher confirm their hypothesis. Others characterized participants as "apprehensive subjects" who are motivated to respond in a manner that will lead them to be evaluated positively [@riecken1962program; @rosenberg1969conditions; @sigall1970cooperative]. @masling1966role argued that participants sometimes interfere with the purpose of the study ["negativistic subjects", see also @cook1970demand], whereas @fillenbaun1970more argued that participants attempt to follow directions as closely as possible ("faithful subjects"). Although seemingly divided, these early theorists agreed on one overarching principle: response bias is driven by participants' motivation (or lack thereof) to provide hypothesis-consistent responses.

In the most prolific era of demand characteristics research, investigators sought to understand which subject goal *predominately* mediated response bias. For example, @sigall1970cooperative found that participants increased performance on a simple task when the experimenter indicated that this was their expectation. However, participants did *not* do so when told that increased performance would be indicative of an obsessive-compulsive personality. Based on these results, @sigall1970cooperative concluded that participants were predominately motivated to secure a positive evaluation---not help the experimenter confirm their hypothesis.

By focusing on testing hypotheses about a single predominate participant goal, less attention was initially paid to the notion that participants might have multiple, sometimes competing motivations [@barbuto1998motivation; @boudreaux2013goal]. Later, though, @rosnow1997people demonstrated that people have *multiple* goals in mind when they conceptualize their role as research participants. Participants describe their role as being similar to situations where one is being altruistic (e.g., giving to charity), being evaluated (e.g., being interviewed for a job), *and* obeying authority (e.g., obeying a no-smoking sign). All these goals may impact the extent to which participants are overall motivated to provide hypothesis-consistent responses. Furthermore, these goals can sometimes conflict. For example, in the @sigall1970cooperative experiment, participants may have been motivated to both (a) secure a positive evaluation, and (b) [presumably to a smaller degree] help the experimenter confirm their hypothesis. The brilliance of Rosnow and Rosenthal's proposal is that it acknowledged that all previous researchers were [at least somewhat] correct; participants are altruistic, apprehensive, negativistic, *and* faithful. This effectively ended debates about which motivational force predominately drove response biases. 

Synthesizing the above observations and reasoning, @rosnow1997people suggested that participants can be characterized as being overall motivated to either (a) non-acquiesce (i.e., not change their responses based on knowledge about the hypothesis), (b) acquiesce (i.e., provide hypothesis-consistent responses), or (c) counter-acquiesce (i.e., provide hypothesis-inconsistent responses). Of course, as we later discuss, motivation can also be conceptualized on a continuum ranging from highly motivated to counter-acquiesce to highly motivated to acquiesce.

#### Opportunity to alter responses

No matter how motivated they are to confirm the hypothesis, @rosnow1997people reasoned that there is variability in the extent to which participants have the opportunity/ability to alter the outcome-of-interest. Taking this third moderator into account, Rosnow and Rosenthal concluded that demand characteristics only produce response biases when participants (1) notice the cues, (2) are motivated to adjust their responses, and (3) can adjust their responses.  This framework directly maps onto psychologists’ playbook for avoiding the impact of demand characteristics: use deception (reduce receptivity), incentivize honest reporting (reduce motivation), and/or deploy difficult-to-control outcome measures (reduce opportunity to adjust responses).

### Response bias and placebo effects

Over the past half century, demand characteristics have generally been conceptually divorced from placebo effects [e.g., @orne1969demand]. Indeed, in the classic book describing artifacts and behavioral research [@rosnow1997people], placebo effects are acknowledged as a historical precursor to research on methodological artifacts but not discussed in the context of demand characteristics. This conceptual separation, however, has recently been challenged by @coles2022fact and @corneille2022sixty, who argued that demand characteristics not only have the potential to lead to response biases, but also placebo effects (Figure \@ref(fig:framework)). Consistent with this reasoning, @coles2022fact found that participants' beliefs did not always match the hypothesis communicated to participants; furthermore, both the communicated hypothesis and measures of participants' beliefs moderated the effects of posed expressions on emotion. Contrary to @rosnow1997people, this work provides preliminary evidence that demand characteristics can produce both response biases and placebo effects. This means that demand characteristics can still bias responses when participants have neither the motivation nor the opportunity to adjust their responses---challenging the conventional playbook for avoiding the impact of this methodological artifact.

## Goals

The goal of the current paper is to take stock of what we know---and what we don't know---about demand characteristics as a methodological artifact. In Study 1a, we report a meta-analysis of strict experimental tests of the effects of demand characteristics, with a focus on the the direction, magnitude, and consistency of the effects. We then examine several study features (e.g., whether participants are paid) that researchers have specified as potential moderators.

In Study 1b, we review an extension of the meta-analysis that examines whether observed effect size variability can be explained by factors theorized to underlie response biases (i.e., motivation and opportunity to adjust responses) and placebo effects (i.e., belief in the experimenter's hypothesis). To do so, we derived estimates of these factors from a new set of participants. These participants read descriptions of each study in the meta-analysis and then reported the extent to which they hypothetically would have (a) been motivated to confirm the experimenter's hypothesis, (b) had the opportunity to adjust their responses, and (c) believed the experimenter's hypothesis. We also examined how well this new set of participants could predict the effects of the studies’ demand characteristic manipulations.

In Study 2, we review a small replication study that re-examines the extent to which demand effects are driven by response biases and placebo effects. In this replication study, we manipulated demand characteristics in an experimental investigation of the proposed effects of facial poses on emotional experience [@coles2019meta; @coles2022multi]. We then examined the extent to which the effect of facial poses was moderated by factors believed to underlie response biases (i.e., self-reported motivation and opportunity to adjust responses) and placebo effects (i.e., self-reported belief in facial feedback effects).

# Study 1a

Study 1a was designed to provide the first quantitative synthesis of strict experimental tests of demand effects, with a focus on their direction, magnitude, and consistency.

## Methodology

We defined the scope of the meta-analysis using the Population, Intervention, Comparison, Outcome framework [@schardt2007utilization]. Our population-of-interest was human subjects participating in non-clinical research studies. We excluded clinical research studies so that we could focus on research that better isolated the discipline (experimental psychology) and mechanism (response bias) most often discussed in the demand characteristics literature. Given that there is a sizable literature on placebo effects, excluding clinical research studies also helped us improve the feasibility of the meta-analysis.

The intervention-of-interest was explicit manipulations of the hypothesis communicated to participants---i.e., scenarios where a researcher tells participants about the effect of an independent variable on a dependent variable. @orne1962social more broadly defined demand characteristics as *any* cue that may impact participants' beliefs about the purpose of the study, including instructions, rumors, and experimenter behavior. However, such a definition creates a blurry and potentially boundless conceptual space where any systematic change in a research design might be considered a test of demand characteristics. Thus, to bound and simplify the conceptual space, we focused on explicit manipulations of the hypothesis communicated to participants.

Our comparison-of-interest were conditions where either no hypothesis or a different hypothesis was communicated to participants. Our outcome-of-interest was the dependent variable described in the communicated hypothesis. For example, in a study that manipulated whether the intervention is described as "mood-boosting" or "mood-dampening", the outcome-of-interest would be any measure of mood.

### Literature search

```{r literature search, include = F}
# open and process literature search data
DF.s <- 
  # open data
  read_xlsx(path = "data/metaware_EsData_raw.xlsx",
            sheet = "records.screening") %>% 
  
  # identify unpublished dissertations by identifying links that contain the word 'dissertation'
  mutate(dissertation = 
           if_else(condition = grepl("dissertation", link),
                   true = 1,
                   false = 0)
         )

# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>% 
  filter(!is.na(Database)) %>% 
  nrow()

# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>% 
  filter(dissertation == 1) %>% 
  nrow()
```

Our literature search strategy was developed in consultation with a librarian at Stanford University. Given the broad nature of the demand characteristics construct, we determined that a truly comprehensive strategy was not feasible. Thus, we sought to design a strategy that best balanced comprehensiveness and feasibility.

We searched APA PsycInfo using broad search terms: "demand characteristics" OR "hypothesis awareness". This yielded `r r.pi` records. We also released a call for unpublished studies on the Society for Personality and Social Psychology Open Forum; Twitter; the Facebook Psychological Methods Discussion group; and the Facebook PsychMAP group. This yielded `r nrow(DF.s) - r.pi` additional records. In total, `r r.unp` of the records were unpublished.

### Screening

```{r final.df, include = F}
# open clean effect size data
DF.es <- 
  read_csv(file = "data/metaware_data_clean.csv")

# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>% 
  unique() %>% 
  length()

# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>% 
  unique() %>% 
  length()

# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>% 
  filter(id.study == 18) %>% 
  summarise(max.es = min(es)) %>% #  using min because largest value is neg
  round(2)
```

To be eligible for inclusion in the meta-analysis, the following criteria must have been met:

-   The researcher manipulated what participants were told about the effect of an independent variable on a dependent variable. This included both *positive demand* (participants told that the dependent variable will increase), *negative demand* (participants told that the dependent variable will decrease) and *nil demand* (participants told the dependent variable will be unaffected) conditions. Often, this was compared to a *control* condition, where participants were not told about an effect of an independent variable on a dependent variable.

    We excluded conditions where the researcher communicated a *non-directional* effect. We did so because participants in these scenarios could not unambiguoursly infer how their responses were expected to change. For example, if participants were told that an independent variable would "impact mood", it is not clear if participants should infer that the mood will be boosted or dampened.

-   The demand characteristics manipulation was not strongly confounded. For example, we excluded a study by @sigall1970cooperative because the manipulation of the stated hypothesis was confounded with a disclosure about the meaning of the behavior (i.e., that confirming the hypothesis would be indicative of an obsessive-compulsive personality disorder).

-   Information necessary for computing at least one effect size was included.

N. C. and a research assistant screened records independently, reviewed potentially relevant records together, and worked together to code the information for moderator analyses and effect size computations. Disagreements and discrepancies were resolved through discussion. It total, `r num.s` studies from `r num.p` records were eligible for inclusion. However, one record [@allen2012demand] was removed because the information reported led to implausibly large effect size estimates (e.g., $d$ = `r outlier.es`).

```{r clean.env.1, include = F}
# remove outlier and re-initialize id factors
DF.es <- DF.es %>% 
  filter(id.study != 18) %>% 
  mutate(id.study = factor(id.study),
         id.es = factor(id.es))

# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
```

### Effect size index

We used standardized mean difference scores (Cohen's $d_{s}$ and $d_{rm}$) as our effect size index [@borenstein2009effect; @cohen1988statistical].

In most scenarios, we estimated the main effect of demand characteristics. For example, @coles2022fact manipulated whether participants were told that posing smiles would increase happiness. Here, the main effect of demand characteristics can be computed by comparing happiness ratings from smiling participants who were either informed or not informed of its mood-boosting effect.

In some scenarios, we estimated the *interactive* effect of demand characteristics. For example, in the same @coles2022fact study, participants provided happiness ratings both after smiling and scowling. Participants' mood generally improved when smiling vs. scowling (i.e., there was a main effect of facial pose). However, the difference was more pronounced when participants were told about the mood-boosting effects of smiling. In other words, there was an interaction between facial pose and demand characteristics. In this scenario, the interactive effect of demand characteristics was computed by calculating a standardized difference-in-differences score. These scores were computed similar to Cohen's $d_{s}$ and $d_{rm}$, but with mean-difference scores (as opposed to means).

Effect sizes were calculated so that positive values indicated an effect consistent with the communicated hypothesis. For example, if participants were told that an intervention should be mood boosting, an increase in mood would be coded as a positive effect. If participants were told that the intervention should be mood dampening, an increase in mood would be coded as a negative effect.

```{r corr.sens, include = F}
# examine how assumed repeated measures correlation impacts general pattern of results

# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")

# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <- 
  sapply(X = sens.df.list, 
         FUN = function(i){
           # open data
           df <- read.csv(paste0("data/r_sensitivity/",
                                    i)
                             ) 
           # fit model
           m <- rma.mv(yi = es,
                       V = es.var,
                       data = DF.es,
                       random = ~ 1 | id.study / id.es)
           
           # return overall es as a number
           m$b %>% 
             as.numeric() %>% 
             return()
           }
         )

# compute range of es values
sens.range <- max(sens.res) - min(sens.res)

# delete vestigial
rm(sens.df.list, sens.res)
```

Whenever possible, we used the *M*'s and *SD*'s reported in a paper to compute Cohen's *d*. If these values were not reported, we used (in order of preference), (1) *t*-values, (2) descriptive statistics extracted from figures (e.g, bar charts) using the WebPlotDigitizer [@drevon2017intercoder], (3) *F*-values, or (4) *p*-values. In instances where this information was not provided but the significance and direction of the effect was described, we assumed *p*-values of .04 and .50 for significant and non-significant effects respectively [e.g., @kenealy1988validation]. In a few instances, the outcome variable in a study was discrete (as opposed to continuous). In these cases, we approximated a Cohen's *d* score based on a transformation of the log odds ratio [@borenstein2011introduction].

For repeated-measure comparisons, the correlation between the repeated measures is needed to calculate Cohen's $d_{rm}$. This correlation is rarely reported, so we followed a recommendation by @borenstein2009effect and performed sensitivity analyses on an assumed correlation. We preregistered a default correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses produced virtually no change in overall effect size estimates---so we do not discuss them further.

```{r mult.eff, include = F}
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>% 
  # identify number of effect sizes for each study (id)
  group_by(id.study) %>% 
  count() %>% 
  
  # code whether each study has more than one effect size
  mutate(dep = if_else(condition = n > 1,
                       true = 1,
                       false = 0)
         ) %>% 
  
  # calculate proportion of studies with more than one effect size 
  ungroup() %>% 
  summarise(mult.eff = mean(dep)) %>% 
  
  # export as percentage
  as.numeric() %>% 
  round(digits = 2) * 100
```

`r mult.eff.per`% of studies contained multiple effect sizes of interest. For example, the full design in Coles et al. (2022) included a positive demand, nil demand, and control condition. Participants also completed several facial expression poses (happy, angry, and neutral) and self-reported several emotions (happiness and anger). To be comprehensive, we recorded all reported effect sizes and accounted for dependencies in our models (described later).

### Potential moderators

We coded several moderators that may help explain variability in demand effects. The first of these moderators allowed us to assess whether demand effects are additive. As a reminder, Cohen's $d$ represents a standardized difference between two groups. Often, this involved a single demand characteristic condition (positive, negative, or nil demand) compared to a control group. Sometimes, however, this comparison involved *two* demand characteristic conditions (e.g., positive demand vs. negative demand). If demand characteristics can be additive, their effects should be larger when two demand characteristic conditions are compared (as opposed to one condition being compared to a control group).

Instances where a demand characteristic condition was compared to a control group allowed us to additional test whether participants respond more strongly to positive, nil, or negative demand characteristics. We thus coded whether the comparison was positive demand vs. control, nil demand vs. control, or negative demand vs. control.

We also coded several study feature moderators that researchers have speculated may moderate demand effects. This included: (1) whether the sample was student, non-student (e.g., MTurk), or mixed, (2) whether the study was conducted online or in-person, (3) whether demand characteristics were manipulated within- vs. between-subjects, and (4) whether participants were paid or unpaid.

### Meta-analytic approach

`r mult.eff.per`% of studies in our meta-analysis contained multiple effect sizes of interest. To model this nested structure, we used three-level meta-analysis (3LMA; also referred to as "multilevel" meta-analysis). 3LMA accommodates nested effect sizes by modeling three sources of variability: the sampling error of individual studies (level 1), variability within studies (level 2), and variability between studies (level 3; often referred to as "random effects"). To estimate the overall effect size, we fit an intercept-only 3LMA model. For moderator analyses, the dummy-coded categorical moderators were separately entered into the model, which were used to estimate the moderating relationship and the effect size within each subgroup of the moderator.

```{r clean.env.2, include = F}
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
```

#### Publication bias analyses

Publication bias refers to the well-documented propensity for hypothesis-inconsistent findings to be disproportionately omitted from the published scientific record [@franco2014publication]. When present, publication bias can lead to inaccurate effect size estimates and inferential errors in meta-analysis. Consequently, we used three main approaches for assessing and correcting for potential publication bias in our estimation of the overall effect of demand characteristics.

First, we visually examined *funnel plots,* wherein observed effect sizes are plotted against a measure of their precision (e.g., standard error). In the absence of publication bias, the distribution typically resembles a funnel; relatively large studies estimate the effect with high precision, and effect sizes fan out in *both* directions as the studies become smaller. If, however, non-significant findings are disproportionately omitted from the scientific record (i.e., there is publication bias), the distribution is often asymmetric/sloped. Funnel plots traditionally contain one effect size per study, but many of our studies produced multiple effect sizes. Thus, we examined two funnel plots: one with all effect sizes and one with dependent effect sizes aggregated. For effect size aggregation, we assumed a default dependent effect size correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses did not change our overall conclusion about publication bias, so we do not discuss them further.

Second, we conducted precision-effect tests [@stanley2014meta]. In precision-effect tests, the relationship between observed effect sizes and their standard errors---which is typically absent when there is no publication bias---is estimated and controlled for in a meta-regression model. The slope of this model is generally interpreted as an estimate of publication bias, and the intercept is interpreted as the bias-corrected overall effect. Precision-effect tests were developed and validated for meta-analyses with independent effect sizes. Nonetheless, @rodgers2021evaluating demonstrated that the method retains fairly good statistical properties when (1) 3LMA is used or (2) dependent effect sizes are aggregated and modeled using random-effects (i.e., two level) meta-regression. We used both approaches.

Third, we used weight-function modeling [@vevea1995general]. In weight-function modeling, weighted distribution theory is used to model biased selection based on the significance of observed effects. If the adjusted model provides increased fit, publication bias is a concern and the model can be used to estimate the bias-corrected overall effect size. Once again, weight-function modeling was designed for independent effect sizes. Nonetheless, it has fairly good statistical properties when non-independent effect sizes are aggregated, which we did here [@rodgers2021evaluating].

As a sensitivity analysis, we included publication status (published or unpublished) as a dummy-coded predictor to our overall-effect 3LMA. This allowed us to estimate the difference in the magnitude of published vs. unpublished effects.

## Results

```{r overall, include = F}
# estimate overall effect size
overall <- 
  rma.mv(yi = es,
         V = es.var,
         data = DF.es,
         random = ~ 1 | id.study / id.es)

# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])

# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
             mean = overall$b,
             sd = tau, 
             lower.tail = F) %>% 
  round(2) * 100

h.i <- pnorm(q = (-.10),
             mean = overall$b,
             sd = tau,
             lower.tail = T) %>% 
  round(2) * 100

# estimate lower and upper bound of effect size distribution
dist.min <- rnorm(n = 1000000,
                  mean = overall$b,
                  sd = tau) %>% 
  min() %>% 
  round(digits = 2)

dist.max <- rnorm(n = 1000000,
                  mean = overall$b,
                  sd = tau) %>% 
  max() %>% 
  round(digits = 2)
```

```{r Mike.play, eval = F}
# type 3 SS / effect coding
options(contrasts = c('contr.sum',
                      'contr.poly'))
library(emmeans)

# super-fitted model

DF.es <- DF.es %>% 
  mutate(student = factor(student),
         paid = factor(paid),
         online = factor(online),
         ref.type = factor(ref.type)) %>%
  filter(!is.na(student),
         !is.na(paid),
         !is.na(online))

m <- rma.mv(yi = es,
            V = es.var,
            data = DF.es,
            random = ~ 1 | id.study / id.es,
            mods = ~ student + paid + online + ref.type,
            test= "t")

m <- rma.mv(yi = es,
            V = es.var,
            data = DF.es,
            random = ~ 1 | id.study / id.es,
            mods = ~ student + paid + online,
            test= "t")

rg <- qdrg(object = m, data = DF.es)

emmeans(rg, 
        pairwise ~ student,
        adjust = "none")

emmeans(rg, #  odd flip
        pairwise ~ paid,
        adjust = "none")

emmeans(rg, "paid")

emmeans(rg,
        pairwise ~ online,
        adjust = "none")

emmeans(rg,
        pairwise ~ ref.type,
        adjust = "none")


rma.mv(yi = es,
       V = es.var,
       data = DF.es,
       random = ~ 1 | id.study / id.es,
       mods = ~ student,
       test= "t")

rma.mv(yi = es,
       V = es.var,
       data = DF.es,
       random = ~ 1 | id.study / id.es,
       mods = ~ online,
       test= "t")

anova(m)

# look at cross tabs
## students are more likely to be in studies that are in-person, unpaid, and positive
table(DF.es$student, DF.es$online) %>% prop.table()
table(DF.es$student, DF.es$paid) %>% prop.table()
table(DF.es$student, DF.es$ref.type) %>% prop.table()

```

Results indicated that, overall, explicit manipulations of demand characteristics cause participants' responses to shift in a manner consistent with the communicated hypothesis, $d$ = `r overall$b`, 95% CI [`r overall$ci.lb`, `r overall$ci.ub`], $t$ = `r overall$zval`, $p$ `r printp(overall$pval)`. For example, if participants were hypothetically told that an intervention should improve mood (positive demand), they would generally report slightly improved moods; if told that an intervention should worsen mood (negative demand), they would generally report slightly worsened moods.

```{r forest_old, fig.cap = "Forest plot of effect sizes (Cohen's d) included in the Study 1 meta-analysis. The rightmost column includes each effect size citation. The center and right columns contain the estimated effect sizes (black diamonds) and their 95% confidence intervals (black error bars).", fig.height = 7.8, fig.width = 6.5, eval = F}
# create a temporary dataset containing effect sizes and 95% CI's
tmp <- DF.es %>%
  rowwise() %>% 
  mutate(se = sqrt(es.var),
         ub = es + (se * 1.96),
         ub = round(ub, 2),
         lb = es - (se * 1.96),
         lb = round(lb, 2),
         es = round(es, 2)) %>% 
  ungroup() %>% 
  arrange(es, id.study)

# create a forest plot
ggplot(data= tmp, 
       aes(y = rev(1: nrow(tmp)), 
           x = es,
           xmin = lb,
           xmax = ub)) +
  
  # create dotted line at d = 0
  geom_vline(xintercept = 0, 
             color = "black", 
             linetype = "dashed", 
             alpha = .5) +

  # add points and error bars
  geom_point(shape = "diamond",
             size = 1,
             colour = "black") +
  geom_errorbarh(height = .5,
                 colour = 'black',
                 size = .1) +
  
  # add citation label
  geom_text(aes(label = citation),
            x = -2.7,
            hjust = 0,
            size = 1) +
  
  # add CI label
  geom_text(aes(label = paste0(es,
                               " [", lb, ", ", ub, "]")),
            x = 3.9,
            size = 1,
            hjust = 1) +  
  
  # remove y-axis and fix x-axis name
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.line.y = element_blank()) +
  
  labs(x = "Cohen's d") +
  
  # increase plotting area
  scale_x_continuous(limits = c(-2.7, 4),
                     breaks = seq(from = -2, to = 3, by = 1),
                     expand = c(.01, .01)) +
  scale_y_continuous(expand = c(.01, 0))
```

```{r forest, fig.cap = "Forest plot of estimated effect sizes (grey diamonds), their 95% confidence intervals (grey error bars), and their citations (left). The estimated effect size distribution is also shown and colored based on whether demand characteristics produce more hypothesis-consistent responding (green; d > 0.10), more hypothesis-inconsistent responding (red; d < -0.10), or negligible shifts in responding (grey; |d| < 0.10).", fig.height = 7, fig.width = 6.5, warning = F}
# create a temporary dataset containing effect sizes and 95% CI's
tmp <- DF.es %>%
  rowwise() %>% 
  mutate(se = sqrt(es.var),
         ub = es + (se * 1.96),
         ub = round(ub, 2),
         lb = es - (se * 1.96),
         lb = round(lb, 2),
         es = round(es, 2)) %>% 
  ungroup() %>% 
  arrange(es, id.study)

# create a forest plot w/ distribution overlay
ggplot(data= tmp, 
       aes(y = rev(1: nrow(tmp)) * 0.007692308, 
           x = es,
           xmin = lb,
           xmax = ub)) +
  
  #hypothesis inconsistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#F8766D", 
            alpha = .25,
            xlim = c(-2, -.10)) +
  
  # negligible effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "grey80", 
            alpha = .25,
            xlim = c(-.10, .10)) +
  
  # hypothesis consistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#00998a", 
            alpha = .25,
            xlim = c(.10, 2)) +
  
  # create dotted line at d = 0
  geom_vline(xintercept = 0, 
             color = "black", 
             linetype = "dotted", 
             alpha = .5, 
             size =.5)  +

  # add points and error bars
  geom_point(shape = "diamond",
             size = 1,
             #colour = "black",
             #alpha = .75,
             alpha = .8,
             color = "dark grey") +
  geom_errorbarh(height = .005,
                 #colour = 'black',
                 size = .1,
                 #alpha = .75,
                 alpha = .8,
                 color = "dark grey") +
  
  # add citation label
  geom_text(aes(label = citation),
            x = -2.7,
            hjust = 0,
            size = 1) +
  
  # add CI label
  geom_text(aes(label = paste0(es,
                               " [", lb, ", ", ub, "]")),
            x = 3.9,
            size = 1,
            hjust = 1) +  
  
  # remove y-axis and fix x-axis name
  # theme(axis.title.y = element_blank(),
  #       axis.text.y = element_blank(),
  #       axis.ticks.y = element_blank(),
  #       axis.line.y = element_blank()) +
  
  labs(x = "Cohen's d",
       y = "density") +
  
  # increase plotting area
  scale_x_continuous(limits = c(-2.7, 4),
                     breaks = seq(from = -2, to = 3, by = 1),
                     expand = c(.01, .01)) +
  scale_y_continuous(expand = c(.005, 0))
```

```{r overall.dist, eval = F}
# create plot of effect size distribution
ggplot(NULL,
       aes(c(-1.2, 1.5))) +
  
  # hypothesis inconsistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#F8766D", 
            alpha = .5,
            xlim = c(-1.2, -.10)) +
  
  ## label
  geom_text(aes(label = paste0(h.i, "%")),
            x = -.3,
            y = .075,
            size = 4) +
  
  # negligible effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "grey80", 
            alpha = .5,
            xlim = c(-.10, .10)) +
  
  ## label
  geom_text(aes(label = paste0(100 - h.i - h.c, "%")),
            x = -0,
            y = .075,
            size = 4) +
  
  # hypothesis consistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#00998a", 
            alpha = .5,
            xlim = c(.10, 1.5)) +
  
  ## label
  geom_text(aes(label = paste0(h.c, "%")),
            x = .3,
            y = .075,
            size = 4) +
  
  
  # aesthetics
  ylim(c(0,1))
  labs(x = "Cohen's d") +
  
  scale_y_continuous(expand = c(0, 0)) +
  
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.line.y = element_blank(),
        )
```

Although demand characteristics produce more hypothesis-consistent responding *on average*, demand effects are not consistent (between-study $\tau$ = `r sqrt(overall$sigma2[1])`; within-study $\sigma$ = `r sqrt(overall$sigma2[2])`; Figure \@ref(fig:forest)). For the sake of example, we arbitrarily classified any effect size less than 0.10 standard deviation in either direction as “negligible”. Based on the meta-analytic mean and standard deviation (between-study $\tau$ + within-study $\sigma$), the estimated distribution of effects suggests that demand characteristics most often produce hypothesis-consistent shifts (`r h.c`%), but sometimes produce negligible shifts (`r 100 - h.c - h.i`%) or shifts in the opposite direction of the communicated hypothesis (`r h.i`%)



the estimated distribution of effects suggests that demand characteristics (a) sometimes produce negligible shifts in responding and (b) other times cause participants' responses to shift in the *opposite* direction of the communicated hypothesis (Figure \@ref(fig:forest)). For the sake of example, we arbitrarily classified any effect size less than 0.10 standard deviation in either direction as "negligible". Based on this classification, `r h.c` % of demand characteristics manipulations produce hypothesis-consistent shifts in responding, `r h.i` % produce hypothesis-inconsistent shifts, and `r 100 - h.c - h.i`% produce negligible shifts in either direction.

### Moderator analyses

```{r mod, include = F}
# create moderator analysis function
ModAnalysis = function(m, df = DF.es) {
  
  # set dataset
  df <- df
  
  # moderator analysis
  mod.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ ", m)),
                  test= "t")
  
  sub.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ 0 + ", m)),
                  test= "t")
  
  # return results as list
  return(list(mod = mod.m,
              sub = sub.m)) 
}

# conduct moderator and subgroup analyses for moderators assessed with full dataset 
mod.l <- c("student", "paid", "online", 
           "design", "ref.r", "published",
           "year")
mod.r <- 
  sapply(X = mod.l,
         simplify = F,
         FUN = ModAnalysis)

rm(mod.l)

# test ref.type moderator in scenarios where there is a control comparison (i.e., ref.r == single)
mod.r[["ref.type"]] <- 
  ModAnalysis(m = "ref.type",
              df = DF.es[DF.es$ref.r == "single", ])

# add motivation, opportunity, belief, and prediction moderators
## Note: comparisons with nil-demand conditions are excluded
mod.r.2 <- 
  sapply(X = c("mot", "opp", "bel", "pre"),
         simplify = F,
         FUN = ModAnalysis,
         df = DF.es %>% 
               filter(ref.type != "cvz" &
                        ref.type != "pvz"))

## combine results
mod.r = c(mod.r, mod.r.2)

# delete vestigial 
rm(mod.r.2)
```

The observed variability in demand effects drastically exceeded what would be expected from sampling error alone, $Q$(`r overall$k - 1`) = `r overall$QE` , $p$ `r printp(overall$QEp)`. This suggests the existence of moderators.

Results indicated that the effects of demand characteristics tended to differ by participant pool, $F$(`r mod.r$student$mod$QMdf[1]`, `r mod.r$student$mod$QMdf[2]`) = `r mod.r$student$mod$QM`, $p$ = `r printp(mod.r$student$mod$QMp)`. The effects were generally positive and medium-to-large in studies with students ($d$ = `r mod.r$student$sub$b[3]`, 95% CI [`r mod.r$student$sub$ci.lb[3]`, `r mod.r$student$sub$ci.ub[3]`], $p$ `r printp(mod.r$student$sub$pval[3])`), positive and extremely small in studies with a mix of students and non-students ($d$ = `r mod.r$student$sub$b[1]`, 95% CI [`r mod.r$student$sub$ci.lb[1]`, `r mod.r$student$sub$ci.ub[1]`], $p$ = `r printp(mod.r$student$sub$pval[1])`), and near-zero in studies with non-students ($d$ = `r mod.r$student$sub$b[2]`, 95% CI [`r mod.r$student$sub$ci.lb[2]`, `r mod.r$student$sub$ci.ub[2]`], $p$ = `r printp(mod.r$student$sub$pval[2])`). The effects of demand characteristics also tended to be slightly more positive for in-person ($d$ = `r mod.r$online$sub$b[1]`, 95% CI [`r mod.r$online$sub$ci.lb[1]`, `r mod.r$online$sub$ci.ub[1]`], $p$ `r printp(mod.r$online$sub$pval[1])`) vs. online ($d$ = `r mod.r$online$sub$b[2]`, 95% CI [`r mod.r$online$sub$ci.lb[2]`, `r mod.r$online$sub$ci.ub[2]`], $p$ = `r printp(mod.r$online$sub$pval[2])`) studies, although this did not meet conventional thresholds of statistical significance, $F$(`r mod.r$online$mod$QMdf[1]`, `r mod.r$online$mod$QMdf[2]`) = `r mod.r$online$mod$QM`, $p$ = `r printp(mod.r$online$mod$QMp)`.

The effects of demand characteristics appeared to be additive. Compared to instances where a demand characteristic condition was compared to a control group ($d$ = `r mod.r$ref.r$sub$b[2]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[2]`, `r mod.r$ref.r$sub$ci.ub[2]`], $p$ = `r printp(mod.r$ref.r$sub$pval[2])`), effect sizes were approximately twice as large when two demand characteristic conditions were compared ($d$ = `r mod.r$ref.r$sub$b[1]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[1]`, `r mod.r$ref.r$sub$ci.ub[1]`], $p$ `r printp(mod.r$ref.r$sub$pval[1])`), $F$(`r mod.r$ref.r$mod$QMdf[1]`, `r mod.r$ref.r$mod$QMdf[2]`) = `r mod.r$ref.r$mod$QM`, $p$ `r printp(mod.r$ref.r$mod$QMp)`. Instances where a demand characteristic condition was compared to a control group allowed us to additional test whether participants respond more strongly to positive, nil, or negative demand characteristics. Results indicated that they might, $F$(`r mod.r$ref.type$mod$QMdf[1]`, `r mod.r$ref.type$mod$QMdf[2]`) = `r mod.r$ref.type$mod$QM`, $p$ = `r printp(mod.r$ref.type$mod$QMp)`. The effect of demand characteristics tended to be nearly twice as large in the nil ($d$ = `r mod.r$ref.type$sub$b[2]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[2]`, `r mod.r$ref.type$sub$ci.ub[2]`], $p$ `r printp(mod.r$ref.type$sub$pval[2])`) vs. positive ($d$ = `r mod.r$ref.type$sub$b[1]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[1]`, `r mod.r$ref.type$sub$ci.ub[1]`], $p$ = `r printp(mod.r$ref.type$sub$pval[1])`), and negative demand conditions ($d$ = `r mod.r$ref.type$sub$b[3]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[3]`, `r mod.r$ref.type$sub$ci.ub[3]`], $p$ = `r printp(mod.r$ref.type$sub$pval[3])`).

We did not find that the effects of demand characteristics tended to differ depending on whether they were manipulated within- ($d$ = `r mod.r$design$sub$b[1]`, 95% CI [`r mod.r$design$sub$ci.lb[1]`, `r mod.r$design$sub$ci.ub[1]`], $p$ `r printp(mod.r$design$sub$pval[1])`) vs. between-subjects ($d$ = `r mod.r$design$sub$b[2]`, 95% CI [`r mod.r$design$sub$ci.lb[2]`, `r mod.r$design$sub$ci.ub[2]`], $p$ = `r printp(mod.r$design$sub$pval[2])`), $F$(`r mod.r$design$mod$QMdf[1]`, `r mod.r$design$mod$QMdf[2]`) = `r mod.r$design$mod$QM`, $p$ = `r printp(mod.r$design$mod$QMp)`. We also did not find that the effects of demand characteristics differed by the year the record was completed or published, $\beta$ = `r mod.r$year$mod$b["year", ]`, 95% CI [`r mod.r$year$mod$ci.lb[2]`, `r mod.r$year$mod$ci.ub[2]`], $t$(`r sum(mod.r$year$mod$QMdf)`) = `r mod.r$year$mod$zval[2]`, $p$ = `r printp(mod.r$year$mod$pval[2])`.

The effects of demand characteristics tended to be *numerically* larger in unpaid ($d$ = `r mod.r$paid$sub$b[1]`, 95% CI [`r mod.r$paid$sub$ci.lb[1]`, `r mod.r$paid$sub$ci.ub[1]`], $p$ `r printp(mod.r$paid$sub$pval[1])`) vs. paid ($d$ = `r mod.r$paid$sub$b[2]`, 95% CI [`r mod.r$paid$sub$ci.lb[2]`, `r mod.r$paid$sub$ci.ub[2]`], $p$ = `r printp(mod.r$paid$sub$pval[2])`) studies---but this difference was not statistically significant, $F$(`r mod.r$paid$mod$QMdf[1]`, `r mod.r$paid$mod$QMdf[2]`) = `r mod.r$paid$mod$QM`, $p$ = `r printp(mod.r$paid$mod$QMp)`.

#### Exploratory attempt to reduce confounding (tmp)

The above moderator analyses indicated that demand effects tend to be positive in studies with student populations, in-person procedures, and no compensation. However, an exploratory inspection of the data revealed that these variables may be confounded. For example, students were more likely to be participants in in-person (61% student samples) vs. online (14% student samples) studies. Students were also more likely to be participants in unpaid (65% student samples) vs. paid (11% student samples) studies. In hindsight, this confounding seems obvious---but it was not anticipated when we pre-registered our analysis plan.

As an exploratory analysis, we fit a three-level meta-analysis with student status, data collection medium, and payment status entered as effect-coded factors. The results should be interpreted with some skepticism because the model may be overfit. Nonetheless, this exploratory analysis indicated that student status---but not data collection medium or payment status---was a significant moderator of demand effects. In other words, only student status is robustly associated with differences in demand effects.

### Publication bias analyses

```{r pub.bias, include = F}
##########################
# Define publication bias analysis that
# 1. Mathur and VanderWeele 2020 sensitivity analyses
# 2. Fits three-level precision-effect test
# 3a. Aggregates dependent effect sizes (with given rho value)
# 3b. Aggregated precision-effect test
# 3b. Fits Vevea and Hedges (1995) Weight-Function Model w/ aggregated effects
# 4a. Fit funnel plot
# 4b. Fit funnel plot w/ aggregated dependencies
# 5. Organizes results into list
##########################

PubBias = function(rho.val = .5){
  # 1. sensitivity analyses
  ########################
  sens <- corrected_meta(yi = DF.es$es,
                         vi = DF.es$es.var,
                         eta = 49,
                         clustervar = DF.es$id.study,
                         model = "robust",
                         favor.positive = T)
  
  # 2. three-level precision-effect test
  ########################
  pe.3l <- rma.mv(yi = es,
                  V = es.var,
                  mods = ~ sqrt(es.var),
                  data = DF.es,
                  random = ~ 1 | id.study / id.es)
  
  # 3a. aggregate dependent effect sizes
  ########################
  DF.agg <- DF.es %>%
    
    # convert to an 'escalc' object so function can run
    escalc(yi = es,
           vi = es.var,
           data = DF.es,
           measure = "SMD") %>%
    
    # delete vestigial: es is now yi; es.var is now vi
    select(-c(es, es.var)) %>% 
    
    # aggregate dependencies
    aggregate(x = .,
              cluster = id.study,
              rho = rho.val)
  
  # 3b. aggregated precision-effect test
  ########################
  pe.a <- rma.uni(yi = yi,
                  vi = vi,
                  mods = ~ sqrt(vi),
                  data = DF.agg,
                  method = "REML")

  # 3c. Weight-function model
  ########################
  weight.funct <- weightfunct(effect = DF.agg$yi,
                              v = DF.agg$vi,
                              mods = NULL,
                              weights= NULL,
                              fe = FALSE,
                              table = TRUE,
                              pval = NULL)
  
  # 4a. funnel plot
  ########################
  par(mfrow=c(1,2))
  
  rma.uni(yi = es,
          vi = es.var,
          data = DF.es,
          method = "REML") %>%   
  metafor::funnel(hlines = "lightgray",
                  xlab = "Cohen's standardized d") 
  
  # 4b. funnel plot w/ aggregated dependencies
  ########################
  rma.uni(yi = yi,
          vi = vi,
          data = DF.agg,
          method = "REML") %>%   
    metafor::funnel(hlines = "lightgray",
                    xlab = "Cohen's standardized d (aggregated)")
  
  # save funnel plot as object
  funnel.plot <- recordPlot()
    
  # clear R environment
  plot.new()
  
  # 5. Organize results in list 
  ########################
  list(sens = sens,
       pe.3l = pe.3l,
       DF.agg = DF.agg,
       pe.a = pe.a,
       weight.funct = weight.funct,
       funnel = funnel.plot) %>%  
    return()
}

# for range of rho values, run publication bias analyses
rho.l = seq(from = .1,
            to = .9,
            by = .2)

pub.r <- lapply(X = rho.l,
                FUN = PubBias)

names(pub.r) = paste0("rho_", rho.l) #  name list

# delete vestigial 
rm(rho.l, PubBias)

# look at sensitivity analyses
## general story: often, but not always, find evidence of reverse publication bias (preference for negative effects)
# lapply(pub.r, function(x){x[["pe.a"]]})
# lapply(pub.r, function(x){x[["peese"]]})
# lapply(pub.r, function(x){x[["weight.funct"]]})
# lapply(pub.r, function(x){x[["funnel"]]})

# plot funnels
# overall %>% 
#   metafor::funnel(x = .,
#                   hlines = "lightgray",
#                   xlab = "Cohen's standardized d")
# 
# pub.r$rho_0.5$pe.3l$b[2]
# 
# pub.r$rho_0.5$weight.funct %>% View()
```

Overall, publication bias analyses were inconclusive. For instance, a funnel plot containing all effect sizes appeared to indicate that publication bias favored instances where participants’ responses shifted in a hypothesis-consistent manner. However, a funnel plot where non-independent effect sizes were aggregated appeared to indicate the opposite: that publication bias favored non-significant or hypothesis-inconsistent shifts in participants' responses.

Precision-effect tests similarly yielded opposite conclusions depending on whether we fit (a) 3LMA with non-aggregated effect size estimates, or (b) two-level meta-analysis with aggregated dependent effect size estimates. On one hand, precision-effect tests with 3LMA provided a non-significant estimate of publication bias that favored hypothesis-consistent shifts in participants' responses, $\beta$ = `r pub.r$rho_0.5$pe.3l$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[2]`, `r pub.r$rho_0.5$pe.3l$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.3l$pval[2])`. The bias-corrected overall effect size estimate did not significantly differ from zero, $d$ = `r pub.r$rho_0.5$pe.3l$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[1]`, `r pub.r$rho_0.5$pe.3l$ci.ub[1]`], $p$ = `r printp(pub.r$rho_0.5$pe.3l$pval[1])`. On the other hand, two-level precision-effect tests with aggregated dependent effect size estimates yielded an opposite pattern: that there was a slightly (but not statistically significant) preference for non-significant or hypothesis-inconsistent shifts in participants' responses, $\beta$ = `r pub.r$rho_0.5$pe.a$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[2]`, `r pub.r$rho_0.5$pe.a$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[2])`. The bias-corrected overall effect size estimate was thus slightly adjusted upward, $d$ = `r pub.r$rho_0.5$pe.a$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[1]`, `r pub.r$rho_0.5$pe.a$ci.ub[1]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[1])`. In other words, depending on how dependencies were handled, precision-effect tests yielded inconsistent conclusions about the direction of publication bias and the significance of the bias-corrected overall effect of demand characteristics.

A weight-function model suggested that better fit was achieved with a model indicating that publication bias favored non-significant or hypothesis-inconsistent shifts in participants' responses, $\chi^2$(1) = 10.80, $p$ = .001. The bias-corrected overall effect size was thus upward-adjusted, $d$ = 0.41, 95% CI [0.19, 0.62], $p$ \< .001. A comparison of unpublished ($d$ = `r mod.r$published$sub$b[1]`, 95% CI [`r mod.r$published$sub$ci.lb[1]`, `r mod.r$published$sub$ci.ub[1]`], $p$ = `r printp(mod.r$published$sub$pval[1])`) and published ($d$ = `r mod.r$published$sub$b[2]`, 95% CI [`r mod.r$published$sub$ci.lb[2]`, `r mod.r$published$sub$ci.ub[2]`], $p$ `r printp(mod.r$published$sub$pval[2])`) studies yielded a similar pattern, although the difference was not statistically significant, $F$(`r mod.r$published$mod$QMdf[1]`, `r mod.r$published$mod$QMdf[2]`) = `r mod.r$published$mod$QM`, $p$ = `r printp(mod.r$published$mod$QMp)`.

```{r funnel, fig.cap = "Raw (A) or aggregated (B) effect sizes ploted against their corresponding standard errors.", fig.height = 3.75, fig.width = 6.5}
########## 
# Funnel plot with non-aggregated dependencies
########## 
# create a temporary dataset with standard error (se) values
tmp <- DF.es %>%  
  rowwise() %>% 
  mutate(se = sqrt(es.var)) %>% 
  ungroup()

# create temporary sequence of ses
se.seq = seq(0, max(tmp$se), 
             length.out = nrow(DF.es))

ll95 = overall$b[1] - (1.96 * se.seq)
ul95 = overall$b[1] + (1.96 * se.seq)

# create coordinates for polygon
t.coord <- rbind(cbind(x = overall$b[1],
                       y = 0),
                 
                 cbind(x = min(ll95),
                       y = max(tmp$se)),
                 
                 cbind(x = max(ul95),
                       y = max(tmp$se))
                 ) %>% 
  as.data.frame()
  
# plot
a <- ggplot(data = tmp,
            aes(x = es,
                y = se)) +
  geom_polygon(data = t.coord,
               aes(x = x,
                   y = y),
               fill = "#3366FF",
               alpha = .1) +
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  scale_y_reverse(expand = c(.01, 0)) +
  scale_x_continuous(limits = c(-1.5, 2.1),
                     expand = c(.01, .01)) +
  geom_vline(xintercept = overall$b[1],
             linetype = "dotted") +
  labs(x = "Cohen's d", 
       y = "Standard error")

# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord)

########## 
# Funnel plot with aggregated dependencies
########## 
# create a temporary dataset with standard error (se) values
tmp <- pub.r$rho_0.5$DF.agg %>% 
  rowwise() %>% 
  mutate(es = yi,
         se = sqrt(vi)) %>% 
  ungroup()

# calculate overall effect size
tmp.meta <- rma.uni(yi = yi,
                    vi = vi,
                    data = tmp,
                    method = "REML")

# create temporary sequence of ses
se.seq = seq(0, max(tmp$se), 
             length.out = nrow(tmp))

ll95 = tmp.meta$b[1] - (1.96 * se.seq)
ul95 = tmp.meta$b[1] + (1.96 * se.seq)

# create coordinates for polygon
t.coord <- rbind(cbind(x = tmp.meta$b[1],
                       y = 0),
                 
                 cbind(x = min(ll95),
                       y = max(tmp$se)),
                 
                 cbind(x = max(ul95),
                       y = max(tmp$se))
                 ) %>% 
  as.data.frame()

b <- ggplot(data = tmp,
       aes(x = es,
           y = se)) +
  geom_polygon(data = t.coord,
               aes(x = x,
                   y = y),
               fill = "#3366FF",
               alpha = .1) +
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  scale_y_reverse(expand = c(.01, 0)) +
  scale_x_continuous(limits = c(-1.5, 2.1),
                     expand = c(.01, .01)) +
  geom_vline(xintercept = tmp.meta$b[1],
             linetype = "dotted") +
  labs(x = "Cohen's d",
       y = "")

# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord, tmp.meta)
  
########## 
# Plot funnels next to each other plot with aggregated dependencies
##########
plot_grid(a, b,
          labels = c("A", "B"))

rm(a, b)
```

## Discussion

Study 1a provides the first quantitative synthesis of strict experimental tests of demand characteristics. Overall, explicit manipulations of demand characteristics cause participants' responses to shift in a manner consistent with the communicated hypothesis. However, significant heterogeneity was observed. Using arbitrary thresholds, we estimated that `r h.c`% of demand characteristics manipulations produce hypothesis-consistent shifts ($d$ \> 0.10), `r h.i`% produce hypothesis-*in*consistent shifts ($d$ \< -0.10), and `r 100 - h.c - h.i`% produce negligible shifts in either direction (-0.10 \< $d$ \> 0.10). Moderator analyses revealed three study features that are associated with more hypothesis-consistent shifts in responses: (1) sampling student populations, (2) running studies in-person, and (3) communicating that the researchers hypothesizes there will be *no* shift in responses (i.e., using nil demand manipulations). We also found non-significant evidence of increases in hypothesis-consistent responding when participants were paid. However, attempts to unconfound these moderator analyses failed to provide robust evidence of moderation by in-person and payment status. Furthermore, publication bias analyses yielded opposite conclusions about the direction and impact of publication bias. Consequently, these results are non-definitive.

Study 1a provides preliminary insights on the magnitude, consistency, and contextual moderators of demand effects. However, it was not designed to evaluate outstanding questions regarding the extent to which these effects are driven by response bias vs. placebo effects. For example, consider our finding that demand characteristics tend to produce more hypothesis-consistent shifts in responses when students are sampled. If this is true, it may occur because students are more motivated to help the experimenter confirm their hypothesis (a response bias). Alternatively, it may occur because students are more likely to *believe* the communicated hypothesis (a placebo effect). In other words, although we have preliminary evidence of contextual modifiers of demand effects, we still lack an explanation of why these contexts matter and how demand effects work more broadly. In Study 1b, we begin investigating this outstanding issue through an extension of the meta-analysis.

# Study 1b

Study 1b was designed to examine whether the meta-analytic effect size variability can be explained by factors theorized to underlie response biases (i.e., motivation and opportunity to adjust responses) and placebo effects (i.e., belief in the experimenter's hypothesis; Figure \@ref(fig:framework)). Unfortunately, these factors were rarely measured in the studies included in the meta-analysis. (See General Discussion for our call for more direct tests of underlying mechanisms.) Thus, we (a) estimated their values through a new set of participants and then (b) tested their moderating role by entering the values into meta-regressions. Also through meta-regression, we examined whether a new set of participants could retroactively predict the effects of the demand characteristic manipulations in the Study 1a meta-analysis.

## Methodology

```{r vig.desc, include = F}
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>% 
  nrow()
```

For each study in the meta-analysis, we created vignettes that described the key details for each demand characteristic condition and dependent variable combination. For example, @standing2008demonstration had two demand characteristic manipulations (positive and negative demand) and two dependent variables (measures of verbal and spatial reasoning). Thus, we created four vignettes for this study (see Figure \@ref(fig:vig)).

In total, there were `r vig.n` vignettes. We did not create vignettes for control conditions because participants were not given information about the experimenter's hypothesis. Because there were no explicit demand characteristics to act upon, we left motivation, belief, and opportunity values blank for this condition.

```{r vig, fig.cap = "Vignettes for Standing et al. (2008), which described the key details for each demand characteristic condition (bolded and underlined) and dependent variable (bolded and italicized) combination."}
knitr::include_graphics("images/metaware_vigs.png")
```

```{r survey.n, include = F}
# identify number of participants who completed the survey
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>% 
  nrow()
```

Using a web-based survey, `r survey.n` undergraduates from Stanford University reviewed 10 randomly-selected vignettes in exchange for course credit. For each vignette, raters were asked to first identify the researcher's hypothesis. Here, participants chose between four options that described a filler effect (usually involving an irrelevant dependent variable) or a positive, negative, or nil effect of the independent variable on the dependent variable. Afterwards, they rated the extent to which they would hypothetically (1) be motivated to adjust responses based on the hypothesis (-3 = "extremely motivated to adjust responses to be inconsistent" to 3 = "extremely motivated to adjust responses to be consistent"), (2) be able to adjust their responses on the outcome-of-interest (0 = "extremely incapable" to 4 = "extremely capable), and (3) believe the experimenter's hypothesis (-3 ="strong disbelief" to 3 = "strong belief"). Raters also indicated whether they believed participants would change their responses to confirm the hypothesis, which we discuss later. These questions were presented in random order.

Ratings were removed in instances where the rater did not correctly identify the hypothesis communicated in the vignette. The remaining ratings were averaged across raters to provide mean estimates of motivation, opportunity, and belief.

```{r mods, fig.cap = "Hypothetical data from a study where a procedure is either described as mood-boosting (positive demand), described as mood-dampening (negative demand), or not described at all (control). Data provides examples of how the effects of demand characteristics (d) on self-reported mood are moderating by participants' reports of their motivation to confirm the stated hypothesis (m, Panel A), belief in the stated hypothesis (b, Panel B), and opportunity to adjust responses (o, Panel C). In each panel, separate examples are provided for scenarios where motivation is invariant (Column 1) and variant (Column 2) across demand characteristic manipulations"}
knitr::include_graphics("images/metaware_mods.png")
```

### Accounting for different demand comparisons

As mentioned before, Cohen's $d$ represents the standardized difference between *two* groups. Thus, for each observation, we summed the motivation, opportunity, and belief ratings for the two groups being compared. Doing so allowed us to accommodate the fact that some comparisons involved two demand characteristics conditions. For example, imagine a study where participants are told a procedure will boost mood (positive demand), told a procedure will dampen mood (negative demand), or not told about an expected effect (control). Compared to a control condition, participants who are motivated to confirm the hypothesis are theorized to have upward-biased responses in the positive demand condition and downward-biased responses in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 1). When comparing the two demand conditions, the size of the demand effect should be doubled because the motivational forces in the two conditions produce an additive effect. In a different hypothetical context, these motivational forces could cancel each other out. This might happen if participants were (a) motivated to confirm the hypothesis in the positive demand condition, and (b) motivated to *dis*confirm the hypothesis in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 2). Summing motivation scores allowed us to accommodate this possibility, and we used the same approach for belief (Figure \@ref(fig:mods), Panel B) and opportunity ratings (Figure \@ref(fig:mods), Panel C).

We did not include nil-hypothesis comparisons in our analyses because our coding strategy could not accommodate the potential moderating role of motivation and belief in these conditions. For example, imagine that a participant is (a) told that an intervention will not impact mood (nil demand), and (b) is extremely motivated to disconfirm the hypothesis. Relative to a control condition, this participant could disconfirm the hypothesis by either increasing *or* decreasing their mood report. Thus, even if motivation does moderate the effects of demand characteristics, we would not expect a systematic pattern to emerge with our coding scheme.

### Rater forecasts of demand effects

Even if researchers cannot explain how demand characteristics work, it might be valuable to be able to predict their effects [@yarkoni2017choosing]. Orne suggested that one group that may be particularly good at predicting these effects is participants themselves [-@orne1969demand]. To examine this, raters also predicted whether other participants would confirm vs. disconfirm the researcher's hypothesis (-3 = "extremely likely to adjust responses to be inconsistent" to 3 = "extremely likely to adjust responses to be consistent"). We processed these data using the same approach as the motivation, opportunity, and belief scores (e.g., summed ratings when comparing two demand conditions).

```{r testfig, eval = F}
rm(list = ls())

library(ggpubr)

tmp <- cbind(mood = c(4, .075, -4),
             moti = c("m = 4", "", "m = 4"),
             cond = c("1.positive", "2.control", "3.negative")) %>% 
  as.data.frame() %>% 
  mutate(mood = as.numeric(mood))

cond = c("1.positive", "2.control", "3.negative")

tmp <- 
  rbind(
    # motivation invariant
    cbind(mood = c(4, .075, -4),
          mod = c("m = 4", "", "m = 4"),
          cond = cond,
          mod.desc = rep("Motivation ratings invariant across conditions", 3)),
    
    # motivation variant
    cbind(mood = c(4, .075, 4),
          mod = c("m = 4", "", "m = -4"),
          cond = cond,
          mod.desc = rep("Motivation ratings variant across conditions", 3))
  ) %>% 
  as.data.frame() %>% 
  mutate(mood = as.numeric(mood))

ggplot(data = tmp,
       aes(x = cond,
           y = mood)) +
  # facet by mod.desc
  facet_grid(cols = vars(mod.desc)) +
  
  # add bar and moderator scores
  geom_bar(stat = "identity") +
  geom_text(aes(label = mod),
            position = position_stack(vjust=0.5),
            color = "white") +
  # add brackets
  geom_bracket(xmin = "1.positive", xmax = "2.control",
               label = "test",
               y.position = 5)
  ylim(c(-10, 10))
```


## Results

```{r modfig, fig.cap= "The effects of demand characteristics on participants' responses were not significantly moderated by motivation (Panel A) or opportunity (Panel B) ratings. They were, however, significantly moderated by belief (Panel C) and prediction (Panel D) ratings.", fig.height = 4.5, fig.width = 6.5}

#################
# motivation plot
#################
# get predicted and actual values into a single dataset
mot.df <- predict(mod.r$mot$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$mot$mod$yi)

# plot
m <- ggplot(data = mot.df,
            aes(x = X.mot,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "Cohen's d", 
       x = "motivation ratings")

rm(mot.df)

#################
# opportunity plot
#################
# get predicted and actual values into a single dataset
opp.df <- predict(mod.r$opp$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$opp$mod$yi)

# plot
o <- ggplot(data = opp.df,
            aes(x = X.opp,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "", 
       x = "opportunity ratings")

rm(opp.df)

#################
# belief plot
#################
# get predicted and actual values into a single dataset
bel.df <- predict(mod.r$bel$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$bel$mod$yi)

# plot
b <- ggplot(data = bel.df,
            aes(x = X.bel,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "Cohen's d", 
       x = "belief ratings")

rm(bel.df)

#################
# prediction plot
#################
# get predicted and actual values into a single dataset
pre.df <- predict(mod.r$pre$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$pre$mod$yi)

# plot
p <- ggplot(data = pre.df,
            aes(x = X.pre,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "", 
       x = "prediction ratings")

rm(pre.df)

#################
# merge plots
#################
plot_grid(m, o, b, p,
          labels = c("A", "B", "C", "D"))

rm(m, o, b, p)
```

If demand effects are driven by response biases, their effects are expected to be moderated by participants' motivation and opportunity to adjust responses (Figure \@ref(fig:framework)). Inconsistent with this view, we did not find that demand effects were moderated by ratings of motivation, $\beta$ = `r mod.r$mot$mod$b["mot", ]`, 95% CI [`r mod.r$mot$mod$ci.lb[2]`, `r mod.r$mot$mod$ci.ub[2]`], $t$(`r mod.r$mot$mod$ddf[["mot"]]`) = `r mod.r$mot$mod$zval[2]`, $p$ = `r printp(mod.r$mot$mod$pval[2])`) or opportunity to adjust responses, $\beta$ = `r mod.r$opp$mod$b["opp", ]`, 95% CI [`r mod.r$opp$mod$ci.lb[2]`, `r mod.r$opp$mod$ci.ub[2]`], $t$(`r mod.r$opp$mod$ddf[["opp"]]`) = `r mod.r$opp$mod$zval[2]`, $p$ = `r printp(mod.r$opp$mod$pval[2])` (Figure @\ref(fig:modfig), Panels A and B).

If demand effects are driven by placebo, their effects should be moderated by participants' belief in the communicated hypothesis. Consistent with this view, demand characteristic effects were positively associated with ratings of belief in the experimenter’s hypothesis, $\beta$ = `r mod.r$bel$mod$b["bel", ]`, 95% CI [`r mod.r$bel$mod$ci.lb[2]`, `r mod.r$bel$mod$ci.ub[2]`], $t$(`r mod.r$bel$mod$ddf[["bel"]]`) = `r mod.r$bel$mod$zval[2]`, $p$ = `r printp(mod.r$bel$mod$pval[2])` (Figure \@ref(fig:modfig), Panel C). Last, we did find that raters’ predictions were significantly associated with observed demand effects, $\beta$ = `r mod.r$pre$mod$b["pre", ]`, 95% CI [`r mod.r$pre$mod$ci.lb[2]`, `r mod.r$pre$mod$ci.ub[2]`], $t$(`r mod.r$pre$mod$ddf[["pre"]]`) = `r mod.r$pre$mod$zval[2]`, $p$ = `r printp(mod.r$pre$mod$pval[2])` (Figure \@ref(fig:modfig), Panel D).

## Discussion

Contrary to classic conceptualizations of the impact of demand characteristics and comprehensive frameworks proposed by @rosnow1997people and @coles2022fact, we did not find evidence of two moderators that have been theorized to underlie a response bias mechanism: motivation and opportunity to adjust responses. We did, however, find evidence that such effects are moderated by a measure of participants’ belief in the communicated effect. This provides preliminary evidence of a placebo-based mechanism that was initially proposed as an *extension* of classic demand characteristic frameworks.

To test the moderating role of participants' motivation to adjust responses, opportunity to adjust responses, and belief in the experimenter's hypothesis, we had to rely on ratings from an new set of participants. This was necessary because researchers have rarely measured these proposed moderators. However, it is not without limitations. First, it is possible that raters did not have enough information to make an accurate prediction about participants' motivation, opportunity to adjust responses, and belief in the communicated hypothesis. For the sake of feasibility, we gave participants a short summary of the study. However, it is not clear if participants could accurately imagine the reality of being in these studies based on these relatively short descriptions. Indeed, when trying to gauge the impact of demand characteristics, @orne1969demand often would provide participants with extensive information about the study---perhaps even by running them through the procedures. It is thus possible that participants would have provided more valid ratings if we would have provided them with more information about the study (e.g., video recreations of the procedures).

Second, it is possible that our specific sample of raters---or maybe even modern-day participants in general---are not representative of the people sampled in previous research. In other words, maybe our 21th century Stanford University undergraduates have different study-related motivations, judgments, and beliefs than the participants who completed previous studies on demand characteristics. This seems likely to be true---but we did not find that it explains our pattern of results. To test the idea, we re-ran our motivation, opportunity, and belief moderator analyses focusing only on studies completed in the *past decade*. Doing so helped minimize differences between the participants who completed the original studies and our rating task. The pattern of results in this sensitive analysis, however, was largely the same as those from the full dataset.

```{r mod.sensitivity, include = F}
# sensitivity analysis
mod.year.sens <-
  sapply(X = c("mot", "opp", "bel"),
         simplify = F,
         FUN = ModAnalysis,
         df = DF.es %>% 
               filter(year > 2012 & # only include studies from past decade
                        ref.type != "cvz" &
                        ref.type != "pvz"))

# inspect results
mod.year.sens$mot$mod
mod.year.sens$opp$mod
mod.year.sens$bel$mod

# delete sensitivity analysis
rm(mod.year.sens)
```

To address these two major limitations, we re-examined the moderators in a small replication of an experiment in the demand characteristics literature.

# Study 2

After Study 1b participants completed the vignette ratings task, we had them complete a close replication of @coles2022fact.

## Methodology

```{r s2.data, include = F}
# open and clean data
DF.s2 <- 
  # open data
  read_csv(file = "data/metaware_survey2_clean.csv") %>% 
  
  # fix variable class
  mutate_at(.vars = c("sub", "demand",
                      "trial", "block.num"),
            .funs = as.factor)
```

After completing the Study 1b ratings, we told `r length(unique(DF.s2$sub))` participants that we hypothesized that posed smiles will either increase (positive demand, n = `r length(unique(DF.s2[DF.s2$demand == "pos", ]$sub))`) or not impact (nil demand, n = `r length(unique(DF.s2[DF.s2$demand == "nil", ]$sub))`) feelings of happiness. Participants than posed happy and neutral expressions across two blocks. For happy poses, participants were instructed to move the corner of their lips toward their ears, elevating their cheeks. For neutral poses, participants were instructed to maintain a blank expression. Participants held each pose for 5 seconds with the assistance of an on-screen timer. After each pose, participants self-reported the extent to which they experienced happiness, satisfaction, and enjoyment (0 = "not at all" to 6 = "maximally"), which were averaged to form a happiness composite score. As filler items, participants also self-reported the extent to which they experienced fear (alarmed, scared, and fear) and anger (irritation, aggravation, and annoyance). Using similar items as Study 1b, participants at the end of the study reported the extent to which they were motivated to confirm the hypothesis, had the opportunity to adjust their responses, and believed the communicated effect. Altogether, the study used a 2 (facial pose: happy or neutral) × 2 (block: first or second) × 2 (demand characteristics: positive demand or nil demand) mixed design, with demand characteristics manipulated between subjects.

## Results

```{r s2.d.int, include = F}
# test facial feedback by demand interaction
## fit mixed-effects model
m.d <- lmer(happy ~ trial * block.num * demand + 
            (1 | sub),
          data = DF.s2)

## omnibus tests
m.d.aov <- anova(m.d)

# simple effect of pose within each level of demand
m.d.joint <- emmeans(m.d, pairwise ~ trial | demand)$contrasts %>% 
  as.data.frame()
```

Following @coles2022fact, we fit a mixed-effect regression with (a) facial pose, demand characteristics, and block number entered as effect-coded factors and (b) random-intercepts for participants. Results indicated that participants reported higher levels of happiness after posing happy vs. neutral expressions, *M~diff~* = `r -(m.d@beta[2])`, *F*(`r m.d.aov["trial",]$NumDF`, `r m.d.aov["trial",]$DenDF`) = `r m.d.aov["trial", 5]`, *p* `r printp(m.d.aov["trial", 6])`. Furthermore, this effect was more pronounced in the positive (*M~diff~* = `r m.d.joint[m.d.joint$demand == "pos", ]$estimate`) vs. nil (*M~diff~* = `r m.d.joint[m.d.joint$demand == "nil", ]$estimate`) demand conditions, *F*(`r m.d.aov["trial:demand",]$NumDF`, `r m.d.aov["trial:demand",]$DenDF`) = `r m.d.aov["trial:demand", 5]`, *p =* `r printp(m.d.aov["trial:demand", 6])`.

```{r s2.int.others, include = F}
# delete vestigial from last moderator analysis
rm(m.d, m.d.aov, m.d.joint)

# moderation by motivation
m.m.aov <- lmer(happy ~ trial + block.num + mot + 
                  (trial : mot) +
                  (1 | sub),
                data = DF.s2) %>% 
  anova()

# moderation by opportunity
m.o.aov <- lmer(happy ~ trial + block.num + opp + 
                  (trial : opp) +
                  (1 | sub),
                data = DF.s2) %>% 
  anova()

# moderation by belief
m.b <- lmer(happy ~ trial + block.num + bel + 
              (trial : bel) +
              (1 | sub),
            data = DF.s2)

m.b.aov <- m.b %>% anova()

```

Most importantly, we replicated the pattern of results from Study 1b. For each moderator (motivation, opportunity, and belief), we fit a mixed-effect regression containing (a) facial pose and block number as effect-coded factors, (b) the moderator entered as a continuous variable, (c) a higher-order facial pose by moderator interaction term, and (d) random intercepts for participants. We did not find that the effect of facial poses on happiness was moderated by ratings of motivation to confirm the hypothesis (*F*(`r m.m.aov["trial:mot",]$NumDF`, `r m.m.aov["trial:mot",]$DenDF`) = `r m.m.aov["trial:mot", 5]`, *p =* `r printp(m.m.aov["trial:mot", 6])` or opportunity to adjust responses (*F*(`r m.o.aov["trial:opp",]$NumDF`, `r m.o.aov["trial:opp",]$DenDF`) = `r m.o.aov["trial:opp", 5]`, *p* = `r printp(m.o.aov["trial:opp", 6])`. We did, however, find that the effect of facial poses was moderated by participants' beliefs about the hypothesized effect (*F*(`r m.b.aov["trial:bel", ]$NumDF`, `r m.b.aov["trial:bel", ]$DenDF`) = `r m.b.aov["trial:bel", 5]`, *p =* `r printp(m.b.aov["trial:bel", 6])`. Specifically, the facial feedback effect was larger among participants who more strongly believed in the effect, $\beta$ = `r -(m.b@beta[2])`. This result is consistent with evidence of placebo effects in three larger studies reported in @coles2022fact.

# General Discussion

A comprehensive examination of strict experimental manipulations of demand characteristics reveal that they typically lead participants to slightly shift their responses in the direction of the communicated hypothesis. However, publication bias analyses are inconclusive and the estimated effects are heterogeneous. Using admittedly arbitrary thresholds, we estimated that `r h.c`% of demand characteristics manipulations produce these hypothesis-consistent shifts in participants' responses ($d$ \> 0.10). `r h.i`% produce hypothesis-*in*consistent shifts in participants' responses ($d$ \< -0.10), and `r 100 - h.c - h.i`% produce negligible shifts in either direction (-0.10 \< $d$ \> 0.10). Most worrisome, the current estimated distribution of demand effects suggests that they can range from approximately $d$ = `r dist.min` to $d$ = `r dist.max`. This range covers the magnitude of almost every conceivable effect in experimental psychology. Thus, in order to distinguish theory-relevant effects from artifactual demand effects, it is essential that experimental psychologists better understand how the latter work.

An examination of the evidence we were able to synthesize provides a surprising clue about how demand characteristics bias participant responses. Contrary to virtually every pre-existing theoretical framework, we did not find evidence that demand effects are driven by response bias. More specifically, we did not find that two factors theorized to underlie response biases---motivation and opportunity to adjust responses---moderated the effects of experimentally-manipulated demand characteristics. Instead, our results are consistent with a more parsimonious view: that demand effects are driven by participant beliefs---i.e., placebo effects. However, our investigation of underlying mechanisms was based on single-item measures that were either (a) completed by an external set of participants who reviewed short descriptions of studies in the meta-analysis (Study 1b), or (b) collected in a single experiment that may not generalize to other study contexts (Study 2). Thus, although the results provide evidence that demand characteristics can produce placebo-related shifts in participants' responses, it would be premature to dismiss other potential mediators (e.g., ones that produce response biases).

Additional moderator analyses indicated that some methodological decisions---such as sampling students, running studies in-person, and not offering payment---are associated with increases in hypothesis-consistent responding. However, student status is the only one of those moderators that was robust in sensitivity analyses. We also found that demand characteristics tended to be more impactful when a nil (as opposed to negative or positive) hypothesis is communicated. Surprisingly, we found that participants had little-to-no ability to predict how demand characteristics would impact other participants, although that it is possible that their performance would improve with more information.

## Implications for conceptualizations of participant roles

```{r part.roles, include = F}
# clean up environment
rm(m.b.aov, m.m.aov, m.o.aov, m.b,
   dist.max, dist.min, h.c, h.i, survey.n,
   tau, vig.n, DF.s2)

# open data
vig.DF <- read.csv("data/metaware_vignette_clean.csv")
```

In his pioneering work on demand characteristics, @orne1962social characterized participants as "good subjects" who are motivated to help the researcher confirm their hypothesis. Our results---although not without their limitations---do not suggest that this is a prominent participant goal. Indeed, across all experimental contexts reviewed by Study1b participants, the estimated mean motivation to help confirm the study hypothesis was near zero (*M* = `r mean(vig.DF$m.mot)`, *SD* = `r sd(vig.DF$m.mot)`). To be sure, there were some study contexts in which participants tended to report slight motivation to confirm the experimenter’s hypothesis, such as when they were told the researcher expected them to (a) prefer a news article that favors their political party (*M* = `r vig.DF[vig.DF$vig == "16_p_new", ]$m.mot`), (b) avoid perceiving a shift in an optical illusion (*M* = `r vig.DF[vig.DF$vig == "03_n_swi", ]$m.mot`), and (c) feel moved by happy and sad music (*M* = `r vig.DF[vig.DF$vig == "25_p_sad", ]$m.mot`). However, this can alternatively be interpreted as motivation to respond in a manner that is consistent with their beliefs---not motivation to help the experimenter. Indeed, participants were generally only motivated to confirm the hypothesis when it conformed with their own beliefs. This was evidenced by a modest-sized correlation between study-level estimates of participants' (a) motivation to confirm the hypothesis, and (b) belief in the hypothesis ($r$ = `r cor(vig.DF$m.mot, vig.DF$m.bel)`). These results are consistent with a placebo-based account of demand characteristics.

A placebo account---in retrospect---could accommodate findings from many classic studies that seemingly demonstrated participants' motivation to (a) help the experimenter, or (b) secure positive evaluations. For instance, when participants exhibited sham symptoms of hypnosis, @orne1962social concluded that the participants did so to please the experimenter. However, an alternative explanation is that these participants were merely acting in accordance with their beliefs about the [sham] symptoms of hypnosis. Similarly, when participants reduced performance on a simple task after being told that high performance was indicative of an obsessive-compulsive personality, @sigall1970cooperative concluded that participants did so to secure a positive evaluation. Once again, though, an alternative explanation is that these participants simply believed they did not possess a personality disorder and behaved accordingly.

Our results are most consistent with a placebo account that has been proposed as an extension---not a replacement---to frameworks that suggests demand characteristics produce response biases. Nonetheless, it seems likely that there are at least some contexts where participants are motivated to secure positive evaluations and/or help researchers confirm their hypotheses (even when controlling for beliefs). For instance, to avoid negative evaluations, participants may be unwilling to explicitly report racial biases---even if they (a) believe that the experimenter expects them to possess the bias, and (b) are consciously aware of the bias. Indeed, other reviews have provided evidence of this "socially desirable' responding [@vesely2020social; @sedikides2010religiosity; but see @lanz2022social]. Conversely, there may be some contexts where participants are motivated to help the experimenter confirm their hypothesis---even if they don’t believe the hypothesis or think their response will impact how they’re evaluated. Although our meta-analysis does not provide robust evidence for these mechanisms, we caution against preemptively dismissing their potential impact.

## Future directions

In work originally published in 1969, @mcguire2009suspiciousness suggested that there are three stages to a methodological artifact: ignorance, coping, and understanding/exploitation. At that same time, McGuire suggested that research on demand characteristics was entering the third stage. Unfortunately, over 50 years since McGuire's initial publication, it would seem that only limited progress through this third stage has been made. Indeed, we found very few direct tests of the mechanisms believed to underlie demand effects. Furthermore, our attempts to test these mechanisms through external ratings and a small replication study largely failed to support pre-existing demand characteristic frameworks. 

If researchers hope to progress through this third and final stage, we suggest that (1) theories will have to be refined, (2) mechanisms will have to be directly probed, and (3) reform for increasing the trustworthiness of study results will have to be implemented.

Theoretically, it is no longer tenable to keep demand characteristics conceptually divorced from related work on placebo effects. Consistent with recently proposed extensions of demand characteristic frameworks, our meta-analysis and replication study most strongly support *participants'* *beliefs* as a driver of demand effects [@coles2022fact; @corneille2022sixty]. This may occur because demand characteristics activate pre-existing beliefs about a phenomenon being investigated---but it is also possible that they cause participants to update pre-existing beliefs or form new beliefs. If true, research on how beliefs are formed, updated, and impact participant responses may help explain the unreliable effects of demand characteristic manipulations. For example, if beliefs are governed by Bayesian principles [for a review, see @kube2021beliefs], demand characteristics should exert smaller effects in contexts where participants’ are relatively certain of their pre-existing beliefs.

Methodologically, the mechanisms believed to underlie demand effects will have to be more directly probed through measurement and manipulation. For instance, similar to Study 2, researchers investigating demand characteristics could measure the extent to which participants believe the hypothesized effect and are motivated to (a) help the experimenter, (b) secure a positive evaluation, and/or (c) adjust their responses. These potential mechanisms could also be manipulated. For example, researchers could manipulate participants' motivation to help the experimenter by providing financial incentives for doing so [@mummolo2019demand].

Implementation-wise, we urge future demand characteristic researchers to engage in open science practices. Records of unpublished or "file-drawered" studies would help our conflicting evidence regarding the existence and impact of publication bias. . Access to open materials would better enable researchers to resolve discrepancies between previous studies through replication efforts [@coles2018costs; @zwaan2018making]. Last, open data and code would better allow researchers to verify published results, reproduce analytic workflows, and explore new questions through secondary analyses.

# Conclusion

We began our paper by mocking Crankology: a fictitious discipline plagued by a methodological artifact that could bias results in any direction, had unreliable effects, and had poorly understood mechanisms of action. However, our quantitative examination of a textbook methodological concern in experimental psychology---demand characteristics---raises humbling questions about the superiority of our own scientific endeavors. After all, the evidence we were able to synthesize indicates that demand characteristics also (a) can bias participant responses in any direction, (b) have heterogeneous effects, and (c) have somewhat unclear mechanisms of action. Contrary to conventional beliefs in the demand characteristics literature, our results provide evidence that placebo-based mechanisms may play a much larger role than previously believed. However, such conclusions are ultimately preliminary given the high heterogeneity, inconclusive publication bias analyses, and primitive measures of potential underlying mechanisms [@flake2020measurement].

The estimated range of demand effects covers the span of almost every conceivable effect in experimental psychology. After taking stock of what we know about demand characteristics, we think it is *reasonable* to question whether the potentially valid methods of experimental psychology are distinguishable from the clearly invalid methods of Crankology. Despite our own crankiness, we remain somewhat optimistic---if, that is, researchers will continue to work


# To-do
[] Double check prediction scores.
[] Rosnow and Rosenthal were right that there can be counter and regular acquiesence.
[] can placebo account for superiority of nil demand manips?

[] You need to put parameter estimates into Study 2 and make it clear that it's very possible that more evidence would have been found with a larger sample

[] You need to enable Type 3 SS for Study 2 onward. Also, to correctly interpret the interaction, I'm pretty sure you need to center the continuous variable

[] Fix bug that is causing figures to not correctly display

[] Look at whether motivation and belief are linked in S2. If not, mention that in the discussion

\-\--

[] F-value looks off here "Instances where a demand characteristic condition was compared to a control group allowed us to additional test whether participants respond more strongly to positive, nil, or negative demand characteristics."

\-\--

[] Clean up folder structure

[] Codebooks

# References

::: {#refs custom-style="Bibliography"}
:::
