---
title             : "A demanding problem: Meta-analysis suggests that demand characteristics can be consequential, unreliable, and difficult to explain"
shorttitle        : "Demand characteristics meta-analysis"
author: 
  - name          : "Anonymous for peer review (NAC)"
    affiliation   : "1"
    corresponding : yes
    address       : "Anonymous for peer review" 
    email         : "Anonymous for peer review"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Data Curation
      - Formal Analysis
      - Investigation
      - Methodology
      - Project administration
      - Software
      - Supervision
      - Visualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Anonymous for peer review (MW)"
    affiliation   : "1"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Data Curation
      - Investigation
      - Project administration
      - Software
      - Writing - Review & Editing
  - name          : "Anonymous for peer review (MCF)"
    affiliation   : "2"
    role:
      - Formal Analysis
      - Investigation
      - Methodology
      - Project administration
      - Resources
      - Software
      - Supervision
      - Visualization
      - Writing - Review & Editing
affiliation:
  - id            : "1"
    institution   : "Anonymous for peer review"
  - id            : "2"
    institution   : "Anonymous for peer review"
authornote: |
  
  
  Public significance statement: A fundamental methodological concern in research with human participants is that their responses are biased by information that convey the study hypothesis (i.e., demand characteristics). In a meta-analysis, we combined evidence from 53 studies that experimentally tested this concern by explicitly manipulating cues about the study hypothesis. Results suggested that explicit information about the researcher’s hypothesis produce biases in participants’ responses that can be potentially large – but are troublingly unreliable and difficult to explain.
  All materials, data (raw and processed), code, and pre-registrations are openly available at https://osf.io/3hkre/?view_only=2dc92af53f194e5eab0d7aecafaf01c2. This work was supported by the John Templeton Foundation (grant # anonymous for peer review). The funder had no role in the preparation of the manuscript or decision to publish. We thank (1) (anonymous for peer review; AC)  for assistance with code review, and (2) (anonymous for peer review; JB) for assistance developing the initial literature search strategy.
abstract: |
  Demand characteristics are a fundamental methodological concern in experimental psychology. Yet, little is known about the direction, magnitude, consistency, and mechanisms underlying their effects. We conducted a three-level meta-analysis of 253 effect sizes from 53 studies that provided experimental tests of demand effects by explicitly manipulating cues about the study hypothesis. These manipulations tended to produce small overall increases in hypothesis-consistent responding (*d* = 0.20, 95% CI [0.11, 0.30]). However, effects were extremely heterogeneous (between-study $\tau$ = 0.29; within-study $\sigma$ = 0.18), with the estimated distribution of population effects ranging from *d* = 1.82 (a massive increase in hypothesis-consistent responding) to *d* = -1.33 (a massive *decrease* in hypothesis-consistent responding). Both the observed and estimated distribution of these effects suggested that demand characteristics can create false positives, false negatives, upward bias, and downward bias. This heterogeneity is difficult to explain. New participants who reviewed key study details were neither able to predict nor provide insights into psychological mechanisms theorized to underlie demand effects. Participants ratings of three theorized moderators --  motivation to adjust responses, opportunity to adjust responses, and belief in the researcher's hypothesis  -- failed to predict observed demand effects. Coded methodological features (e.g., whether participants were paid) also often failed to predict observed effects. Although the meta-analysis did not capture the full depth of the demand characteristics construct, the synthesis of even a narrow subset of the literature suggests that their effects can be inferentially consequential, unreliable, and difficult to explain.
  
keywords          : "demand characteristics, expectancies, meta-analysis, methodology, confound"
bibliography      : "r-references.bib"
nocite: | 
  @orne1964contribution; @kersbergen2019hypothesis; @martin2018attention; @hoogeveen2018did; @rose2014choice; @durgin2012social; @busch2007follow; @cramer2005effect; @kanter2004experimental; @coles2022fact; @mummolo2019demand; @allen2012demand; @isager2022student; @terhune2006induction;  @cramer1995effect; @barabasz1991effects; @veitch1991demand; @allen2012demand; @kenealy1988validation; @smith1986influence; @siegel1982influence; @lamberth1971similarity; @polivy1980laboratory; @earn1979experimental; @perry1978demand; @mcginley1975subject; @mcglynn1972experimental; @verpaelst2007demand; @standing2008demonstration; @verpaelst2007demand; @lana1971subject; @schauer1969demand; @cramer2004effect; @larsen2011further; @tsai2018great; @coles2023replication; @fresson2017role; @balze1998role; @palomba1995dissociation; @coles2024replication
floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
annotate_references : yes
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include = F}
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")

# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")

# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")

# identify paper references
r_refs("r-references.bib")

# turn scientific notation off
options(scipen = 999)

# set seed to year of lead author's favorite [unfinished] album, SMiLE
set.seed(1967)

# set theme
theme_set(theme_classic())
```

```{r session.info, eval = F}
# note: this code is set to not evaluate. 
# for computational reproducibility purposes, the code exports package version info to a text file
# this will help others see which package versions were used when the code was written
writeLines(text = sessionInfo() %>% 
             capture.output(), 
           con = "sessionInfo.txt")
```

Imagine that one day a mysterious person approaches you and begins telling you about a new method they invented for understanding humans. They tell you that their method is useful for estimating causal relationships, but add that there is one issue: it can sometimes be thrown off by a *methodological artifact*. They explain that this artifact sometimes causes researchers to detect an effect that's not real, and other times causes them to miss an effect that is real; that it sometimes biases estimates upward and other times downward. Then, they offer a confession: the artifact doesn't always impact their conclusions -- and their effects still remain difficult to predict and explain after over a half century of research.

If the above scenario was real, the noted limitations would likely call their whole method into question. However, in the present meta-analytic review, we advance an unexpected conclusion: *demand characteristics* present psychologists with a troublingly similar set of methodological challenges.

In a seminal paper published over a half century ago, Martin Orne argued that human subjects are perceptive to demand characteristics -- "cues which convey an experimental hypothesis" -- and generally use these cues to help the experimenter confirm their hypothesis [-@orne1962social, p. 779]. Orne initially presented evidence that demand characteristics can lead to false positives, such as patients exhibiting sham symptoms of hypnosis [@orne1959nature]. However, demand characteristics can also lead to false negatives. For example, participants will ignore visual cues of depth when they believe that disregarding them is the purpose of the experiment [@hayes1967two]. In addition to creating inferential errors, demand characteristics can bias estimates of causal relationships. For example, the effects of facial poses on self-reported emotion can be amplified *or* attenuated depending on whether the experimenter communicates expectations of positive or nil effects [@coles2022fact]. Puzzlingly, though, demand characteristics do not always seem to matter. For example, in a set of large replications of classic studies in behavioral economics, direct manipulations of demand characteristics consistently failed to significantly impact participants' responses [@mummolo2019demand].

As this brief review shows, demand characteristics are uncomfortably close to the mysterious methodological artifact described in the opening of the paper. Demand effects are a literal textbook methodological concern in experimental psychology [@sharpe2016frightened]. However, their magnitude, direction, and consistency remain unclear. In the present paper, we use meta-analysis to take stock of what experimental psychologists have learned -- if anything -- about demand effects in the 50+ years since Orne influentially described them [@orne1962social]. We begin by briefly reviewing one of the most comprehensive and influential frameworkS describing their effects.

## How do demand characteristics alter participant responses?

One of the most influential frameworks for conceptualizing demand effects was developed by Rosnow and colleagues [@rosnow1997people; @rosnow1973mediation; @strohmetz2008research]. In this framework, they described three key moderators we discuss in the present work: (1) receptivity to cues, (2) motivation to provide hypothesis-consistent responses, and (3) opportunity to alter responses.

To start, Rosnow and colleagues reasoned that participants must be receptive to demand characteristics for there to be subsequent shifts in participants' responses (see also, Orne, 1958). As an extreme example, imagine that a researcher hands an infant a sheet of paper that precisely explains the study hypothesis. Demand characteristics are certainly present, but they are not predicted to have an impact because the infant is not receptive to the cues. Even if the infant possessed the astonishing ability to read, it's possible they would misunderstand the cues [@corneille2022sixty] -- which we will consider another form of non-receptivity in the present work.

If participants correctly interpret demand characteristics, Rosnow and colleagues theorized that subsequent changes in participants' responses would be driven by their motivation (or lack thereof) to provide hypothesis-consistent responses. Early work on demand characteristics was marked by debates about whether participants are motivated to adjust their responses to (a) help the researcher confirm their hypothesis [@orne1962social], (b) receive positive evaluations [@riecken1962program; @rosenberg1969conditions; @sigall1970cooperative], (c) interfere with the purpose of the study [@cook1970demand; @masling1966role], or (d) follow directions as closely as possible [@fillenbaun1970more]. Rosnow and colleagues advanced this line of thinking by illustrating that participants have *multiple* shifting motivations in mind when they conceptualize their roles as subjects [@rosnow1997people; see also @silverman1965demand]. For example, participants appear to be motivated to increase performance on simple tasks when told that this is the experimenter's expectation -- but not when the experimenter adds that the increase in performance will be indicative of a negative personality trait [@sigall1970cooperative]. Rosnow and colleagues, thus, suggested that participants in any given context can be characterized as being overall motivated to either: (a) non-acquiesce (i.e., not change their responses based on knowledge about the hypothesis), (b) acquiesce (i.e., provide hypothesis-consistent responses), or (c) counter-acquiesce (i.e., provide hypothesis-inconsistent responses).

If participants are motivated to adjust their response, Rosnow and colleagues theorized that subsequent changes in participants' responses would then be driven by their ability to alter the outcome of interest. As elaborated by @corneille2022sixty, this could occur through faking, imagination, or phenomenological control (voluntary changes experienced by the participant as involuntary). Taking this third moderator -- opportunity -- into account, Rosnow and colleagues concluded that demand characteristics bias responses when participants (1) notice the cues, (2) are motivated to adjust their responses, and (3) can adjust their responses. This framework directly maps onto many psychologists' typical playbook for avoiding the impact of demand characteristics: use deception and/or unobtrusive procedures (reduce receptivity), incentivize honest reporting (reduce motivation), and/or deploy difficult-to-control outcome measures (reduce opportunity to adjust responses).

Of course, other researchers have since expanded upon and/or challenged parts of Rosnow and colleagues' framework. For example, by elaborating upon underlying mechanisms like imagination, @corneille2022sixty more clearly highlight that participants can willingly change many outcomes that may initially seem outside their control. For example, a participant who wants to help a researcher confirm that a manuscript reviewing research artifacts is physiologically arousing could likely do so by simply imagining a physiologically arousing context. Relatedly, @coles2022fact argued that demand characteristics may sometimes impact participants in cases where they are *not* motivated to adjust responses – e.g., via conditioned responses or other mechanisms discussed in conceptually-related work on placebo effects [@stewart2004placebo]. We focus our review on Rosnow and colleagues' influential framework, but we revisit complementary ideas throughout.

# Methodology

The goal of the current paper is to quantitatively take stock of what experimental psychologists have learned -- if anything -- about demand effects in the 50+ years since Orne influentially described them [@orne1962social]. Although several excellent *narrative reviews* exist [@rosnow1997people; @corneille2022sixty; @sharpe2016frightened; @strohmetz2008research], meta-analysis allows us to quantitatively evaluate the magnitude, consistency, and potential moderators of demand effects.

We defined the scope of the meta-analysis using the Population, Intervention, Comparison, Outcome framework [@schardt2007utilization]. Our population-of-interest was human subjects participating in non-clinical psychology experiments. Given that there is a sizable literature and number of reviews on conceptually-related placebo effects, excluding clinical studies improved the feasibility and reduced the redundancy of our work.

The intervention-of-interest was explicit manipulations of the hypothesis communicated to participants -- i.e., scenarios where a researcher tells participants about the effect of an independent variable on a dependent variable. Demand characteristics are sometimes defined as *any* cue that may impact participants' beliefs about the purpose of the study, including instructions, rumors, and experimenter behavior [@orne1962social]. However, such a definition creates a potentially boundless conceptual space where *any* systematic change in a research design might be considered a test of demand characteristics. Furthermore, our own review of the literature revealed that researchers tend to study such effects by manipulating the study hypothesis *explicitly* communicated to participants. For these reasons, the present review focuses on relatively explicit manipulations of the study hypothesis. (See *Limitations* section for further reflection.)

Our comparison-of-interest were conditions where either no hypothesis or a different hypothesis was communicated to participants. Our outcome-of-interest was the dependent variable described in the communicated hypothesis. For example, in a study that manipulated whether the intervention is described as "mood-boosting", the outcome-of-interest would be any measure of mood.

### Literature search

```{r literature search, include = F}
# open and process literature search data
DF.s <- 
  # open data
  read_xlsx(path = "data/metaware_EsData_raw.xlsx",
            sheet = "records.screening") %>% 
  
  # identify:
  # unpublished dissertations by identifying links that contain the word 'dissertation' AND
  # records identified manually by identify links that contain 'NA'
  mutate(unpub = 
           if_else(condition = grepl("dissertation", link) |
                     link == "NA",
                   true = 1,
                   false = 0)
         )

# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pub <- DF.s %>% 
  filter(unpub == 0) %>% 
  nrow()

# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>% 
  filter(unpub == 1) %>% 
  nrow()
```

```{r prisma.info, include = F}
records.per.search <- DF.s %>% 
  group_by(database) %>% 
  summarise(n = n())
```

Figure \@ref(fig:prisma) provides a PRISMA-style flowchart summarizing our literature search and screening process [@page2021prisma].

The literature search was initially developed in consultation with a librarian at (anonymous for peer review) and later expanded based on reviewer feedback. On January 12, 2022, we searched APA PsycInfo using broad search terms: "demand characteristics" OR "hypothesis awareness" (n = `r records.per.search[1,2]` records identified). On April 17, 2024, we repeated the search to identify records published after the initial search (n = `r records.per.search[4,2]` records identified). At that time, we also expanded the search to include conceptually similar terms found in the appendix of Rosnow and Rosenthal's [-@rosnow1997people] book on experimental artifacts: “participant role” OR “demand effects” OR “good subject effect” OR “expectancy effect” OR “evaluative apprehension” (n = `r records.per.search[3,2]` records identified). We also released a call for unpublished studies on the Society for Personality and Social Psychology Open Forum, Twitter, the Facebook Psychological Methods Discussion group, and the Facebook PsychMAP group (n = `r records.per.search[2,2]` records identified).

Our search did not have language restrictions and went as far back as 1840, which yielded `r r.pub + r.unp` records, `r r.unp` of which were unpublished.

```{r prisma, fig.cap = "PRISMA-style flowchart illustrating the identification, screening, and selection of studies."}
knitr::include_graphics("images/metaware_PRISMA.png")
```

### Screening

```{r final.df, include = F}
# open clean effect size data
DF.es <- 
  read_csv(file = "data/metaware_meta_clean.csv")

# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>% 
  unique() %>% 
  length()

# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>% 
  unique() %>% 
  length()

# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>% 
  filter(id.study == 18) %>% 
  summarise(max.es = min(es)) %>% #  using min because largest value is neg
  round(2)
```

Put simply, records must have met the following criteria in order to be eligible for inclusion in the meta-analysis:

-   The researcher manipulated what participants were told about the effect of an independent variable on a dependent variable.[^1] In most cases, the effect of the independent variable was described explicitly, but there were some included studies where it was strongly implied.

-   The demand characteristics manipulation was not strongly confounded with another manipulation. For example, we excluded a study by @sigall1970cooperative because the manipulation of the stated hypothesis was confounded with a disclosure about the meaning of the behavior (i.e., that confirming the hypothesis would be indicative of an obsessive-compulsive personality disorder).

-   A non-clinical population was studied.

-   Information necessary for computing at least one effect size was included.

[^1]: We excluded conditions where the researcher communicated a *non-directional* effect. We did so because participants in these scenarios could not unambiguously infer how their responses were expected to change. For example, if participants were told that an independent variable would "impact mood", it is not clear if participants should infer that the mood will be boosted or dampened.

Figure \@ref(fig:prisma) more thoroughly summarizes exclusion criteria. In instances where multiple exclusion criteria applied, coders were asked to choose only one option.

N. C. and M. W. screened records independently, reviewed potentially relevant records together, and worked together to code the information for moderator analyses and effect size computations. Any disagreements were resolved through discussion. Abstracts and (if necessary) full texts were reviewed in a single step so that records did not have to be reviewed twice during screening. In total, `r num.s` studies from `r num.p` records were eligible for inclusion. However, one record [@allen2012demand] was removed because the information provided led to implausibly large effect size estimates (e.g., $d$ = `r outlier.es`).

```{r clean.env.1, include = F}
# remove outlier and re-initialize id factors
DF.es <- DF.es %>% 
  filter(id.study != 18) %>% 
  mutate(id.study = factor(id.study),
         id.es = factor(id.es))

# clean environment
rm(DF.s, r.pub, r.unp, num.s, 
   num.p, outlier.es, records.per.search)
```

### Effect size index

We used standardized mean difference scores with small-sample correction (Hedge's $g$) as our effect size index [@borenstein2009effect; @cohen1988statistical].

In most scenarios, we estimated the main effect of explicit demand characteristics. For example, @coles2022fact manipulated whether participants were told that posing smiles would increase happiness. Here, the main effect of explicit demand characteristics can be computed by comparing happiness ratings from smiling participants who were either informed or not informed of the mood-boosting effect of smiling. 

In some scenarios, we estimated the *interactive* effect of explicit demand characteristics. For example, in the same @coles2022fact study, participants provided happiness ratings both after smiling and scowling. Participants' mood generally improved when smiling vs. scowling (i.e., there was a main effect of facial pose). However, the difference was more pronounced when participants were told about the mood-boosting effects of smiling. In other words, there was an interaction between facial pose and explicit demand characteristics. In this scenario, the interactive effect of explicit demand characteristics was computed by calculating a standardized difference-in-differences score.

Effect sizes were calculated so that positive values indicated an effect consistent with the communicated hypothesis. For example, if participants were told that an intervention should be mood boosting, an increase in mood would be coded as a positive effect. If, however, participants were told that the intervention should be mood *dampening*, that same increase in mood would be coded as a negative effect.

```{r corr.sens, include = F}
# examine how assumed repeated measures correlation impacts general pattern of results

# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")

# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <- 
  sapply(X = sens.df.list, 
         FUN = function(i){
           # open data
           df <- read.csv(paste0("data/r_sensitivity/",
                                    i)
                             ) 
           # fit model
           m <- rma.mv(yi = es,
                       V = es.var,
                       data = DF.es,
                       random = ~ 1 | id.study / id.es) %>% 
             robust(x = .,
                    cluster = id.study,
                    clubSandwich = T)
           
           # return overall es as a number
           m$b %>% 
             as.numeric() %>% 
             return()
           }
         )

# compute range of es values
sens.range <- max(sens.res) - min(sens.res)

# delete vestigial
rm(sens.df.list, sens.res)
```

We calculated Hedge's $g$ by applying a small sample correction to Cohen's $d_{s}$ (for between-subject designs) and $d_{rm}$ (for within-subject designs) estimates. Whenever possible, we used the *M*'s and *SD*'s reported in a paper to compute Cohen's *d*. If these values were not reported, we used (in order of preference), (1) *t*-values, (2) descriptive statistics extracted from figures (e.g., bar charts) using the WebPlotDigitizer [@drevon2017intercoder], (3) *F*-values, or (4) *p*-values. In instances where relevant information was not provided but the significance and direction of the effect was described, we assumed *p*-values of .04 and .50 for significant and non-significant effects respectively [e.g., @kenealy1988validation]. In a few instances, an outcome variable in a study was discrete, as opposed to continuous [e.g., @orne1964contribution]. In these cases, we approximated a Cohen's *d* score based on a transformation of the log odds ratio [@borenstein2011introduction].

For repeated-measure comparisons, the correlation between the repeated measures is needed to calculate Cohen's $d_{rm}$. This correlation is rarely reported, so we followed a recommendation by @borenstein2009effect and performed sensitivity analyses on an assumed correlation. We preregistered a default correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses produced virtually no change in overall effect size estimates, so we do not discuss them further.

```{r mult.eff, include = F}
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>% 
  # identify number of effect sizes for each study (id)
  group_by(id.study) %>% 
  count() %>% 
  
  # code whether each study has more than one effect size
  mutate(dep = if_else(condition = n > 1,
                       true = 1,
                       false = 0)
         ) %>% 
  
  # calculate proportion of studies with more than one effect size 
  ungroup() %>% 
  summarise(mult.eff = mean(dep)) %>% 
  
  # export as percentage
  as.numeric() %>% 
  round(digits = 2) * 100
```

Nearly all studies (`r mult.eff.per`%) contained multiple effect sizes of interest. For example, the full design in @coles2022fact included a positive demand, nil demand, and control condition. Participants also completed several facial expression poses (happy, angry, and neutral) and self-reported several emotions (happiness and anger). To be comprehensive, we recorded all reported effect sizes and accounted for dependencies using three-level meta-analysis (described later).

### Potential study feature moderators

The studies we included in our meta-analysis were methodologically varied (for more information, see *Results* and *Limitations*). Below, we describe study features we coded as potential moderators of demand effects:

-   *Group comparison.* Most studies included in our meta-analysis examined the effects of *positive demand*, wherein participants were told that the dependent variable will increase. However, a notable subset of studies examined the impact of *negative demand* (participants told that the dependent variable will decrease) or *nil demand* (participants told the dependent variable will be unaffected). Often these conditions were compared to a *control* condition, wherein participants were not told about an effect of an independent variable on a dependent variable. Sometimes, though, one demand condition was compared to another.

-   *Control vs. non-control group comparison.* Demand effects should presumably be additive. For example, imagine a study where the effect of a task is either (a) not described at all (a control condition), (b) described as mood-boosting (positive demand) or (c) described as mood-dampening (negative demand). Further imagine that participants are motivated and able to adjust their responses. Compared to the control condition, participants' moods are predicted to be boosted in the positive demand condition and dampened in the negative demand condition. If this is the case, the mean difference in mood should be larger when the positive demand condition is compared to the negative demand condition (as opposed to the control condition). To test this, we coded whether comparisons were made to a control group or a different demand condition.

-   *Control group comparison.* Instances where a demand characteristic condition was compared to a control group also allowed us to test whether participants' responses shift more when the researcher hypothesizes an increase (positive demand), a decrease (negative demand), or no change in the dependent variable (nil demand).

-   *Design of demand characteristics manipulation.* Whether demand characteristics were manipulation within- vs. between-subjects.

-   *Participant pool.* Whether students, non-students (e.g., MTurk workers), or a mix of students and non-students were sampled.

-   *Setting.* Whether the study was conducted online or in-person.

-   *Payment.* Whether participants were paid or unpaid.

-   *Publication status.* Whether the study was published or unpublished.

### Can participants help us understand demand effects?

During our literature review, we found very few papers that tested mechanisms that may help predict demand effects. We thus turned to a population that @orne1969demand believed may help researchers understand demand effects: participants. As recently reviewed by @corneille2023instruction, participants can successfully predict a variety of effects in experimental psychology, including the approach-avoidance effect, mere exposure effect, and the rubber hand illusion. When this occurs, it raises concerns that the original effect may have been driven by demand characteristics [@bartels2019revisiting]. Here, we attempt to extend this methodology not to raise concerns about participants' potential responses to demand characteristics -- but instead to evaluate whether they can explain *when* and *how* such effects operate.

As we describe below, we asked a new set of participants to review vignettes describing key details of studies included in the meta-analysis. We then solicited judgments of not only whether they believed demand effects would emerge, but also the extent to which they (a) correctly identified the communicated hypothesis, (b) would be motivated to adjust responses, (c) would be able to adjust responses, and (d) would believe the experimenter's hypothesis.

#### Vignette rating methodology

```{r vig.dem, include = F}
# identify total number of vignettes
vig.n <- read.csv(file = "admin/vig/metaware_VigCombined.csv") %>% 
  nrow()

# compile demographic info for the two vignette studies

# import data from first survey
vig.surv1 <- 
  read_csv(file = "data/metaware_SurvData_raw.csv") %>% 
  filter(Finished == 1) %>% 
  select(indiv_gend_var : ethnicity) %>% 
  mutate(survey = "survey_1")

vig.surv1.n = nrow(vig.surv1)

vig.surv2 <- 
  read_csv(file = "data/metaware_SurvData2_raw.csv") %>% 
  filter(Finished == 1) %>% 
  select(indiv_gend_var : ethnicity) %>% 
  mutate(survey = "survey_2")

vig.surv <- rbind(vig.surv1, vig.surv2)
rm(vig.surv1, vig.surv2)

# describe participant demographics
survey.n <- nrow(vig.surv)

survey.gend <- 
  vig.surv$indiv_gend_var %>% 
  table() %>% 
  prop.table() %>% 
  round(2) * 100

survey.eth <-
  vig.surv$ethnicity %>% 
  table() %>% 
  prop.table() %>% 
  round(2) * 100

survey.age.m <- 
  mean(vig.surv$indiv_agee_var %>% as.numeric(),
       na.rm = T) %>% 
  round(2)

survey.age.sd <- 
  sd(vig.surv$indiv_agee_var %>% as.numeric(),
     na.rm = T) %>% 
  round(2)

# put demographics into table
survey.dem = list(n = survey.n,
                  gend = survey.gend,
                  ethnicity = survey.eth,
                  age.m = survey.age.m,
                  age.sd = survey.age.sd)

# remove vestigial
rm(survey.n, survey.gend,
   survey.eth, survey.age.m, survey.age.sd)
```

```{r vig.rel, include = F}
# open reliability data
vig1.rel <- readRDS("output/vig.surv1.rel.rds")
```

For each study included in the meta-analysis after our original literature search[^3], we created vignettes that described the key details for each demand characteristic condition and dependent variable combination. For example, @standing2008demonstration had two demand characteristic manipulations (positive and negative demand) and two dependent variables (measures of verbal and spatial reasoning). Thus, we created four vignettes for this study (Figure \@ref(fig:vig)). In an effort to help participants understand the study context, vignettes also contained information about (a) whether students vs. non-students were sampled, (b) whether subjects received compensation, and (c) whether the study was conducted online or in-person.

[^3]: As a reminder, we performed two literature searches. The second literature search was inspired by reviewer feedback, which we received after we started collecting data using the vignette methodology.

```{r vig, fig.cap = "Vignettes for Standing et al. (2008), which described the key details for each demand characteristic condition (bolded and underlined) and dependent variable (bolded and italicized) combination."}
knitr::include_graphics("images/metaware_vigs.png")
```

In total, there were `r vig.n` vignettes. We did not create vignettes for control conditions because participants were not given information about the experimenter's hypothesis (i.e., there were no explicit demand characteristics to act upon).

Using a web-based Qualtrics survey, participants reviewed 10 randomly selected vignettes. Much like the sample in the studies they reviewed, these participants were a convenience sample. For each study, participants were asked to first identify the researcher's hypothesis. Here, participants chose between four options that described a filler effect (usually involving an irrelevant dependent variable) or a positive, negative, or nil effect of the independent variable on the dependent variable. Although not originally pre-registered, the proportion of participants who correctly identified the hypothesis in each vignette (0 - 1) will later be used to evaluate Rosnow and colleagues' proposed receptivity moderator.

Afterwards, participants rated the extent to which they would hypothetically (a) be motivated to adjust responses based on the researcher's stated hypothesis (-3 = "extremely motivated to adjust responses to be inconsistent" to 3 = "extremely motivated to adjust responses to be consistent"), (b) be able to adjust their responses on the outcome-of-interest (0 = "extremely incapable" to 4 = "extremely capable"), and (c) believe the hypothesized effect would occur (-3 = "strong disbelief" to 3 = "strong belief"). Participants also indicated the extent to which they expected other participants to adjust their responses to confirm the hypothesized effect (-3 = "extremely likely to adjust responses to be *inconsistent*" to 3 "extremely likely to adjust responses to be consistent"). These rating scales were presented in random order.

Sample size was initially based on availability of resources.[^4] We originally collected as much data as possible (n = `r vig.surv1.n`) in a single quarter from undergraduates from (anonymous for peer review). Following a reviewer recommendation, we performed post-hoc examinations of the reliability of their ratings. More specifically, we calculated intraclass correlations using mixed effects models. For ratings of predicted demand effects, motivation to adjust responses, opportunity to adjust responses, and belief in the hypothesized effect, we used the lme4 package [@R-lme4] in R [@R-base] to fit an intercept-only mixed effect model with random intercepts at the level of participant and vignette. We then used the performance package [@R-performance] to calculate the intraclass correlation for the participant random intercept. The intraclass coefficient for predicted demand effects (ICC = `r vig1.rel$pre$ICC[1]`), motivation to adjust responses (ICC = `r vig1.rel$mot$ICC[1]`), opportunity to adjust responses (ICC = `r vig1.rel$opp$ICC[1]`), and belief in the researcher's stated hypothesis (ICC = `r vig1.rel$bel$ICC[1]`) was low.

[^4]: For transparency, we would like to note that earlier analyses with our initial sample of participants suggested that observed demand effects were moderated by ratings of the extent to which they believed the researcher's stated hypothesis. This finding, however, did not replicate in our full sample.

The low intraclass correlations from our original sample indicates that participants strongly disagree about how they will respond to explicit demand cues. Nonetheless, the Law of Large Numbers stipulates that these relatively imprecise ratings should converge into relatively precise estimates of the true mean at larger samples. We attempted to exploit this statistical tendency by collecting additional ratings from Prolific workers. This left us with a total of `r survey.dem$n` participants (`r survey.dem$gend[1] %>% round(0)`% female; `r survey.dem$gend[2]`% male; all other participants indicated they were transgender, gender non-conforming, some other gender, or unwilling to disclose gender). `r survey.dem$ethnicity[1]`% of participants reported they were White/Caucasian, `r survey.dem$ethnicity[10] %>% round(0)`% Asian, `r survey.dem$ethnicity[7]`% Black/African American. All other participants declined to respond or indicated their ethnicity could not be described by a single (or any) provided category. The average participant age was `r survey.dem$age.m` ($SD$ = `r survey.dem$age.sd`).

```{r del.vest, eval = F}
rm(survey.dem, vig1.rel, vig.n)
```

##### Accounting for different demand comparisons

As mentioned before, Hedge's $g$ represents the standardized difference between *two* groups. Thus, for each observation in the meta-analysis, we summed participants' average motivation, opportunity, and belief ratings (after removing cases where they identified the wrong hypothesis). We also summed the estimates of how likely participants were to correctly identify the communicated hypothesis. Doing so allowed us to accommodate the fact that some comparisons involved two demand characteristics conditions. For example, imagine a study where participants are told a procedure will boost mood (positive demand), told a procedure will dampen mood (negative demand), or not told about an expected effect (control). Compared to a control condition, participants who are motivated to confirm the hypothesis are theorized to have upward-biased responses in the positive demand condition and downward-biased responses in the negative demand condition. If those demand conditions are compared to each other -- instead of a control condition -- their effects should be additive. Summing participants ratings allowed us to accommodate this possibility.

We did not include nil-hypothesis comparisons in our analyses because our coding strategy could not accommodate the potential moderating role of motivation and belief in these conditions. For example, imagine that a participant is (a) told that an intervention will not impact mood (nil demand), and (b) is motivated to disconfirm the hypothesis. Relative to a control condition, this participant could disconfirm the hypothesis by either increasing *or* decreasing their mood report. Thus, even if motivation does moderate the effects of demand characteristics, we would not expect a systematic pattern to emerge with our coding scheme.

### Quality ratings

Following a reviewer recommendation, N.C. coded the quality of each record included in the meta-analysis. To do so, a modified version of the @downs1998feasibility checklist was used. This original checklist contains ten items designed to evaluate reporting quality (e.g., "Are the main findings of the study clearly described?"), three to evaluate external validity (e.g., "Were the subjects asked to participate in the study representative of the entire population from which they were recruited?"), seven to evaluate internal validity (e.g., "Was an attempt made to blind those measuring the main outcomes of the intervention?), six to evaluate selection bias (e.g., "Were losses of patients to follow-up taken into account?"), and one to evaluate statistical power.

Many of the items in the @downs1998feasibility checklist are difficult to evaluate or inapplicable to the literature we evaluated[^5]. We thus focused our analysis on seven reporting quality, one external validity, and three internal validity items. Each item was coded as either a 1 ("yes"), 0 ("no") or NA ("not applicable"). For each record in the meta-analysis, the scores within each category of the checklist were averaged.

[^5]: The @downs1998feasibility checklist has often been endorsed as a measure of the quality of records included in meta-analyses [e.g., @johnson2021toward]. We do not share this endorsement; Many items were not applicable to the work we were evaluating (e.g., whether distributions of principle confounders are described). Many others were difficult to evaluate (e.g., whether the study had adequate statistical power).

### Meta-analytic approach

For our meta-analytic approach, we used three-level meta-analysis (3LMA; also referred to as "multilevel" meta-analysis). Rather than assume that there is a single true effect of demand characteristics, 3LMA assumes that there is a distribution containing *multiple true effects*. To separate variability in these true effects from mere sampling error, 3LMA models three sources of variability: sampling error of individual studies (level 1), variability within studies (level 2), and variability between studies (level 3; often referred to as "random effects").

We fit all models using the metafor package [@R-metafor] in R [@R-base]. We weighed effect sizes based on their inverse-variance and used cluster-robust methods for estimating variance-covariance matrices [@pustejovsky2018small]. To estimate the overall effect size, we fit an intercept-only 3LMA model. We conducted moderator analyses by separately entering variables into a new model. In doing so, we hoped to avoid issues with collinearity and overfitting. Categorical moderators were dummy coded. To test the significance of each moderator, we used model comparison *F*-tests. To estimate effect sizes within each subgroup of the moderator, we used model-derived estimates.

```{r clean.env.2, include = F}
# delete vestigial
rm(mult.eff.per, sens.range)
```

#### Publication bias analyses

Publication bias refers to the well-documented propensity for hypothesis-inconsistent findings to be disproportionately omitted from the published scientific record [@franco2014publication]. When present, publication bias can lead to inaccurate effect size estimates and inferential errors. Consequently, we used three main approaches for assessing and correcting for potential publication bias in our estimation of the overall effect of demand characteristics.

First, we visually examined *funnel plots,* wherein observed effect sizes are plotted against a measure of their precision (e.g., standard error). In the absence of publication bias, the distribution typically resembles a funnel; relatively large studies estimate the effect with high precision, and effect sizes fan out in *both* directions as the studies become smaller. If, however, non-significant findings are disproportionately omitted from the scientific record (i.e., there is publication bias), the distribution is often asymmetric/sloped. Funnel plots traditionally contain one effect size per study, but many of our studies included multiple relevant effect sizes. Thus, we examined two funnel plots: one with all effect sizes and one with the dependent effect sizes aggregated[^6].

[^6]: For effect size aggregation, we assumed a default dependent effect size correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses did not change our overall conclusion about publication bias, so we do not discuss them further.

Second, we conducted precision-effect tests [@stanley2014meta]. In precision-effect tests, the relationship between observed effect sizes and their standard errors -- which is often absent when there is no publication bias -- is estimated and controlled for in a meta-regression model. The slope of this model is often interpreted as an estimate of publication bias, and the intercept is often interpreted as the bias-corrected overall effect. These precision-effect tests were developed and validated for meta-analyses with independent effect sizes. Nonetheless, @rodgers2021evaluating demonstrated that the method retains fairly good statistical properties when (1) 3LMA is used, or (2) dependent effect sizes are aggregated and modeled using random-effects (i.e., two level) meta-regression. We used both approaches[^7].

[^7]: When assessing publication bias using 3LMA, we also fit an exploratory model that included cluster-robust estimates of the variance covariance matrix. Cluster-robust estimation procedures did not change our inferences.

Third, we deployed weight-function modeling using the weightR package [@R-weightr]. In weight-function modeling, weighted distribution theory is used to model biased selection based on the significance of observed effects [@vevea1995general]. If the adjusted model provides increased fit, publication bias is a concern and the model can be used to estimate the bias-corrected overall effect size. Once again, weight-function modeling was designed for independent effect sizes. Nonetheless, it has fairly good statistical properties when non-independent effect sizes are aggregated, which we did here [@rodgers2021evaluating].

As a sensitivity analysis, we used the PublicationBias package in R [@R-PublicationBias] to estimate the ratio in which publication bias would have to favor affirmative studies in order make the overall effect size in a robust random effects model non-significant [@mathur2020sensitivity]. We also estimated the difference in the magnitude of published vs. unpublished effects in a moderator analysis.

### Transparency and openness

The project pre-registration, materials, data, and code are openly available at <https://osf.io/3hkre/?view_only=2dc92af53f194e5eab0d7aecafaf01c2>. This link also contains a list of amendments/deviations we made to our pre-registration as the project evolved and reviewer feedback was received. These amendments were largely concerned with (a) whether and how many vignette ratings to collect, (b) whether to make vignette-related tests confirmatory vs. exploratory, (c) whether new primary data collected for a @coles2024replication record is described in the main text vs. summarized in the meta-analysis, (d) whether to code the quality of included records, (e) expansions to the literature search, (f) whether to use Cohen's *d* or Hedge's *g* as our effect size index, and (g) decisions regarding whether to use robust variance estimation, mixed-effects models, or both. 

For the meta-analysis, sample size was determined by the availability of relevant records. For the vignette ratings, sample size was initially determined by the availability of resources (i.e., we collected as much data as possible in a single quarter). However, our second wave of participant recruitment considered precision -- and was designed to (a) yield an equal number of participant ratings per vignette, and (b) decrease the length of the 95% confidence intervals of the predicted demand effect, motivation, opportunity, and belief ratings to 1.

Ethics approval was not requested for the meta-analysis because no new data were collected. The vignette rating study was reviewed and approved by the (anonymous for peer review) IRB (protocol #: anonymous for peer review; protocol title: anonymous for peer review).

All code has been checked for reproducibility, including the script used to generate a computationally reproducible manuscript using the papaja R package [@R-papaja].

## Results

```{r overall, include = F}
# estimate overall effect size
overall <- 
  rma.mv(yi = es,
         V = es.var,
         data = DF.es,
         random = ~ 1 | id.study / id.es) %>% 
  robust(x = .,
         cluster = id.study,
         clubSandwich = T)

# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])

# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
             mean = overall$b,
             sd = tau, 
             lower.tail = F) %>% 
  round(digits = 2) * 100

h.i <- pnorm(q = (-.10),
             mean = overall$b,
             sd = tau,
             lower.tail = T) %>% 
  round(digits = 2) * 100

# estimate lower and upper bound of effect size distribution
dist <- rnorm(n = 1000000,
              mean = overall$b,
              sd = tau)

dist.min <- dist %>% 
  min() %>% 
  round(digits = 2)

dist.max <- dist %>% 
  max() %>% 
  round(digits = 2)

rm(dist)
```

In total, we extracted `r overall$k` effect sizes from `r overall$n` studies from between the years `r DF.es$year %>% min()` and `r DF.es$year %>% max()` (*M* = `r DF.es$year %>% mean() %>% round(0)`, *SD* = `r DF.es$year %>% sd()`). `r DF.es %>% filter(published == 'no') %>% reframe(n.study = unique(id.study)) %>% nrow()` of these studies were unpublished. Ratings of reporting quality were modest (*M* *=* `r DF.es$report %>% mean(na.rm = T) %>% round(2)`, *SD* = `r DF.es$report %>% sd(na.rm = T) %>% round(2)`); ratings of internal validity were high (*M* *=* `r DF.es$internal %>% mean(na.rm = T) %>% round(2)`, *SD* = `r DF.es$internal %>% sd(na.rm = T) %>% round(2)`); and ratings of external validity were consistently `r DF.es$external %>% mean(na.rm = T) %>% round(2)`. The low external validity scores were driven by the reliance on non-representative sampling methods, an unfortunately common limitation in experimental psychology [@frank2023experimentology]. Quality ratings were not significantly associated with observed effect sizes and are not discussed further.

In order of frequency, effect sizes represented a positive demand compared to a control group (*k* = `r DF.es %>% filter(ref.type == 'cvp') %>% nrow()`), positive demand to negative demand (*k* = `r DF.es %>% filter(ref.type == 'pvn') %>% nrow()`), negative demand to a control group (*k* = `r DF.es %>% filter(ref.type == 'nvc') %>% nrow()`), positive demand to a nil demand group (*k* = `r DF.es %>% filter(ref.type == 'pvz') %>% nrow()`), or nil demand to a control group (*k* = `r DF.es %>% filter(ref.type == 'cvz') %>% nrow()`).

More broadly, effect sizes tended to compare one demand condition to a control group (*k* = `r DF.es %>% filter(ref.r == "single") %>% nrow()`) – as opposed to a group exposed to a different type of explicit demand cues (*k* = `r DF.es %>% filter(ref.r == "double") %>% nrow()`). Regardless of what type of demand manipulation was used, it was more common to manipulate the cues between (*k* = `r DF.es %>% filter(design == "between") %>% nrow()`) vs. within subjects (*k* = `r DF.es %>% filter(design == "within") %>% nrow()`).

Most effect sizes came from student samples (*k* = `r DF.es %>% filter(student == "yes") %>% nrow()`), although some samples were non-students (*k* = `r DF.es %>% filter(student == "no") %>% nrow()`), a mix of students and non-students (*k* = `r DF.es %>% filter(student == "mix") %>% nrow()`), or not described thoroughly enough to make a determination (*k* = `r DF.es %>% filter(is.na(student)) %>% nrow()`). Most effect sizes came from unpaid samples (*k* = `r DF.es %>% filter(paid == "no") %>% nrow()`), although some were paid (*k* = `r DF.es %>% filter(paid == "yes") %>% nrow()`) and some were not described thoroughly enough to make a determination (*k* = `r DF.es %>% filter(is.na(paid)) %>% nrow()`). The majority of effect sizes came from in-person studies (*k* = `r DF.es %>% filter(online == "no") %>% nrow()`), but some were from online studies (*k* = `r DF.es %>% filter(online == "yes") %>% nrow()`) or not described thoroughly enough to make a determination (*k* = `r DF.es %>% filter(is.na(online)) %>% nrow()`).

### Overall results

Overall, results indicated that explicit manipulations of demand characteristics cause participants' responses to shift in a manner consistent with the communicated hypothesis, $g$ = `r overall$b`, 95% CI [`r overall$ci.lb`, `r overall$ci.ub`], $t$(`r overall$dfs %>% round(2)`) = `r overall$zval`, $p$ `r printp(overall$pval)`. As a hypothetical example, if participants were told that the researcher hypothesizes that an intervention will improve mood (positive demand), they would generally report slightly improved moods; if told that the researcher hypothesizes that an intervention will worsen mood (negative demand), they would generally report slightly worsened moods.

```{r forest, fig.cap = "Forest plot of effect sizes (grey diamonds), their 95% confidence intervals (grey error bars), and their citations (left). For visualization purposes, effect sizes are aggregated within-studies (see openly-available data for non-aggregated effect sizes). The estimated effect size distribution is also shown and colored based on whether demand characteristics produce more hypothesis-consistent responding (green; g > 0.10), more hypothesis-inconsistent responding (red; g < -0.10), or negligible shifts in responding (grey; |g| < 0.10).", fig.height = 6.64, fig.width = 6.5, warning = F}
# create a temporary dataset that (a) aggregates dependent effect sizes, and (b) computes 95% CI's
tmp <- DF.es %>%
  # convert to an 'escalc' object so function can run
  escalc(yi = es,
         vi = es.var,
         data = DF.es,
         measure = "SMD") %>% 
  
  # delete vestigial: es is now yi; es.var is now vi
  select(-c(es, es.var)) %>% 
  
  # aggregate dependencies
  aggregate(x = .,
            cluster = id.study,
            rho = .5) %>% 
  
  # compute CI's
  rowwise() %>% 
  mutate(se = sqrt(vi),
         ub = yi + (se * 1.96),
         ub = round(ub, 2),
         lb = yi - (se * 1.96),
         lb = round(lb, 2),
         yi = round(yi, 2)) %>% 
  ungroup() %>% 
  arrange(yi, id.study)

# create a forest plot w/ distribution overlay
ggplot(data = tmp, 
       aes(y = rev(1: nrow(tmp)) * .05, 
           x = yi,
           xmin = lb,
           xmax = ub)) +
  
  #hypothesis inconsistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#F8766D", 
            alpha = .25,
            xlim = c(-2, -.10)) +
  
  # negligible effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "grey80", 
            alpha = .25,
            xlim = c(-.10, .10)) +
  
  # hypothesis consistent effects
  ## area
  geom_area(stat = "function", 
            fun = dnorm,
            args = list(mean = overall$b, 
                        sd = tau),
            fill = "#00998a", 
            alpha = .25,
            xlim = c(.10, 2)) +
  
  # create dotted line at d = 0
  geom_vline(xintercept = 0, 
             color = "black", 
             linetype = "dotted", 
             alpha = .5, 
             size =.5)  +

  # add points and error bars
  geom_point(shape = "diamond",
             size = 2.5,
             alpha = .8,
             color = "dark grey") +
  geom_errorbarh(height = .005,
                 size = .75,
                 alpha = .8,
                 color = "dark grey") +
  
  # add citation label
  geom_text(aes(label = citation),
            x = -3.15,
            hjust = 0,
            size = 2.5) +
  
  # add CI label
  geom_text(aes(label = paste0(yi,
                               " [", lb, ", ", ub, "]")),
            x = 3.6,
            size = 2.5,
            hjust = 1) +
  
  labs(x = expression(paste("Hedge's ", italic("g"))),
       y = "density") +
  
  # increase plotting area
  scale_x_continuous(limits = c(-3.25, 3.75),
                     breaks = seq(from = -2, to = 2, by = 1),
                     expand = c(.01, .01)) +
  scale_y_continuous(expand = c(.005, 0))

```

As a reminder, rather than assuming that there is a *single true effect* of demand characteristics, 3LMA assumes a distribution of *multiple true effects.* Consistent with this assumption, observed variability in demand effects drastically exceeded what would be expected from sampling error alone (between-study $\tau$ = `r sqrt(overall$sigma2[1])`; within-study $\sigma$ = `r sqrt(overall$sigma2[2])`; $Q$(`r overall$k - 1`) = `r overall$QE`, $p$ `r printp(overall$QEp)`). 3LMA often assumes that the multiple true effects form a normal distribution, which we recreated based on estimates of the average effect size and variability attributed to sources other than sampling error (between-study $\tau$ + within-study $\sigma$). As shown in Figure \@ref(fig:forest), this estimated distribution suggests that demand effects can range from approximately $g$ = `r dist.min` to $g$ = `r dist.max` --- covering the range of most conceivable effects in psychology.

```{r neg.demand, include = F}
# compute number of effect sizes that were negative and significant
neg.demand.raw <- DF.es %>% 
  filter(ub < 0) %>% 
  nrow()

# compute number of *aggregated* effect sizes that were negative and significant
neg.demand.agg <- tmp %>%
  filter(ub < 0) %>% 
  nrow()
  
rm(tmp)
```

As a heuristic, we arbitrarily classified any effect size less than 0.10 standard deviation in either direction (i.e., \|$g$\| \< .10) as "negligible". Based on this classification, the estimated distribution of effects suggested that demand characteristics most often produce hypothesis-consistent shifts (`r h.c`%), but sometimes produce negligible shifts (`r 100 - h.c - h.i`%) or shifts in the *opposite* direction of the communicated hypothesis (`r h.i`%). Such results are consistent with Rosnow and colleagues' prediction that demand characteristics can lead to both hypothesis-consistent and hypothesis-*inconsistent* shifts in participants' responses. However, both the observed effects and estimated distribution in Figure \@ref(fig:forest) suggest that hypothesis-inconsistent shifts in participants' responses are quite rare. The distribution of effect sizes aggregated at the study level reveal that significant hypothesis-inconsistent shifts in participants' responses were only observed in `r neg.demand.agg` case (Figure \@ref(fig:forest)). Even when considering non-aggregated effect sizes, significant hypothesis-inconsistent shifts in participants' responses were only observed in `r neg.demand.raw` cases.

```{r clean.env.99, include = F}
rm(neg.demand.raw, neg.demand.agg)
```

### Moderator analyses

```{r mod, include = F}
# create moderator analysis function
ModAnalysis = function(m, df = DF.es) {
  
  # set dataset
  df <- df
  
  # moderator analysis
  mod.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ ", m)),
                  test= "t") %>% 
    robust(x = .,
           cluster = id.study,
           clubSandwich = T)
  
  sub.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ 0 + ", m)),
                  test= "t") %>%
  robust(x = .,
         cluster = id.study,
         clubSandwich = T)
  
  # return results as list
  return(list(mod = mod.m,
              sub = sub.m)) 
}

# conduct moderator and subgroup analyses for moderators assessed with full dataset 
mod.l <- c("student", "paid", "online", 
           "design", "ref.r", "published",
           "year", #"att",
           
           # exploratory quality control measures
           'report', 'internal')
mod.r <- 
  sapply(X = mod.l,
         simplify = F,
         FUN = ModAnalysis)

rm(mod.l)

# test ref.type moderator in scenarios where there is a control comparison (i.e., ref.r == single)
mod.r[["ref.type"]] <- 
  ModAnalysis(m = "ref.type",
              df = DF.es[DF.es$ref.r == "single", ])

# test all ref.type
mod.r[["ref.type.full"]] <- 
  ModAnalysis(m = "ref.type")

# add attention, motivation, opportunity, belief, and prediction moderators
## Note: comparisons with nil-demand conditions are excluded
mod.r.2 <- 
  sapply(X = c("att", "mot", "opp", "bel", "pre"),
         simplify = F,
         FUN = ModAnalysis,
         df = DF.es %>% 
               filter(ref.type != "cvz",
                      ref.type != "pvz"))

# retest attention, motivation, opportunity, belief, and prediction moderators in situations where there is only a control group
mod.r[["att.control"]] <- 
  ModAnalysis(m = "att",
              df = DF.es %>% 
                filter(ref.r == "single",
                       ref.type != "cvz",
                       ref.type != "pvz"))

mod.r[["mot.control"]] <- 
  ModAnalysis(m = "mot",
              df = DF.es %>% 
                filter(ref.r == "single",
                       ref.type != "cvz",
                       ref.type != "pvz"))

mod.r[["opp.control"]] <- 
  ModAnalysis(m = "opp",
              df = DF.es %>% 
                filter(ref.r == "single",
                       ref.type != "cvz",
                       ref.type != "pvz"))

mod.r[["bel.control"]] <- 
  ModAnalysis(m = "bel",
              df = DF.es %>% 
                filter(ref.r == "single",
                       ref.type != "cvz",
                       ref.type != "pvz"))

mod.r[["pre.control"]] <- 
  ModAnalysis(m = "pre",
              df = DF.es %>% 
                filter(ref.r == "single",
                       ref.type != "cvz",
                       ref.type != "pvz"))

# explore possible higher-order interactions
## center continuous variables
mod.r[["att.mot.opp.int"]] <- 
  ModAnalysis(m = "att * mot * opp",
              df = DF.es %>% 
                filter(ref.type != "cvz" &
                         ref.type != "pvz"))

## combine results
mod.r = c(mod.r, mod.r.2)

# delete vestigial 
rm(mod.r.2)
```

When variability in effect sizes exceeds what would be expected from sampling error alone, it suggests the presence of moderators. Below, we examine several potential candidates.

#### Study features

In general, we did not find much evidence that demand effects are moderated by study features (see Table 1). The two exceptions were (1) whether the demand characteristics condition was compared to a control group (vs. another condition with demand characteristics), and (2) whether the study was conducted in-person (vs. online).

The average effect sizes was estimated to be twice as large when two demand characteristic conditions were compared ($d$ = `r mod.r$ref.r$sub$b[1]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[1]`, `r mod.r$ref.r$sub$ci.ub[1]`], $p$ `r printp(mod.r$ref.r$sub$pval[1])`), as opposed to one demand characteristic condition being compared to a control group ($d$ = `r mod.r$ref.r$sub$b[2]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[2]`, `r mod.r$ref.r$sub$ci.ub[2]`], $p$ = `r printp(mod.r$ref.r$sub$pval[2])`), $F$(`r mod.r$ref.r$mod$QMdf[1]`, `r mod.r$ref.r$mod$QMdf[2]`) = `r mod.r$ref.r$mod$QM`, $p$ = `r printp(mod.r$ref.r$mod$QMp)`. This provides preliminary evidence that the effects of demand characteristics are additive. However, these results should be interpreted with some caution, as a broader test of whether *all* specific types of comparisons varied was not statistically significant, $F$(`r mod.r$ref.type.full$mod$QMdf[1]`, `r mod.r$ref.type.full$mod$QMdf[2]`) = `r mod.r$ref.type.full$mod$QM`, $p$ = `r printp(mod.r$ref.type.full$mod$QMp)`.

Instances where a demand characteristic condition was compared to a control group allowed us to test whether participants responses shift more when the researcher hypothesizes an increase (i.e., positive demand; $d$ = `r mod.r$ref.type$sub$b[1]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[1]`, `r mod.r$ref.type$sub$ci.ub[1]`], $p$ = `r printp(mod.r$ref.type$sub$pval[1])`), a decrease (i.e., negative demand; $d$ = `r mod.r$ref.type$sub$b[3]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[3]`, `r mod.r$ref.type$sub$ci.ub[3]`], $p$ = `r printp(mod.r$ref.type$sub$pval[3])`), or no change in the dependent variable (i.e., nil demand; $d$ = `r mod.r$ref.type$sub$b[2]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[2]`, `r mod.r$ref.type$sub$ci.ub[2]`], $p$ = `r printp(mod.r$ref.type$sub$pval[2])`). We did not find this to be the case, $F$(`r mod.r$ref.type$mod$QMdf[1]`, `r mod.r$ref.type$mod$QMdf[2]`) = `r mod.r$ref.type$mod$QM`, $p$ = `r printp(mod.r$ref.type$mod$QMp)`. We also did not find that demand effects significantly varied depending on whether they were manipulated within- ($d$ = `r mod.r$design$sub$b[1]`, 95% CI [`r mod.r$design$sub$ci.lb[1]`, `r mod.r$design$sub$ci.ub[1]`], $p$ `r printp(mod.r$design$sub$pval[1])`) vs. between-subjects ($d$ = `r mod.r$design$sub$b[2]`, 95% CI [`r mod.r$design$sub$ci.lb[2]`, `r mod.r$design$sub$ci.ub[2]`], $p$ = `r printp(mod.r$design$sub$pval[2])`), $F$(`r mod.r$design$mod$QMdf[1]`, `r mod.r$design$mod$QMdf[2]`) = `r mod.r$design$mod$QM`, $p$ = `r printp(mod.r$design$mod$QMp)`

Demand effects tended to be slightly more positive for in-person ($g$ = `r mod.r$online$sub$b[1]`, 95% CI [`r mod.r$online$sub$ci.lb[1]`, `r mod.r$online$sub$ci.ub[1]`], $p$ `r printp(mod.r$online$sub$pval[1])`) vs. online ($g$ = `r mod.r$online$sub$b[2]`, 95% CI [`r mod.r$online$sub$ci.lb[2]`, `r mod.r$online$sub$ci.ub[2]`], $p$ = `r printp(mod.r$online$sub$pval[2])`) studies, $F$(`r mod.r$online$mod$QMdf[1]`, `r mod.r$online$mod$QMdf[2]`) = `r mod.r$online$mod$QM`, $p$ = `r printp(mod.r$online$mod$QMp)`. However, we did not find that demand effects significantly varied depending on whether students ($d$ = `r mod.r$student$sub$b[3]`, 95% CI [`r mod.r$student$sub$ci.lb[3]`, `r mod.r$student$sub$ci.ub[3]`], $p$ `r printp(mod.r$student$sub$pval[3])`), non-students ($d$ = `r mod.r$student$sub$b[2]`, 95% CI [`r mod.r$student$sub$ci.lb[2]`, `r mod.r$student$sub$ci.ub[2]`], $p$ = `r printp(mod.r$student$sub$pval[2])`), or a mix of students and non-students ($d$ = `r mod.r$student$sub$b[1]`, 95% CI [`r mod.r$student$sub$ci.lb[1]`, `r mod.r$student$sub$ci.ub[1]`], $p$ = `r printp(mod.r$student$sub$pval[1])`) were sampled, $F$(`r mod.r$student$mod$QMdf[1]`, `r mod.r$student$mod$QMdf[2]`) = `r mod.r$student$mod$QM`, $p$ = `r printp(mod.r$student$mod$QMp)`. We also did not find that demand effects significantly varied depending on whether those participants were paid ($d$ = `r mod.r$paid$sub$b[2]`, 95% CI [`r mod.r$paid$sub$ci.lb[2]`, `r mod.r$paid$sub$ci.ub[2]`], $p$ = `r printp(mod.r$paid$sub$pval[2])`) vs. unpaid ($d$ = `r mod.r$paid$sub$b[1]`, 95% CI [`r mod.r$paid$sub$ci.lb[1]`, `r mod.r$paid$sub$ci.ub[1]`], $p$ = `r printp(mod.r$paid$sub$pval[1])`), $F$(`r mod.r$paid$mod$QMdf[1]`, `r mod.r$paid$mod$QMdf[2]`) = `r mod.r$paid$mod$QM`, $p$ = `r printp(mod.r$paid$mod$QMp)`.


```{r f.mod.table}
feature.mod.table <- rbind(
  ################
  # Group comparison
  ################
  cbind(
    `Moderator (bolded) and level` = "Group comparison",
    s = mod.r$ref.type.full$mod$n,
    k = mod.r$ref.type.full$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$ref.type.full$mod$QM %>% round(2),
    p = mod.r$ref.type.full$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## cvp
  cbind(
    `Moderator (bolded) and level` = "     positive vs. control",
    s = DF.es %>% 
      filter(ref.type == 'cvp') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.type == 'cvp') %>% 
      nrow(),
    g =  mod.r$ref.type.full$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.type.full$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$ref.type.full$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.type.full$sub$zval[[1]] ^ 2 %>% 
      round(2),
    p = mod.r$ref.type.full$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ## cvz
  cbind(
    `Moderator (bolded) and level` = "     nil vs. control",
    s = DF.es %>% 
      filter(ref.type == 'cvz') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.type == 'cvz') %>% 
      nrow(),
    g =  mod.r$ref.type.full$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.type.full$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$ref.type.full$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.type.full$sub$zval[[2]] ^ 2 %>% 
      round(2),
    p = mod.r$ref.type.full$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),
  
  ## nvc
  cbind(
    `Moderator (bolded) and level` = "     negative vs. control",
    s = DF.es %>% 
      filter(ref.type == 'nvc') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.type == 'nvc') %>% 
      nrow(),
    g =  mod.r$ref.type.full$sub$b[[3]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.type.full$sub$ci.lb[[3]] %>% round(2),
        ", ",
        mod.r$ref.type.full$sub$ci.ub[[3]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.type.full$sub$zval[[3]] ^ 2 %>% 
      round(2),
    p = mod.r$ref.type.full$sub$pval[[3]] %>% as.numeric %>% apa_p()
  ),
  
  ## pvz
  cbind(
    `Moderator (bolded) and level` = "     positive vs. nil",
    s = DF.es %>% 
      filter(ref.type == 'pvz') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.type == 'pvz') %>% 
      nrow(),
    g =  mod.r$ref.type.full$sub$b[[5]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.type.full$sub$ci.lb[[5]] %>% round(2),
        ", ",
        mod.r$ref.type.full$sub$ci.ub[[5]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.type.full$sub$zval[[5]] ^ 2 %>% 
      round(2),
    p = mod.r$ref.type.full$sub$pval[[5]] %>% as.numeric %>% apa_p()
  ),
  
  ## pvn
  cbind(
    `Moderator (bolded) and level` = "     positive vs. negative",
    s = DF.es %>% 
      filter(ref.type == 'pvn') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.type == 'pvn') %>% 
      nrow(),
    g =  mod.r$ref.type.full$sub$b[[4]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.type.full$sub$ci.lb[[4]] %>% round(2),
        ", ",
        mod.r$ref.type.full$sub$ci.ub[[4]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.type.full$sub$zval[[4]] ^ 2 %>% 
      round(2),
    p = mod.r$ref.type.full$sub$pval[[4]] %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Control vs. non-control group comparison
  ################
  cbind(
    `Moderator (bolded) and level` = "Control vs. non-control group comparison",
    s = mod.r$ref.r$mod$n,
    k = mod.r$ref.r$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$ref.r$mod$QM %>% round(2),
    p = mod.r$ref.r$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## control
  cbind(
    `Moderator (bolded) and level` = "     control",
    s = DF.es %>% 
      filter(ref.r == "single") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.r == "single") %>% 
      nrow(),
    g =  mod.r$ref.r$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.r$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$ref.r$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.r$sub$zval[[2]] ^ 2 %>%
      round(2),
    p = mod.r$ref.r$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ), 
  
  ## non-control
  cbind(
    `Moderator (bolded) and level` = "     non-control",
    s = DF.es %>% 
      filter(ref.r == "double") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(ref.r == "double") %>% 
      nrow(),
    g =  mod.r$ref.r$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$ref.r$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$ref.r$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$ref.r$sub$zval[[1]] ^ 2 %>%
      round(2),
    p = mod.r$ref.r$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ###############
  # Control group comparison
  ################
  cbind(
    `Moderator (bolded) and level` = "Control group comparison (see levels above)",
    s = mod.r$ref.type$mod$n,
    k = mod.r$ref.type$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$ref.type$mod$QM %>% round(2),
    p = mod.r$ref.type$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Design of demand characteristics manipulation
  ################
  cbind(
    `Moderator (bolded) and level` = "Design of demand characteristics manipulation",
    s = mod.r$design$mod$n,
    k = mod.r$design$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$design$mod$QM %>% round(2),
    p = mod.r$design$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## within
  cbind(
    `Moderator (bolded) and level` = "     within-subjects",
    s = DF.es %>% 
      filter(design == "within") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(design == 'within') %>% 
      nrow(),
    g =  mod.r$design$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$design$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$design$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$design$sub$zval[[2]] ^ 2 %>%
      round(2),
    p = mod.r$design$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),
  
  # in person
  cbind(
    `Moderator (bolded) and level` = "     between-subjects",
    s = DF.es %>% 
      filter(design == "between") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(design == 'between') %>% 
      nrow(),
    g =  mod.r$design$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$design$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$design$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$design$sub$zval[[1]] ^ 2 %>%
      round(2),
    p = mod.r$design$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Participant pool
  ################
  cbind(
    `Moderator (bolded) and level` = "Participant pool",
    s = mod.r$student$mod$n,
    k = mod.r$student$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$student$mod$QM %>% round(2),
    p = mod.r$student$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## student: yes
  cbind(
    `Moderator (bolded) and level` = "     students",
    s = DF.es %>% 
      filter(student == "yes") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(student == "yes") %>% 
      nrow(),
    g =  mod.r$student$sub$b[[3]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$student$sub$ci.lb[[3]] %>% round(2),
        ", ",
        mod.r$student$sub$ci.ub[[3]] %>% round(2),
        "]"
      ),
    `F` = mod.r$student$sub$zval[[3]] ^ 2 %>%
      round(2),
    p = mod.r$student$sub$pval[[3]] %>% as.numeric %>% apa_p()
  ),
  
  ## student: no
  cbind(
    `Moderator (bolded) and level` = "     non-students",
    s = DF.es %>% 
      filter(student == "no") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(student == "no") %>% 
      nrow(),
    g =  mod.r$student$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$student$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$student$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$student$sub$zval[[2]] ^ 2 %>%
      round(2),
    p = mod.r$student$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),
  
  ## student: mix
  cbind(
    `Moderator (bolded) and level` = "     mix",
    s = DF.es %>% 
      filter(student == "mix") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(student == "mix") %>% 
      nrow(),
    g =  mod.r$student$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$student$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$student$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$student$sub$zval[[1]] ^ 2 %>%
      round(2),
    p = mod.r$student$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Setting
  ################
  cbind(
    `Moderator (bolded) and level` = "Setting",
    s = mod.r$online$mod$n,
    k = mod.r$online$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$online$mod$QM %>% round(2),
    p = mod.r$online$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## online
  cbind(
    `Moderator (bolded) and level` = "     online",
    s = DF.es %>% 
      filter(online == "yes") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k =  DF.es %>% 
      filter(online == 'yes') %>% 
      nrow(),
    g =  mod.r$online$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$online$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$online$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$online$sub$zval[[2]] ^ 2 %>%
      round(2),
    p = mod.r$online$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),
  
  # in person
  cbind(
    `Moderator (bolded) and level` = "     in-person",
    s = DF.es %>% 
      filter(online == "no") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(online == 'no') %>% 
      nrow(),
    g =  mod.r$online$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$online$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$online$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$online$sub$zval[[1]] ^ 2 %>%
      round(2),
    p = mod.r$online$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Payment
  ################
  cbind(
    `Moderator (bolded) and level` = "Payment",
    s = mod.r$paid$mod$n,
    k = mod.r$paid$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$paid$mod$QM %>% round(2),
    p = mod.r$paid$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## yes
  cbind(
    `Moderator (bolded) and level` = "     yes",
    s = DF.es %>% 
      filter(paid == "yes") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(paid == 'yes') %>% 
      nrow(),
    g =  mod.r$paid$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$paid$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$paid$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$paid$sub$zval[[2]] ^ 2 %>%
      round(2),
    p = mod.r$paid$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),
  
  # no
  cbind(
    `Moderator (bolded) and level` = "     no",
    s = DF.es %>% 
      filter(paid == "no") %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k =  DF.es %>% 
      filter(paid == 'no') %>% 
      nrow(),
    g =  mod.r$paid$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$paid$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$paid$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$paid$sub$zval[[1]] ^ 2 %>%
      round(2),
    p = mod.r$paid$sub$pval[[1]] %>% as.numeric %>% apa_p()
  ),
  
  ################
  # Publication status
  ################
  cbind(
    `Moderator (bolded) and level` = "Publication status",
    s = mod.r$published$mod$n,
    k = mod.r$published$mod$k,
    g =  "--",
    `95% CI` = "--",
    `F` = mod.r$published$mod$QM %>% round(2),
    p = mod.r$published$mod$QMp %>% as.numeric %>% apa_p()
  ),
  
  ## published
  cbind(
    `Moderator (bolded) and level` = "     published",
    s = DF.es %>% 
      filter(published == 'yes') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(published == 'yes') %>% 
      nrow(),
    g =  mod.r$published$sub$b[[2]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$published$sub$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$published$sub$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$published$sub$zval[[2]] ^ 2 %>% 
      round(2),
    p = mod.r$published$sub$pval[[2]] %>% as.numeric %>% apa_p()
  ),

  ## unpublished
  cbind(
    `Moderator (bolded) and level` = "     unpublished",
    s = DF.es %>% 
      filter(published == 'no') %>%
      reframe(n.study = unique(id.study)) %>% 
      nrow(),
    k = DF.es %>% 
      filter(published == 'no') %>% 
      nrow(),
    g =  mod.r$published$sub$b[[1]] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$published$sub$ci.lb[[1]] %>% round(2),
        ", ",
        mod.r$published$sub$ci.ub[[1]] %>% round(2),
        "]"
      ),
    `F` = mod.r$published$sub$zval[[1]] ^ 2 %>% 
      round(2),
    p = mod.r$published$sub$pval[[1]] %>% as.numeric %>% apa_p()
    )
  )

apa_table(
  feature.mod.table,
  caption = "Study feature moderator and subgroup analyses",
  note = "s = number of studies; k = number of effect size estimates; g = Hedge's g; 95% CI corresponds to the estimated value of Hedge's g; F-values represent the test of moderation in bolded rows -- and tests of the model-derived overall effect size in non-bolded rows; The number of studies listed for a moderator analysis is not necessarily the sum of the number of studies listed for the individual levels of the moderators because many studies yielded effect sizes for multiple levels of the moderator.")
```


##### Residual variability

```{r r2, include = F}
# calculate a pseudo-R2
## see this page for a discussion of the method:
## https://stackoverflow.com/questions/22356450/getting-r-squared-from-a-mixed-effects-multilevel-model-in-metafor

## pairwise delete observations where we don't have information for moderator analyses
## this is to ensure that the two models we are comparing have the same observations
DF.cmplt <- DF.es %>% 
  filter(!is.na(student),
         !is.na(ref.r),
         !is.na(att))

## fit intercept-only model
m.int <- rma.mv(yi = es,
                V = es.var,
                data = DF.cmplt,
                random = ~ 1 | id.study / id.es,
                test = "t") %>% 
  robust(x = .,
         cluster = id.study,
         clubSandwich = T)

# fit moderator model and calculate r2
m.mod <- rma.mv(yi = es,
               V = es.var,
               data = DF.cmplt,
               random = ~ 1 | id.study / id.es,
               mods = ~ student + ref.r,
               test = "t") %>% 
  robust(x = .,
         cluster = id.study,
         clubSandwich = T)

r2 <- (sum(m.int$sigma2) - sum(m.mod$sigma2)) / sum(m.int$sigma2) 

rm(DF.cmplt, m.int, m.mod)
```

To evaluate how much in-sample variability in demand effects is currently accounted for by study feature moderators, we calculated a pseudo-$R^2$ statistic. We did so by comparing the sum of the variance components (between-study $\tau^2$ + within-study $\sigma^2$) in a model containing only an intercept and a model containing the two study feature moderators that achieved statistical significance: (1) whether the demand characteristics condition was compared to a control group (vs. another condition with demand characteristics), and (2) whether the study was conducted in-person (vs. online). Results indicated that the significant moderators accounted for approximately `r round(r2 * 100, 2)`% of in-sample variability in demand effects.

#### Can participants help us understand demand effects?

```{r vig.rel.full, include = F}
vig.rel <- readRDS("output/vig.survfull.rel.rds")

vig.desc <- read.csv("output/surv.sum.csv")
```

Participants correctly identified the described hypothesis in `r vig.desc$m.att %>% mean(na.rm = T) %>% round(2) * 100` of cases. However, participants did not generally report having strong beliefs about whether such hypothesized effects would occur (*M* = `r vig.desc$m.bel %>% mean(na.rm = T)`, *SD* = `r vig.desc$m.bel %>% sd(na.rm = T)`). Participants reported that they would be highly capable of adjusting their responses (*M* = `r vig.desc$m.opp %>% mean(na.rm = T)`, *SD* = `r vig.desc$m.opp %>% sd(na.rm = T)`), but not very motivated to do so (*M* = `r vig.desc$m.mot %>% mean(na.rm = T)`, *SD* = `r vig.desc$m.mot %>% sd(na.rm = T)`). Participants also predicted that other subjects would be generally unlikely to adjust their responses to fit the experimenter's hypothesis (*M* = `r vig.desc$m.pre %>% mean(na.rm = T)`, *SD* = `r vig.desc$m.pre %>% sd(na.rm = T)`).

The above results suggest that participants generally report being receptive to demand characteristics, agnostic about hypothesized effects, capable of adjusting their responses, but not motivated to do so. That being said, we remind the reader that these ratings exhibited low reliability (motivation ICC = `r vig.rel$mot$ICC[1]`; opportunity to adjust responses ICC = `r vig.rel$opp$ICC[1]`; belief ICC = `r vig.rel$bel$ICC[1]`). This may be indicative of strong individual differences, but we also later describe the possibility of measurement difficulties (see *Limitations*).

As shown in Table 2, we did not uncover a significant association between observed demand effects and (a) the extent to which participants correctly identified the hypothesis described in the vignettes, ($\beta$ = `r mod.r$att$mod$b["att", ]`, 95% CI [`r mod.r$att$mod$ci.lb[2]`, `r mod.r$att$mod$ci.ub[2]`], $t$(`r mod.r$att$mod$dfs[2]`) = `r mod.r$att$mod$zval[2]`, $p$ = `r printp(mod.r$att$mod$pval[2])`), (b) ratings of motivation to adjust responses ($\beta$ = `r mod.r$mot$mod$b["mot", ]`, 95% CI [`r mod.r$mot$mod$ci.lb[2]`, `r mod.r$mot$mod$ci.ub[2]`], $t$(`r mod.r$mot$mod$dfs[2]`) = `r mod.r$mot$mod$zval[2]`, $p$ = `r printp(mod.r$mot$mod$pval[2])`), (c) ratings of opportunity to adjust responses ($\beta$ = `r mod.r$opp$mod$b["opp", ]`, 95% CI [`r mod.r$opp$mod$ci.lb[2]`, `r mod.r$opp$mod$ci.ub[2]`], $t$(`r mod.r$opp$mod$dfs[2]`) = `r mod.r$opp$mod$zval[2]`, $p$ = `r printp(mod.r$opp$mod$pval[2])`), and (d) rated belief in the hypothesized effect ($\beta$ = `r mod.r$bel$mod$b["bel", ]`, 95% CI [`r mod.r$bel$mod$ci.lb[2]`, `r mod.r$bel$mod$ci.ub[2]`], $t$(`r mod.r$bel$mod$dfs[2]`) = `r mod.r$bel$mod$zval[2]`, $p$ = `r printp(mod.r$bel$mod$pval[2])`). Of course, Rosnow and colleagues posited that receptivity, motivation, and opportunity *interact* to shape demand effects. However, when we explored this question, we did not find robust evidence that including all possible higher order interactions significantly improved model fit, *F*(`r mod.r$att.mot.opp.int$mod$QMdf[1]`, `r mod.r$att.mot.opp.int$mod$QMdf[2]`) = `r mod.r$att.mot.opp.int$mod$QM %>% round(2)`, *p* = `r mod.r$att.mot.opp.int$mod$QMp %>% apa_p()`.

Even after averaging across a large number of noisy forecasts (ICC = `r vig.rel$pre$ICC[1]`, *M* = `r vig.desc$m.pre %>% mean(na.rm = T)`, *SD* = `r vig.desc$m.pre %>% sd(na.rm = T)`), we also failed to find that participants were able to predict the magnitude of demand effects, $\beta$ = `r mod.r$pre$mod$b["pre", ]`, 95% CI [`r mod.r$pre$mod$ci.lb[2]`, `r mod.r$pre$mod$ci.ub[2]`], $t$(`r mod.r$pre$mod$dfs[2]`) = `r mod.r$pre$mod$zval[2]`, $p$ = `r printp(mod.r$pre$mod$pval[2])`.

```{r}
rbind(
  # predicted demand effects
  cbind(
    `Moderator (bolded) and level` = "predicted demand effects",
    s = mod.r$pre$mod$n,
    k = mod.r$pre$mod$k,
    B1 = mod.r$pre$mod$b[2] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$pre$mod$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$pre$mod$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$pre$mod$QM %>% round(2),
    p = mod.r$pre$mod$QMp %>% apa_p()
    ),
  
  # understanding of study hypothesis
  cbind(
    `Moderator (bolded) and level` = "understanding of study hypothesis",
    s = mod.r$att$mod$n,
    k = mod.r$att$mod$k,
    B1 = mod.r$att$mod$b[2] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$att$mod$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$att$mod$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$att$mod$QM %>% round(2),
    p = mod.r$att$mod$QMp %>% apa_p()
  ),
    
  # motivation to adjust responses
  cbind(
    `Moderator (bolded) and level` = "motivation to adjust responses",
    s = mod.r$mot$mod$n,
    k = mod.r$mot$mod$k,
    B1 = mod.r$mot$mod$b[2] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$mot$mod$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$mot$mod$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$mot$mod$QM %>% round(2),
    p = mod.r$mot$mod$QMp %>% apa_p()
  ),
  
  # opportunity to adjust responses
  cbind(
    `Moderator (bolded) and level` = "opportunity to adjust responses",
    s = mod.r$opp$mod$n,
    k = mod.r$opp$mod$k,
    B1 = mod.r$opp$mod$b[2] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$opp$mod$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$opp$mod$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$opp$mod$QM%>% round(2),
    p = mod.r$opp$mod$QMp %>% apa_p()
  ),
  
  # belief in communicated hypothesis
  cbind(
    `Moderator (bolded) and level` = "belief in communicated hypothesis",
    s = mod.r$bel$mod$n,
    k = mod.r$bel$mod$k,
    B1 = mod.r$bel$mod$b[2] %>% round(2),
    `95% CI` =
      paste0(
        "[",
        mod.r$bel$mod$ci.lb[[2]] %>% round(2),
        ", ",
        mod.r$bel$mod$ci.ub[[2]] %>% round(2),
        "]"
      ),
    `F` = mod.r$bel$mod$QM %>% round(2),
    p = mod.r$bel$mod$QMp %>% apa_p()
    )
  ) %>%
  apa_table(caption = "Participant rating moderator analyses",
            note = "s = number of studies; k = number of effect size estimates; B1 = estimated linear relationship between participant ratings and observed Hedge's g scores; 95% CI corresponds to the estimated value of B1.")
```

### Publication bias analyses

```{r pub.bias, include = F}
# delete vestigial
rm(in.s, on.s, v.s, p.s,
   m.s1, m.s2,
   m.sens, m.sens.student, m.sens.online, m.sens.pay)

# Define publication bias analysis that
# 1. Mathur and VanderWeele 2020 sensitivity analyses
# 2. Fits three-level precision-effect test
# 3a. Aggregates dependent effect sizes (with given rho value)
# 3b. Aggregated precision-effect test
# 3b. Fits Vevea and Hedges (1995) Weight-Function Model w/ aggregated effects
# 4a. Fit funnel plot
# 4b. Fit funnel plot w/ aggregated dependencies
# 5. Organizes results into list
##########################

PubBias = function(rho.val = .5){
  # 1. sensitivity analyses
  ########################
  sens <- pubbias_meta(yi = DF.es$es,
                       vi = DF.es$es.var, 
                       cluster = DF.es$id.study, 
                       selection_ratio = 10000000, 
                       model_type = "robust", 
                       favor_positive = T)
  
  # you can run the code below to see how nonsensicle it is to look at disfavor bias
  # pubbias_meta(yi = DF.es$es,
  #              vi = DF.es$es.var, 
  #              cluster = DF.es$id.study,
  #              selection_ratio = 10000000, 
  #              model_type = "robust", 
  #              favor_positive = F)
  
  # 2a. three-level precision-effect test
  ########################
  pe.3l <- rma.mv(yi = es,
                  V = es.var,
                  mods = ~ sqrt(es.var),
                  data = DF.es,
                  random = ~ 1 | id.study / id.es)
  
  # 2b. cluster robust three-level precision-effect test
  ########################
  pe.3l.r <- rma.mv(yi = es,
                    V = es.var,
                    mods = ~ sqrt(es.var),
                    data = DF.es,
                    random = ~ 1 | id.study / id.es) %>%  
    robust(x = .,
           cluster = id.study,
           clubSandwich = T)
  
  # 3a. aggregate dependent effect sizes
  ########################
  DF.agg <- DF.es %>%
    
    # convert to an 'escalc' object so function can run
    escalc(yi = es,
           vi = es.var,
           data = DF.es,
           measure = "SMD") %>%
    
    # delete vestigial: es is now yi; es.var is now vi
    select(-c(es, es.var)) %>% 
    
    # aggregate dependencies
    aggregate(x = .,
              cluster = id.study,
              rho = rho.val)
  
  # 3b. aggregated precision-effect test
  ########################
  pe.a <- rma.uni(yi = yi,
                  vi = vi,
                  mods = ~ sqrt(vi),
                  data = DF.agg,
                  method = "REML")
  
  # temp code: aggregated precision-effect test
  ########################
  rma.uni(yi = yi,
          vi = vi, 
          mods = ~ sqrt(vi), 
          data = DF.agg %>% filter(id.study != 30,
                                   id.study != 55), 
          method = "REML")

  # 3c. Weight-function model
  ########################
  weight.funct <- weightfunct(effect = DF.agg$yi,
                              v = DF.agg$vi,
                              mods = NULL,
                              weights= NULL,
                              fe = FALSE,
                              table = TRUE,
                              pval = NULL)
  
  # 4a. funnel plot
  ########################
  par(mfrow=c(1,2))
  
  rma.uni(yi = es,
          vi = es.var,
          data = DF.es,
          method = "REML") %>%   
  metafor::funnel(hlines = "lightgray",
                  xlab = "Cohen's standardized d") 
  
  # 4b. funnel plot w/ aggregated dependencies
  ########################
  rma.uni(yi = yi,
          vi = vi,
          data = DF.agg,
          method = "REML") %>%   
    metafor::funnel(hlines = "lightgray",
                    xlab = "Cohen's standardized d (aggregated)")
  
  # save funnel plot as object
  funnel.plot <- recordPlot()
    
  # clear R environment
  plot.new()
  
  # 5. Organize results in list 
  ########################
  list(sens = sens,
       pe.3l = pe.3l,
       pe.3l.r = pe.3l.r,
       DF.agg = DF.agg,
       pe.a = pe.a,
       weight.funct = weight.funct,
       funnel = funnel.plot) %>%  
    return()
}

# for range of rho values, run publication bias analyses
rho.l = seq(from = .1,
            to = .9,
            by = .2)

pub.r <- lapply(X = rho.l,
                FUN = PubBias)

names(pub.r) = paste0("rho_", rho.l) #  name list

# delete vestigial 
rm(rho.l, PubBias)

# look at sensitivity analyses, if you'd like
## general story: often, but not always, find evidence of reverse publication bias (preference for negative effects)
# lapply(pub.r, function(x){x[["pe.a"]]})
# lapply(pub.r, function(x){x[["weight.funct"]]})
# lapply(pub.r, function(x){x[["funnel"]]})
```

Overall, publication bias analyses were inconclusive. Both precision-effect tests with 3LMA[^8] ($\beta$ = `r pub.r$rho_0.5$pe.3l$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[2]`, `r pub.r$rho_0.5$pe.3l$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.3l$pval[2])`) and aggregated dependencies ($\beta$ = `r pub.r$rho_0.5$pe.a$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[2]`, `r pub.r$rho_0.5$pe.a$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[2])`) provided non-significant evidence of publication bias that favored hypothesis-consistent shifts in participants’ responses. The bias-corrected overall effect size estimates with both 3LMA ($g$ = `r pub.r$rho_0.5$pe.3l$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[1]`, `r pub.r$rho_0.5$pe.3l$ci.ub[1]`], $p$ = `r printp(pub.r$rho_0.5$pe.3l$pval[1])`) and aggregated dependencies ($g$ = `r pub.r$rho_0.5$pe.a$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[1]`, `r pub.r$rho_0.5$pe.a$ci.ub[1]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[1])`) did not significantly vary from zero. In other words, precision-effect tests failed to uncover evidence of publication bias, but also suggested that the overall effect size may not be robust if publication bias does exist. Further complicating matters is the unusual distribution of the funnel plots, especially in regards to two unusually large aggregated effect size estimates (see Figure \@ref(fig:funnel)).

[^8]: A reviewer suggested that we consider using cluster-robust methods for estimating variance-covariance matrices in precision-effect tests with 3LMA. Doing so widens the confidence intervals of the insignificant estimates of publication bias and the bias-correct overall effect size.

Examining aggregated effect sizes using weight-function modeling – as opposed to precision effect tests – yields a different pattern: better fit is achieved in a model where publication bias favored non-significant or hypothesis-inconsistent shifts in participants’ responses, $\chi^2$(1) = 6.50, $p$ = .01. The bias-corrected overall effect size was thus upward-adjusted, $g$ = 0.32, 95% CI [0.15, 0.49], $p$ \< .001. The discrepancy between precision-effect tests and weight-function modeling may be driven by the unusual distribution of the funnel plots (see Figure \@ref(fig:funnel)).

We did not find significant differences in the magnitude of demand effects between published ($g$ = `r mod.r$published$sub$b[2]`, 95% CI [`r mod.r$published$sub$ci.lb[2]`, `r mod.r$published$sub$ci.ub[2]`], $p$ = `r printp(mod.r$published$sub$pval[2])`) and unpublished ($g$ = `r mod.r$published$sub$b[1]`, 95% CI [`r mod.r$published$sub$ci.lb[1]`, `r mod.r$published$sub$ci.ub[1]`], $p$ = `r printp(mod.r$published$sub$pval[1])`) studies, $F$(`r mod.r$published$mod$QMdf[1]`, `r mod.r$published$mod$QMdf[2]`) = `r mod.r$published$mod$QM`, $p$ = `r printp(mod.r$published$mod$QMp)`. If there is a biased selection of instances where participants responses shift in a hypothesis consistent manner, sensitivity analyses indicated that it would have to be extreme selection pressure to make the effect size non-significant [@mathur2020sensitivity]. Even if hypothesis-consistent shifts were `r pub.r$rho_0.5$sens$values$selection_ratio` times more likely to be published, the overall effect would still be `r pub.r$rho_0.5$sens$stats$estimate`, 95% CI [`r pub.r$rho_0.5$sens$stats$ci_lower`, `r pub.r$rho_0.5$sens$stats$ci_upper`], *p* = `r printp(pub.r$rho_0.5$sens$stats$p_value)`.

```{r funnel, fig.cap = "Raw (Panel A) or aggregated (Panel B) effect sizes plotted against their corresponding standard errors. Funnel plot is inverted to illustrate correspondence with slope estimates from precision-effect tests.", fig.height = 3.75, fig.width = 6.5}
########## 
# Funnel plot with non-aggregated dependencies
########## 
# create a temporary dataset with standard error (se) values
tmp <- DF.es %>%  
  rowwise() %>% 
  mutate(se = sqrt(es.var)) %>% 
  ungroup()

# create temporary sequence of ses
se.seq = seq(0, max(tmp$se), 
             length.out = nrow(DF.es))

ll95 = overall$b[1] - (1.96 * se.seq)
ul95 = overall$b[1] + (1.96 * se.seq)

# create coordinates for polygon
t.coord <- rbind(cbind(x = overall$b[1],
                       y = 0),
                 
                 cbind(x = min(ll95),
                       y = max(tmp$se)),
                 
                 cbind(x = max(ul95),
                       y = max(tmp$se))
                 ) %>% 
  as.data.frame()
  
# plot
a <- ggplot(data = tmp,
            aes(x = es,
                y = se)) +
  geom_polygon(data = t.coord,
               aes(x = x,
                   y = y),
               fill = "#3366FF",
               alpha = .1) +
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  # scale_y_reverse(expand = c(.01, 0)) +
  scale_y_continuous(expand = c(.01, 0)) +
  scale_x_continuous(limits = c(-1.5, 2.1),
                     expand = c(.01, .01)) +
  geom_vline(xintercept = overall$b[1],
             linetype = "dotted") +
  labs(x = expression(paste("Hedge's ", italic("g"))), 
       y = "Standard error")

# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord)

########## 
# Funnel plot with aggregated dependencies
########## 
# create a temporary dataset with standard error (se) values
tmp <- pub.r$rho_0.5$DF.agg %>% 
  rowwise() %>% 
  mutate(es = yi,
         se = sqrt(vi)) %>% 
  ungroup()

# calculate overall effect size
tmp.meta <- rma.uni(yi = yi,
                    vi = vi,
                    data = tmp,
                    method = "REML")

# create temporary sequence of ses
se.seq = seq(0, max(tmp$se), 
             length.out = nrow(tmp))

ll95 = tmp.meta$b[1] - (1.96 * se.seq)
ul95 = tmp.meta$b[1] + (1.96 * se.seq)

# create coordinates for polygon
t.coord <- rbind(cbind(x = tmp.meta$b[1],
                       y = 0),
                 
                 cbind(x = min(ll95),
                       y = max(tmp$se)),
                 
                 cbind(x = max(ul95),
                       y = max(tmp$se))
                 ) %>% 
  as.data.frame()

b <- ggplot(data = tmp,
       aes(x = es,
           y = se)) +
  geom_polygon(data = t.coord,
               aes(x = x,
                   y = y),
               fill = "#3366FF",
               alpha = .1) +
  geom_jitter(alpha = .8,
              fill = "dark grey",
              color = "dark grey") +
  #scale_y_reverse(expand = c(.01, 0)) +
  scale_y_continuous(expand = c(.01, 0)) +
  scale_x_continuous(limits = c(-1.5, 2.1),
                     expand = c(.01, .01)) +
  geom_vline(xintercept = tmp.meta$b[1],
             linetype = "dotted") +
  labs(x = expression(paste("Hedge's ", italic("g"))),
       y = "")

# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord, tmp.meta)
  
########## 
# Plot funnels next to each other plot with aggregated dependencies
##########
plot_grid(a, b,
          labels = c("A", "B"))

rm(a, b)
```

# Discussion

In the *Introduction*, we described a fictitious discipline that we suspect would be met with extreme skepticism – one plagued by a methodological artifact that (a) can lead to both false positives and false negatives, (b) can create both upward bias and downward bias, (c) has unreliable effects, and (d) is difficult to explain. If one agrees that such a characterization is problematic, they face an uncomfortable observation: our meta-analysis suggests that this characterization also currently applies to experimental psychology.

Since Orne popularized the concept in the mid-1900's, demand characteristics have become a literal textbook methodological concern in experimental psychology. We synthesized a subset of this literature, focusing on `r overall$k` effect sizes from `r overall$n` studies that provided experimental tests of demand effects by explicitly manipulating cues about the study hypothesis. Consistent with an influential framework developed by Rosnow and colleagues [@rosnow1997people; @rosnow1973mediation; @strohmetz2008research], the observed and estimated true distribution of these effects suggest that demand characteristics can create false positives [@orne1959nature], false negatives [@hayes1967two], and upward and downward bias [@coles2022fact]. Even more concerning is our observation that such effects are unreliable across studies. On average, explicit hypothesis cues lead to small increases in hypothesis-consistent responding. However, such cues also frequently lead to shifts in responses that might be characterized as "negligible" -- and occasionally they even lead to *decreases* in hypothesis-consistent responding. The estimated distribution of these effects currently ranges from $g$ = `r dist.min` to $g$ = `r dist.max` – covering the span of pretty much any conceivable effect in experimental psychology [e.g., see @lovakov2021empirically].

Observing demand effects that are strong, inferentially consequential, and unreliable would be less concerning if researchers had an explanation for how such effects operate. Unfortunately, our synthesis offers little explanation. Coded study features failed to generate profound insights – revealing only that demand effects tend to be larger when studies are run in-person and include comparisons between two different demand characteristic conditions. Although such insights are certainly useful, they only explained an estimated `r round(r2 * 100, 2)`% of in-sample variability in demand effects. Given that in-sample (vs. out-of-sample) estimates of $R^2$ are often inflated by overfitting [@de2020cross], we suspect that the true proportion of explained variability is even lower.

We next examined a popular and influential framework developed by Rosnow and colleagues, who successfully predicted that demand effects are heterogeneous [@rosnow1997people; @rosnow1973mediation; @strohmetz2008research]. However, we found few attempts to test their proposed explanation for such heterogeneity: differences in the extent to which participants are (a) receptive, (b) motivated, and (c) able to respond to demand characteristics. [One exception is an *unpublished* record by @coles2024replication.]

Contrary to early advice by Orne [-@orne1969demand], we did not find that much clarity emerged when consulting participants themselves. When we provided a large set of naïve participants with summaries of the studies in our meta-analysis, we found that their predictions about demand effects and their underlying mechanisms were unreliable. Indeed, participants generally indicated that they would be receptive to demand characteristics, agnostic about the communicated effects, capable of adjusting their responses, but not motivated to do so. They also generally predicted that other participants would not respond to demand characteristics. Such predictions are clearly at odds with the demand effects observed in our meta-analysis.

Even when averaging across a large number of participant judgments, we failed to find that they were able to predict or explain the mechanisms underlying demand effects. Demand effects were not significantly predicted by the extent to which they (a) correctly identified the communicated hypothesis, (b) reported they would be motivated to adjust responses, (c) reported they would be able to adjust responses, and (d) reported they would expect the hypothesized effect to emerge.

We currently lack a satisfying explanation for our results. One possibility is that demand characteristics are not driven by receptivity, motivation, opportunity, and/or belief in the experimenter's hypothesis. However, we find this explanation unlikely given the face validity of those proposed mechanisms. Indeed, @rosnow1997people argued that opportunity was the least important mechanism because "...not many experimenters would design a study so that a participant would be *incapable* of responding to cues closely tied to the experimenter's own expectation." A second possibility is that these mechanisms are not as essential as Rosnow and colleagues expected. For example, @coles2022fact argued that demand characteristics may activate mechanisms that do not require motivation or direct ability to adjust responses (e.g., conditioned responses). If demand effects are multiply determined, the effect of any single moderator may be weaker than previously expected. A third possibility is that participants are not able to accurately reflect on these mechanisms. For example, @corneille2022sixty suggested that participants may occasionally be unaware that they were motivated to adjust their responses -- e.g., in cases of phenomenological control. A fourth possibility, of course, is that our own methodological limitations inhibited our ability to detect the effects of these moderators. We discuss these limitations next.

## Limitations

Our meta-analysis is, of course, not without limitations. It still remains unclear why participants were generally unable to predict and explain demand effects. It is unclear if our inclusion criteria were too narrow – or not narrow enough. And we suspect that there are many supplemental analyses that can be performed on our openly-available data to further probe the nature of demand effects.

@orne1969demand suggested that participants themselves may help researchers understand demand effects. At first glance, this assumption seems reasonable. Participants are capable of predicting a variety of effects in psychology when exposed to information about the study procedures [@corneille2023instruction] – and this very procedure is often used to raise concerns about demand characteristics [@bartels2019revisiting]. We, however, failed to find that similar procedures could be used to predict or explain demand effects at the meta-analytic level. Yet, it is unclear whether this is a valid and important insight in itself -- or indicative of our own methodological shortcomings. For example, perhaps participants are too different than the original participants [@gergen1973social], perhaps they need to experience the study context first-hand [@orne1969demand], and perhaps they need better measures of the psychological mechanisms that may underlie demand effects [@flake2020measurement].

Broad definitions of the demand characteristics construct presented us with yet another challenge. At its broadest, demand characteristics are defined as almost *any* cue that may impact participants' understanding of the purpose of the study, including instructions, rumors, and experimenter behavior [@orne1962social]. However, such a definition arguably creates a boundless conceptual space where any systematic change in a research design or setting might be considered a threat to scientific inferences. We focused our meta-analysis on a subset of the conceptual space that is more amenable to precise definition and study: explicit cues of the study hypothesis. Although we do not reject broader definitions of demand characteristics, we suspect that such broadening will only further deepen the mystery surrounding their effects.

Even with our relatively narrow subset of the demand characteristics literature, there are commensurability challenges. Researchers have tested the effects of explicit hypothesis cues on a variety of outcomes, including hypnosis symptoms [e.g., @orne1964contribution], eating behavior [e.g., @kersbergen2019hypothesis], visual judgments [e.g., @durgin2012social], relationship satisfaction [e.g., @cramer2005effect], mood [e.g., @coles2022fact], policy support [e.g., @mummolo2019demand], test scores [e.g., @veitch1991demand], and so on. Researchers also varied in how they conducted their investigations – e.g., in whether they (a) conducted their studies in-person [e.g., @orne1964contribution] vs. online [e.g., @mummolo2019demand], (b) sampled students [e.g., @rose2014choice] vs. non-students [e.g., @terhune2006induction], and (c) manipulated hypothesis cues within- [e.g., @martin2018attention] vs. between-subjects [e.g., @coles2022fact].[^9] We generally failed to uncover evidence that such methodological differences explain a meaningful proportion of variability in demand effects. Nonetheless, it is possible that such a large number of [often unsystematic] differences between studies limits power to detect meaningful moderators in the demand characteristics literature.

[^9]: Whether meta-analysts should combine effects from within and between-subjects design has sparked considerable debate [@morris2002combining]. We felt that combining such effects was justified given that (a) effect sizes were converted into a common metric using design-specific estimates of sampling variance, (b) sensitivity analyses of assumed within-subject correlations produced virtually no change in our overall effect size estimate, and (c) we did not detect significant differences between studies that manipulated explicit hypothesis cues within- vs. between-subjects. However, if this methodological decision eventually proves to be problematic, we suspect it won't change our conclusions: that demand effects are inferentially consequential, heterogeneous, and difficult-to-explain.

Last, although we performed a large number of robustness checks, these checks certainly were not exhaustive. Future researchers may wish to consider alternative search strategies, inclusion criteria, approaches to quantifying effects, methods for estimating participant judgments, and decisions about how to model the data.

We do not deny the importance of these methodological limitations. Instead, we point out that we suspect they do little to change our conclusion: demand effects can be inferentially consequential – but are unreliable, difficult to predict, and challenging to explain.

## Concluding Remarks

Since @orne1962social famously described the idea 50+ years ago, demand characteristics have become a literal textbook methodological concern in experimental psychology [@sharpe2016frightened]. @orne1962social further suggested that demand characteristics constituted an omnipresent threat to the validity of experimental psychology, arguing that "...all experiments will have demand characteristics, and these will always have some effects" [-@orne1962social, p. 779]. Consequently, it is perhaps not surprising that significant effort has been dedicated to their study.

Unfortunately, it is not clear if much has been learned in these 50+ years. Our review suggested that demand effects can indeed be inferentially consequential -- but are also unreliable, difficult to predict, and challenging to explain. Whether such observations should undermine confidence in the utility of experimental psychology is a question we believe demands more attention.

# References

References marked with an asterisk indicate studies included in the meta-analysis.

::: {#refs custom-style="Bibliography"}
:::
