---
title             : "A meta-analysis of the effects of demand characteristics"
shorttitle        : "Demand characteristics meta-analysis"
author: 
  - name          : "Nicholas A. Coles"
    affiliation   : "1"
    corresponding : yes
    address       : "Cordura Hall, 210 Panama St, Stanford, CA 94305" 
    email         : "ncoles@stanford.edu"
  - name          : "Michael C. Frank"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Center for the Study of Language and Information, Stanford University"
authornote: |
  All materials, data, and code are available at <insert OSF link>. 
abstract: |
  MARS guidelines: The problem of relation(s) under investigation; Study eligibility criteria; Type(s) of participants included in primary studies; Meta‐analysis methods (indicating whether a fixed‐effects or random‐effects model was used); Main results (including the more important effect sizes and any important moderators of these effect sizes); Conclusion (including limitations); Implications for theory, policy, and/or practice
  
keywords          : "demand characteristics, hypothesis awareness, placebo effect, research methods, meta-analysis"
wordcount         : "TBD"
bibliography      : "r-references.bib"
floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options: 
  chunk_output_type: console
---

```{r setup, include = F}
# load packages writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("gridExtra")

# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")

# identify refs
r_refs("r-references.bib")
```

Imagine that one day a mysterious person approaches you and begins telling you about a new method for understanding humans: Colesology. The person explains that Colesology is useful for estimating causal relationships---but adds that it can sometimes be thrown off by a *methodological artifact.* When you ask the Colesologist about this artifact, they explain that it sometimes causes researchers to detect an effect that's not real, and other times causes them to miss an effect that is real. They explain that it sometimes causes relationships to be biased upward and other times causes them to be biased downward. And then they offer a confession: they don't understand how the artifact works. Because sometimes the artifact seems to matter and other times it doesn't.

If this scenario were real, you would reasonably question whether Colesology is a valid method of scientific inquiry. But what if we're not merely describing Colesology? What if we are also, to some extent, describing *experimental psychology*.

## Demand characteristics as a methodological artifact

In 1962, Martin Orne published a seminal paper highlighting a view that challenged deeply-ingrained beliefs about experimental psychology. In this paper, Orne argued that research participants are not passive responders to the experimental context. Instead, he suggested that participants actively try to make sense of the situation based on their assumptions, beliefs, and motivations. One factor that Orne believed played a particularly powerful role was a methodological artifact called *demand characteristics*: "cues which convey an experimental hypothesis to the subject" (p. 779). This idea was controversial at first, with some researchers arguing that Orne's claims about demand characteristics were vague and/or overblown [e.g., @berkowitz1971weapons; @milgram1972interpreting; @kruglanski1975human]. Nonetheless, over the next 60 years, demand characteristics would become recognized as a literal textbook methodological concern in experimental psychology [@sharpe2016frightened].

Orne initially focused on evidence that demand characteristics can lead to false positives---such as patients exhibiting sham symptoms of hypnosis [@orne1959nature]. Follow-up research, though, indicated that demand characteristics can also lead to false negatives. For example, @hayes1967two demonstrated that participants will ignore visual cues of depth when they believe that doing so is the purpose of the experiment. In addition to creating inferential errors, demand characteristics can bias estimates of causal relationships. For example, @coles2022fact found that the estimated effect of facial poses on self-reported emotion could be amplified *or* attenuated based on the communicated purpose of the study. However, not all researchers have found that demand characteristics matter. For example, in large replications of classic studies in behavioral economics, @mummolo2019demand consistently failed to find that manipulations of the communicated hypothesis impacted participants' responses.

After over 60 years, experimental psychologists are left with an uncomfortable state of affairs: demand characteristics are a literal textbook methodological concern, but it is not clear when and how their effects emerge. The goal of the current paper is to use meta-analysis to take stock of what we know---and what we don't know---about this methodological artifact. We first provide an overview of a framework designed to accommodate the potentially heterogeneous effects of demand characteristics [@rosnow1997people]. We also review a complementary framework that attempts to bridge the gap between research on demand characteristics and research on placebo effects [@coles2022fact]. We then use meta-analysis to conduct the first quantitative synthesis of strict experimental tests of the effects of demand characteristics. Through this meta-analysis, we not only estimate the overall impact of demand characteristics, but also use moderator analyses to provide preliminary tests of predictions made by @rosnow1997people and @coles2022fact. We end with a discussion of the steps we believe are required to transform vague frameworks about demand characteristics into formal theories---theories we believe might help distinguish a valid experimental psychology from the invalid methods of Colesology.

## Rosnow and Rosenthal's (1997) demand characteristics framework

@rosnow1997people proposed that there are three key moderators of the effects of demand characteristic: (1) receptivity to cues, (2) motivation to provide hypothesis-consistent responses, and (3) opportunity to alter their responses (Figure \@ref(fig:framework)).

```{r framework, fig.cap = "Rosnow and Rosenthal’s (1997) and Coles et al.’s (2022) frameworks for conceptualizing the impact of demand characteristics on participants’ responses."}
knitr::include_graphics("images/metaware_framework.png")
```

### Receptivity to the cues

@rosnow1997people argued that participants must be perceptive to demand characteristics in order for them to impact downstream responses [see also @rosnow1973mediation; @strohmetz2008research]. As an extreme example, imagine that a researcher hands an infant participant a sheet of paper that precisely explains the researcher's hypothesis. Demand characteristics are certainly present, but they are not predicted to have an impact because the infant is not receptive to the cues (i.e., cannot read).

### Motivation to provide hypothesis-consistent responses

Early in the history of research on demand characteristics, researchers debated which motivational forces typically underlie the effects of demand characteristics [for a review, see @weber1972subject]. @orne1962social originally characterized participants as "good subjects" who change their responses because they are altruistically motivated to help the researcher confirm their hypothesis. Others characterized participants as "apprehensive subjects" who are motivated to respond in a manner that will lead them to be evaluated positively [@riecken1962program; @rosenberg1969conditions; @sigall1970cooperative]. @masling1966role argued that participants sometimes interfere with the purpose of the study ["negativistic subjects", see also @cook1970demand], whereas @fillenbaun1970more argued that participants attempt to respond as naturally as possible ("faithful subjects"). Although seemingly divided, these early theorists agreed on one overarching principle: that participants' motivation to provide hypothesis-consistent responses drives the effects of demand characteristics.

Because early demand characteristic theorists often focused on a single predominant subject goal--such as the goal to help the experimenter, be evaluated positively, or respond faithfully--less attention was paid to the notion that participants may have multiple, sometimes competing motivations [@barbuto1998motivation; @boudreaux2013goal]. Indeed, when the idea of multiple motivations was examined, it was often done so to highlight the more prominent role of a specific goal [e.g., evaluation apprehension vs. motivation to help the experimenter, @sigall1970cooperative]. However, @rosnow1997people found that people have multiple goals in mind when they conceptualize their role as research participants. Participants describe their role as being similar to situations where one is being altruistic (e.g., giving to charity), being evaluated (e.g., being interviewed for a job), *and* obeying authority (e.g., obeying a no-smoking sign). All these goals may impact the extent to which participants are overall motivated to provide hypothesis-consistent responses. Furthermore, these goals can sometimes conflict. For example, imagine that an experimenter is friendly towards the participant and that the participant is thus motivated to help the experimenter. Now imagine that the participant learns that the experimenter hypothesizes that they will show a race-based preference for job applicants. In this scenario, the motivation to help the experimenter may conflict with the participant's desire to respond in a socially desirable manner.

Based on the above observations and reasoning, @rosnow1997people suggested that participants can be characterized as being motivated to either (a) non-acquiesce (i.e., not change their responses), (b) acquiesce (i.e., provide hypothesis-consistent responses), or (c) counter-acquiesce (i.e., provide hypothesis-inconsistent responses). Of course, as we later discuss, motivation might also be conceptualized on a continuum ranging from highly motivated to counter-acquiesce to highly motivated to acquiesce.

### Opportunity to alter responses

No matter how motivated they are to confirm the hypothesis, @rosnow1997people reasoned that there is variability in the extent to which participants have the opportunity to alter the outcome-of-interest. Thus, they posited that demand characteristics can only impact outcomes that participants can readily alter.

Taken together, @rosnow1997people posited that demand characteristics only bias participants responses when they (1) notice the cues, (2) are motivated to adjust their responses, and (3) are capable of adjusting their responses.

## Coles et al.'s (2022) framework

Demand characteristic theorists have traditionally conceptualized the effects of the methodological artifact as a *response bias* [@orne1962social; @rosnow1973mediation; @strohmetz2008research]. For example, demand characteristics that indicate the researcher expects an intervention to boost mood is posited to impact participants' mood *reports*--not necessarily their actual mood.

@coles2022fact argued that demand characteristics not only have the potential to lead to response biases, but also placebo biases (Figure \@ref(fig:framework)). They defined (a) response biases as changes mediated by relatively deliberate changes that participants make to their responses, and (b) placebo effects as changes mediated by relatively automatic activation of beliefs or pre-existing conditioned responses [@zion2018mindsets]. Thus, unlike @rosnow1997people, @coles2022fact argued that demand characteristics can impact responses when participants have neither the motivation nor the opportunity to adjust their responses. @coles2022fact also provided preliminary evidence that demand characteristics and placebo effects have distinct mechanisms. They found that participants' beliefs did not always match the demand characteristics manipulation--and that both the demand characteristics manipulations and measures of participants' beliefs moderated the effects of posed expressions on emotion.

# Methodology

The present meta-analysis was designed to (1) provide the first quantitative synthesis of strict experimental tests of demand effects, and (2) test predictions made by @rosnow1997people and @coles2022fact.

We defined the scope of the meta-analysis using the Population, Intervention, Comparison, Outcome framework [@schardt2007utilization]. Our population-of-interest was human subjects participating in non-clinical research studies. We excluded clinical research studies so that we could focus on research that better isolated the mechanism most often discussed in the demand characteristics literature: response biases (as opposed to placebo effects). Given that there is a sizable literature on placebo effects, excluding clinical tests of demand characteristics also helped us improve the feasibility of the project.

The intervention-of-interest was explicit manipulations of the hypothesis communicated to participants---i.e., scenarios where a researcher tells participants about the effect of an independent variable on a dependent variable. @orne1962social more broadly defined demand characteristics as *any* cue that may impact participants' beliefs about the purpose of the study, including instructions, rumors, and experimenter behavior. However, such a definition creates a boundless and blurry conceptual space where any systematic change in a research design might be considered a test of demand characteristics. Thus, to bound and simplify the conceptual space, we focused on explicit manipulations of the hypothesis communicated to participants.

Our comparison-of-interest were conditions where either no hypothesis or a different hypothesis was communicated to participants. Our outcome-of-interest was the dependent variable described in the communicated hypothesis. For example, in a study that manipulated whether the intervention is described as "mood-boosting" or "mood-dampening", the outcome-of-interest would be any measure of mood.

## Literature search

```{r literature search, include = F}
# open and process literature search data
DF.s <- 
  # open data
  read_xlsx(path = "data/metaware_EsData_raw.xlsx",
            sheet = "records.screening") %>% 
  
  # identify unpublished dissertations by identifying links that contain the word 'dissertation'
  mutate(dissertation = 
           if_else(condition = grepl("dissertation", link),
                   true = 1,
                   false = 0)
         )

# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>% 
  filter(!is.na(Database)) %>% 
  nrow()

# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>% 
  filter(dissertation == 1) %>% 
  nrow()
```

Our literature search strategy was developed in consultation with a librarian at Stanford University. Given the broad nature of the demand characteristics construct, we determined that a truly comprehensive strategy was not feasible (see Limitations section). Thus, we sought to design a strategy that best balanced comprehensiveness and feasibility.

We searched APA PsycInfo using broad search terms: "demand characteristics" OR "hypothesis awareness". This yielded `r r.pi` records. We additionally released a call for unpublished studies on the Society for Personality and Social Psychology Open Forum; Twitter; the Facebook Psychological Methods Discussion group; and the Facebook PsychMAP group. This yielded `r nrow(DF.s) - r.pi` additional records. In total, `r r.unp` of the records were unpublished.

## Screening

```{r final.df, include = F}
# open clean effect size data
DF.es <- 
  read_csv(file = "data/metaware_data_clean.csv")

# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>% 
  unique() %>% 
  length()

# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>% 
  unique() %>% 
  length()

# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>% 
  filter(id.study == 18) %>% 
  summarise(max.es = min(es)) %>% #  using min because largest value is neg
  round(2)
```

To be eligible for inclusion in the meta-analysis, the following criteria must have been met:

-   The researcher manipulated what participants were told about the effect of an independent variable on a dependent variable. This included both *positive demand* (participants told that the dependent variable will increase), *negative demand* (participants told that the dependent variable will decrease) and *nil demand* (participants told the dependent variable will be unaffected) conditions.

    We excluded scenarios where the researcher described an effect that was *non-directional*. We did so because participants in these scenarios could not readily infer how their responses should change. For example, if participants were told that an independent variable would "impact mood", it is not clear if participants should infer that the mood will be boosted (positive demand) or dampened (negative demand).

-   The demand characteristics manipulation was not strongly confounded. For example, a study by @sigall1970cooperative was excluded because the manipulation of the stated hypothesis was confounded with a disclosure about the meaning of the behavior. Specifically, participants were either informed or not informed that the researcher expected them to copy a large quantity of numbers. When participants were informed about this hypothesis, they were also told that such behavior would be indicative of an undesirable personality trait.

-   Information necessary for computing at least one effect size was included.

N. C. and a research assistant screened records independently, reviewed potentially relevant records together, and worked together to code the information for moderator analyses and effect size computations. Disagreements and discrepancies were resolved through discussion. It total, `r num.s` studies from `r num.p` records were eligible for inclusion. However, one record [@allen2012demand] was removed because the information reported led to implausibly large effect size estimates (e.g., $d$ = `r outlier.es`).

```{r clean.env.1, include = F}
# remove outlier and re-initialize id factors
DF.es <- DF.es %>% 
  filter(id.study != 18) %>% 
  mutate(id.study = factor(id.study),
         id.es = factor(id.es))

# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
```

## Effect size index

We used standardized mean difference scores (Cohen's $d_{s}$ and $d_{rm}$) as our effect size index [@borenstein2009effect; @cohen1988statistical].

In some scenarios, we estimated the main effect of demand characteristics. For example, @coles2022fact manipulated whether participants were told that posing smiles would increase happiness. Here, the main effect of demand characteristics can be computed by comparing happiness ratings from smiling participants who were either informed or not informed about its expected effect.

In other scenarios, we estimated the interactive effect of demand characteristics. For example, in the same @coles2022fact study, participants provided happiness ratings both after smiling and scowling. Participants' mood generally improved when smiling vs. scowling (i.e., there was a main effect of facial pose). However, the difference was more pronounced when participants were told about the mood-boosting effects of smiling. In other words, there was an interaction between facial pose and demand characteristics. In this scenario, the interactive effect of demand characteristics was computed by calculating a standardized difference-in-differences score. These scores were computed similar to Cohen's $d_{s}$ and $d_{rm}$, but with mean-difference scores (as opposed to means).

Effect sizes were calculated so that positive values indicated an effect consistent with the demand characteristics manipulation (i.e., acquiescence occured). For example, if participants were told that an intervention should increase mood, an increase in mood would be coded as a positive effect. If participants were told that an intervention should decrease mood, an increase in mood would be coded as a negative effect.

```{r corr.sens, include = F}
# examine how assumed repeated measures correlation impacts general pattern of results

# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")

# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <- 
  sapply(X = sens.df.list, 
         FUN = function(i){
           # open data
           df <- read.csv(paste0("data/r_sensitivity/",
                                    i)
                             ) 
           # fit model
           m <- rma.mv(yi = es,
                       V = es.var,
                       data = DF.es,
                       random = ~ 1 | id.study / id.es)
           
           # return overall es as a number
           m$b %>% 
             as.numeric() %>% 
             return()
           }
         )

# compute range of es values
sens.range <- max(sens.res) - min(sens.res)

# delete vestigial
rm(sens.df.list, sens.res)
```

For repeated-measure comparisons, the correlation between the repeated measures is needed to calculate Cohen's $d_{rm}$. This correlation is rarely reported, so we followed a recommendation by @borenstein2009effect and performed sensitivity analyses on an assumed correlation. We preregistered a default correlation of $r$ = .50 but performed sensitivity analysis with $r$ = .10, .30, .50, .70, and .90. These sensitivity analyses only produced a `r sens.range` range in overall effect size estimates---so we do not discuss them further.

```{r mult.eff, include = F}
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>% 
  # identify number of effect sizes for each study (id)
  group_by(id.study) %>% 
  count() %>% 
  
  # code whether each study has more than one effect size
  mutate(dep = if_else(condition = n > 1,
                       true = 1,
                       false = 0)
         ) %>% 
  
  # calculate proportion of studies with more than one effect size 
  ungroup() %>% 
  summarise(mult.eff = mean(dep)) %>% 
  
  # export as percentage
  as.numeric() %>% 
  round(digits = 2) * 100
```

`r mult.eff.per`% of studies contained multiple effect sizes of interest. For example, the full design in Coles et al. (2022) included a positive demand, nil demand, and control condition. Participants also completed several facial expression poses (happy, angry, and neutral) and self-reported several emotions (happiness and anger). To be comprehensive, we recorded all reported effect sizes and account for dependencies in our models (described later).

## Types of demand characteristic comparisons

Cohen's $d$ represents a standardized mean difference between two groups. Often, this involved a single demand characteristic condition (positive, negative, or nil demand) compared to a control group. Sometimes, however, this comparison involved two demand characteristic conditions (e.g., positive demand vs. negative demand). We thus coded whether the comparison involved one vs. two demand characteristic conditions. In addition, we coded each type of comparison: positive demand vs. control, nil demand vs. control, negative demand vs. control, positive vs. nil demand , positive vs. negative demand, and nil vs. negative demand.

## Post-hoc measures of motivation, opportunity, and belief

```{r vig.desc, include = F}
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>% 
  nrow()
```

Both @rosnow1997people and @coles2022fact posited that the effects of demand characteristics are moderated by participants' (1) motivation to provide hypothesis-consistent responses, and (2) opportunity to adjust their responses (Figure \@ref(fig:framework)). @coles2022fact additionally predicted a third moderator: (3) participants' belief in the hypothesized effect. Unfortunately, these variables were rarely estimated in the studies included in the meta-analysis.

As an indirect measure of these moderators-of-interest, we estimated their values through a new set of participants. For each demand characteristic condition and dependent variable combination, we created vignettes that described key study details. For example, @standing2008demonstration had two demand characteristics manipulations (positive and negative demand) and two dependent variables (measures of verbal and spatial reasoning). Thus, we created four vignettes for this study (see Figure \@ref(fig:vig)).

In total, there were `r vig.n` vignettes. We did not create vignettes for control conditions because participants were not given information about the experimenter's hypothesis. Because there were no explicit demand characteristics to act upon, we left motivation, belief, and opportunity values blank for this condition.

```{r vig, fig.cap = "Vignettes for Standing et al., 2008."}
knitr::include_graphics("images/metaware_vigs.png")
```

```{r survey.n, include = F}
# identify number of participants who completed the survey
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>% 
  nrow()
```

`r survey.n` undergraduates from Stanford University reviewed 10 randomly-selected vignettes in exchange for course credit. For each vignette, raters were asked to first identify the researcher's hypothesis. Here, participants chose between four options that described a filler effect (usually involving a different dependent variable) or a positive, negative, or nil effect of the independent variable on the dependent variable. Afterwards, they rated the extent to which they would (1) be motivated to provide hypothesis-consistent responses (-3 = "extremely motivated to adjust responses to be inconsistent" to 3 = "extremely motivated to adjust responses to be consistent"), (2) be able to adjust their responses on the outcome-of-interest (0 = "extremely incapable" to 4 = "extremely capable), and (3) believe the experimenter's hypothesis (-3 ="strong disbelief" to 7 = "strong belief"). Raters also indicated whether they believed the actual participants would confirm the hypothesis, which we discuss later. These questions were presented in random order.

For each vignette, ratings were removed if the rater did not correctly identify the communicated hypothesis. The remaining ratings were averaged across raters to provide mean estimates of motivation, opportunity, and belief.

### Accounting for different demand comparisons

```{r mods, fig.cap = "Hypothetical data from a study where a procedure is either described as mood-boosting (positive demand), described as mood-dampening (negative demand), or not described at all (control). Data provides examples of how the effects of demand characteristics (d) on self-reported mood are moderating by participants' reports of their motivation to confirm the stated hypothesis (m, Panel A), belief in the stated hypothesis (b, Panel B), and opportunity to adjust responses (c, Panel C). In each panel, separate examples are provided for scenarios where motivation is invariant (Column 1) and variant (Column 2) across levels of demand characteristics"}
knitr::include_graphics("images/metaware_mods.png")
```

As we've mentioned above, Cohen's $d$ represents the standardized difference between *two* groups. Thus, for each effect size, we summed the motivation, opportunity, and belief ratings for the two groups being compared. Doing so allowed us to accommodate the fact that some comparisons involved two demand characteristics conditions. For example, imagine a study where participants are told a procedure will boost mood (positive demand), told a procedure will dampen mood (negative demand), or not told about an expected effect (control). Compared to a control condition, participants who are motivated to confirm the hypothesis will have upward-biased responses in the positive demand condition and downward-biased responses in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 1). When comparing the two demand conditions, the size of the demand effect should be doubled because the motivational forces in the two conditions produce an additive effect. Alternatively, these motivational forces could hypothetically cancel each other out. This might happen if participants were (a) motivated to confirm the hypothesis in the positive demand condition, and (b) motivated to disconfirm the hypothesis in the negative demand condition (see Figure \@ref(fig:mods), Panel A, Column 2). Summing motivation scores allowed us to accommodate this possibility, and we used the same approach for belief (Panel B) and opportunity ratings (Figure \@ref(fig:mods), Panel C).

We did not include nil-hypothesis comparisons in our analyses because our coding strategy could not accommodate the potential moderating role of motivation and belief in this condition. For example, imagine that a participant is (a) told that an intervention will not impact mood (nil demand), and (b) is extremely motivated to disconfirm the hypothesis. Relative to a control condition, this participant could disconfirm the hypothesis by either increasing *or* decreasing their mood report. Thus, even if motivation does moderate the effects of demand characteristics, we would not expect a systematic pattern to emerge with our coding scheme.

## Rater forecasts of demand effects

Even if researchers cannot explain how demand characteristics work, it might be valuable to be able to predict their effects [@yarkoni2017choosing]. Orne suggested that one group that may be particularly good at predicting these effects is participants themselves [@orne1969demand]. To examine this, raters who reviewed the vignettes also predicted whether other participants would confirm vs. disconfirm the researcher's hypothesis (-3 = "extremely likely to adjust responses to be inconsistent" to 3 = "extremely likely to adjust responses to be consistent"). We processed these data using the same approach as the motivation, opportunity, and belief scores (e.g., summed ratings when comparing two demand conditions).

## Other moderators

We also coded several moderators that researchers have speculated may moderate demand effects but for which we personally had no a-priori hypotheses. This included: (1) whether the sample was student, non-student (e.g., MTurk), or mixed, (2) whether the study was conducted online or in-person, (3) whether the comparison involved one (e.g., positive demand vs. control) or two demand conditions (e.g., positive vs. negative-demand), (4) in the former, whether a positive, nil, or negative demand manipulation was used, (5) whether demand characteristics were manipulated within- vs. between-subjects, (6) the year the record was completed or published, and (7) whether participants were paid or unpaid.

## Meta-analytic approach

`r mult.eff.per`% of studies in our meta-analysis contained multiple effect sizes of interest. To model this nested structure, we used random-effect three-level meta-analysis (3LMA; also referred to as "multivariate" or "multilevel" meta-analysis). 3LMA accommodates nested effect sizes by modeling three sources of variability: the sampling error of individual studies (level 1), variability within studies (level 2), and variability between studies (level 3; often referred to as "random effect"). To estimate the overall effect size, we fit an intercept-only 3LMA model. For moderator analyses, continuous and dummy-coded categorical moderators were separately entered into the model. For categorical moderators, we used the models to estimate overall effect sizes within each subgroup of the moderator.

```{r clean.env.2, include = F}
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
```

### Publication bias analyses

Publication bias refers to the well-documented propensity for hypothesis-inconsistent findings to be disproportionately omitted from the published scientific record [@franco2014publication]. When present, publication bias can lead to inaccurate effect size estimates and inferential errors. Consequently, we used three main approaches for assessing and correcting for potential publication bias in our overall effect size model.

First, we visually examined *funnel plots,* wherein observed effect sizes are plotted against a measure of their precision (e.g., standard error). In the absence of publication bias, the distribution typically resembles a funnel; relatively large studies estimate the effect with high precision, and effect sizes fan out in *both* directions as the studies become smaller. If, however, non-significant findings are omitted from the scientific record (i.e., there is publication bias), the distribution is often asymmetric/sloped. Funnel plots traditionally contain one effect size per study. However, given that many of our studies produced multiple effect sizes, we also examined funnel plots where non-independent effect sizes were aggregated.

Second, we conducted precision-effect tests [@stanley2014meta]. In precision-effect tests, the relationship between observed effect sizes and their standard errors---which would be absent when there is no publication bias---is estimated and controlled for in a meta-regression model. This subsequently produces estimates of publication bias and the bias-corrected overall effect. Precision-effect tests were developed and validated for meta-analyses with independent effect sizes. Nonetheless, @rodgers2021evaluating demonstrated that the method retains fairly good statistical properties when 3LMA is used.

Third, we used weight-function modeling [@vevea1995general]. In weight-function modeling, weighted distribution theory is used to model biased selection based on the significance of observed effects, which is then compared to an unadjusted model. If the adjusted model provides increased fit, publication bias is a concern and the model can be used to estimate the bias-corrected overall effect size. Once again, weight-function modeling was designed for meta-analyses with independent effect sizes. Nonetheless, @rodgers2021evaluating demonstrated that the method retains fairly good statistical properties when non-independent effect sizes are aggregated.

As a sensitivity analysis, we included publication status as a dummy-coded predictor to our overall-effect 3LMA. This allowed us to estimate the difference in the magnitude of published vs. unpublished effects.

# Results

```{r overall, include = F}
# estimate overall effect size
overall <- 
  rma.mv(yi = es,
         V = es.var,
         data = DF.es,
         random = ~ 1 | id.study / id.es)
# overall <-
#   rma.uni(yi = es,
#           vi = es.var,
#           data = DF.es,
#           method = "REML") %>% 
#   robust(cluster = DF.es$id)
```

Results indicated that, overall, explicit manipulations of demand characteristics create a small acquiescence effect, $d$ = `r overall$b`, 95% CI [`r overall$ci.lb`, `r overall$ci.ub`], $t$ = `r overall$zval`, $p$ `r printp(overall$pval)`. In other words, when explicitly told a hypothesis, participants responses tend to shift in a manner consistent with that hypothesis.

### Moderator analyses

```{r mod, include = F}
# create moderator analysis function
ModAnalysis = function(m, df = DF.es) {
  
  # set dataset
  df <- df
  
  # moderator analysis
  mod.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ ", m)),
                  test= "t")
  
  sub.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = as.formula(paste0("~ 0 + ", m)),
                  test= "t")
  
  # return results as list
  return(list(mod = mod.m,
              sub = sub.m)) 
}

# conduct moderator and subgroup analyses for moderators assessed with full dataset 
mod.l <- c("student", "paid", "online", 
           "design", "ref.r", "published",
           "year")
mod.r <- 
  sapply(X = mod.l,
         simplify = F,
         FUN = ModAnalysis)

rm(mod.l)

# test ref.type moderator in scenarios where there is a control comparison (i.e., ref.r == single)
mod.r[["ref.type"]] <- 
  ModAnalysis(m = "ref.type",
              df = DF.es[DF.es$ref.r == "single", ])

# add motivation, opportunity, belief, and prediction moderators
## Note: comparisons with nil-demand conditions are excluded
mod.r.2 <- 
  sapply(X = c("mot", "opp", "bel", "pre"),
         simplify = F,
         FUN = ModAnalysis,
         df = DF.es %>% 
               filter(ref.type != "cvz" &
                        ref.type != "pvz"))

## combine results
mod.r = c(mod.r, mod.r.2)

# delete vestigial 
rm(ModAnalysis, mod.r.2)
```

The observed effects of demand characteristics were highly heterogeneous. Indeed, the between-study ($\tau$ = `r sqrt(overall$sigma2[1])`) and within-study ($\sigma$ = `r sqrt(overall$sigma2[2])`) effect standard deviation was approximately the same size of the overall effect. This suggests that moderators (as opposed to mere sampling error) are producing meaningful shifts in the effects of demand characteristics.

#### Motivation, opportunity, and belief

Contrary to predictions made by @rosnow1997people and @coles2022fact, we did not find that the effects of demand characteristics were moderated by ratings of the extent to which participants would be motivated to confirm the experimenter's hypothesis, $\beta$ = `r mod.r$mot$mod$b["mot", ]`, 95% CI [`r mod.r$mot$mod$ci.lb[2]`, `r mod.r$mot$mod$ci.ub[2]`], $t$(`r mod.r$mot$mod$ddf[["mot"]]`) = `r mod.r$mot$mod$zval[2]`, $p$ = `r printp(mod.r$mot$mod$pval[2])`. We also did not find that demand characteristic effects were moderated by ratings of the extent to which participants could control the outcome of interest, $\beta$ = `r mod.r$opp$mod$b["opp", ]`, 95% CI [`r mod.r$opp$mod$ci.lb[2]`, `r mod.r$opp$mod$ci.ub[2]`], $t$(`r mod.r$opp$mod$ddf[["opp"]]`) = `r mod.r$opp$mod$zval[2]`, $p$ = `r printp(mod.r$opp$mod$pval[2])`. Consistent with @coles2022fact, however, demand characteristic effects were larger when raters indicated that they believed the experimenter's hypothesis, $\beta$ = `r mod.r$bel$mod$b["bel", ]`, 95% CI [`r mod.r$bel$mod$ci.lb[2]`, `r mod.r$bel$mod$ci.ub[2]`], $t$(`r mod.r$bel$mod$ddf[["bel"]]`) = `r mod.r$bel$mod$zval[2]`, $p$ = `r printp(mod.r$bel$mod$pval[2])`.

#### Other moderators

Results indicated that the effects of demand characteristics were moderated by participant pool, $F$(`r mod.r$student$mod$QMdf[1]`, `r mod.r$student$mod$QMdf[2]`) = `r mod.r$student$mod$QM`, $p$ = `r printp(mod.r$student$mod$QMp)`. The effects were medium-to-large in studies with students ($d$ = `r mod.r$student$sub$b[3]`, 95% CI [`r mod.r$student$sub$ci.lb[3]`, `r mod.r$student$sub$ci.ub[3]`], $p$ `r printp(mod.r$student$sub$pval[3])`), extremely small in studies with a mix of students and non-students ($d$ = `r mod.r$student$sub$b[1]`, 95% CI [`r mod.r$student$sub$ci.lb[1]`, `r mod.r$student$sub$ci.ub[1]`], $p$ = `r printp(mod.r$student$sub$pval[1])`), and near-zero in studies with non-students ($d$ = `r mod.r$student$sub$b[2]`, 95% CI [`r mod.r$student$sub$ci.lb[2]`, `r mod.r$student$sub$ci.ub[2]`], $p$ = `r printp(mod.r$student$sub$pval[2])`). The effects of demand characteristics also tended to be larger for in-person ($d$ = `r mod.r$online$sub$b[1]`, 95% CI [`r mod.r$online$sub$ci.lb[1]`, `r mod.r$online$sub$ci.ub[1]`], $p$ `r printp(mod.r$online$sub$pval[1])`) vs. online ($d$ = `r mod.r$online$sub$b[2]`, 95% CI [`r mod.r$online$sub$ci.lb[2]`, `r mod.r$online$sub$ci.ub[2]`], $p$ = `r printp(mod.r$online$sub$pval[2])`) studies, $F$(`r mod.r$online$mod$QMdf[1]`, `r mod.r$online$mod$QMdf[2]`) = `r mod.r$online$mod$QM`, $p$ = `r printp(mod.r$online$mod$QMp)`.

The effects of demand characteristics appeared to be additive. Compared to instances where a demand characteristic condition was compared to a control group ($d$ = `r mod.r$ref.r$sub$b[2]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[2]`, `r mod.r$ref.r$sub$ci.ub[2]`], $p$ = `r printp(mod.r$ref.r$sub$pval[2])`), effect sizes were approximately twice as large when two demand characteristic conditions were compared ($d$ = `r mod.r$ref.r$sub$b[1]`, 95% CI [`r mod.r$ref.r$sub$ci.lb[1]`, `r mod.r$ref.r$sub$ci.ub[1]`], $p$ `r printp(mod.r$ref.r$sub$pval[1])`), $F$(`r mod.r$ref.r$mod$QMdf[1]`, `r mod.r$ref.r$mod$QMdf[2]`) = `r mod.r$ref.r$mod$QM`, $p$ = `r printp(mod.r$ref.r$mod$QMp)`. Instances where a demand characteristic condition was compared to a control group allowed us to additional test whether participants respond more strongly to positive, nil, or negative demand characteristics. Results indicated that they do, $F$(`r mod.r$ref.type$mod$QMdf[1]`, `r mod.r$ref.type$mod$QMdf[2]`) = `r mod.r$ref.type$mod$QM`, $p$ `r printp(mod.r$ref.type$mod$QMp)`. The effect of demand characteristics was nearly twice as large in the nil ($d$ = `r mod.r$ref.type$sub$b[2]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[2]`, `r mod.r$ref.type$sub$ci.ub[2]`], $p$ `r printp(mod.r$ref.type$sub$pval[2])`) vs. positive ($d$ = `r mod.r$ref.type$sub$b[1]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[1]`, `r mod.r$ref.type$sub$ci.ub[1]`], $p$ = `r printp(mod.r$ref.type$sub$pval[1])`), and negative demand conditions ($d$ = `r mod.r$ref.type$sub$b[3]`, 95% CI [`r mod.r$ref.type$sub$ci.lb[3]`, `r mod.r$ref.type$sub$ci.ub[3]`], $p$ = `r printp(mod.r$ref.type$sub$pval[3])`).

We did not find that the effects of demand characteristics were moderated by whether researchers manipulated demand characteristics within- ($d$ = `r mod.r$design$sub$b[1]`, 95% CI [`r mod.r$design$sub$ci.lb[1]`, `r mod.r$design$sub$ci.ub[1]`], $p$ `r printp(mod.r$design$sub$pval[1])`) vs. between-subjects ($d$ = `r mod.r$design$sub$b[2]`, 95% CI [`r mod.r$design$sub$ci.lb[2]`, `r mod.r$design$sub$ci.ub[2]`], $p$ = `r printp(mod.r$design$sub$pval[2])`), $F$(`r mod.r$design$mod$QMdf[1]`, `r mod.r$design$mod$QMdf[2]`) = `r mod.r$design$mod$QM`, $p$ = `r printp(mod.r$design$mod$QMp)`. We also did not find that the effects of demand characteristics were moderated by the year the record was completed or published, $\beta$ = `r mod.r$year$mod$b["year", ]`, 95% CI [`r mod.r$year$mod$ci.lb[2]`, `r mod.r$year$mod$ci.ub[2]`], $t$(`r sum(mod.r$year$mod$QMdf)`) = `r mod.r$year$mod$zval[2]`, $p$ = `r printp(mod.r$year$mod$pval[2])`. The effects of demand characteristics tended to be numerically larger in unpaid ($d$ = `r mod.r$paid$sub$b[1]`, 95% CI [`r mod.r$paid$sub$ci.lb[1]`, `r mod.r$paid$sub$ci.ub[1]`], $p$ `r printp(mod.r$paid$sub$pval[1])`) vs. paid ($d$ = `r mod.r$paid$sub$b[2]`, 95% CI [`r mod.r$paid$sub$ci.lb[2]`, `r mod.r$paid$sub$ci.ub[2]`], $p$ = `r printp(mod.r$paid$sub$pval[2])`) studies---but this difference was also not statistically significant, $F$(`r mod.r$paid$mod$QMdf[1]`, `r mod.r$paid$mod$QMdf[2]`) = `r mod.r$paid$mod$QM`, $p$ = `r printp(mod.r$paid$mod$QMp)`.

### Forecasts

Raters predictions about the effects of demand characteristics were associated with the observed effects, but this association was weak, $\beta$ = `r mod.r$pre$mod$b["pre", ]`, 95% CI [`r mod.r$pre$mod$ci.lb[2]`, `r mod.r$pre$mod$ci.ub[2]`], $t$(`r mod.r$pre$mod$ddf[["pre"]]`) = `r mod.r$pre$mod$zval[2]`, $p$ = `r printp(mod.r$pre$mod$pval[2])`.

Would be nice to be able to explain how much heterogeneity is explained by this variable...

### Publication bias analyses

```{r pub.bias, include = F}
##########################
# Define publication bias analysis that
# 1. Mathur and VanderWeele 2020 sensitivity analyses
# 2. Fits three-level precision-effect test
# 3a. Aggregates dependent effect sizes (with given rho value)
# 3b. Aggregated precision-effect test
# 3b. Fits Vevea and Hedges (1995) Weight-Function Model w/ aggregated effects
# 4a. Fit funnel plot
# 4b. Fit funnel plot w/ aggregated dependencies
# 5. Organizes results into list
##########################

PubBias = function(rho.val = .5){
  # 1. sensitivity analyses
  ########################
  sens <- corrected_meta(yi = DF.es$es,
                         vi = DF.es$es.var,
                         eta = 49,
                         clustervar = DF.es$id.study,
                         model = "robust",
                         favor.positive = T)
  
  # 2. three-level precision-effect test
  ########################
  pe.3l <- rma.mv(yi = es,
                  V = es.var,
                  mods = ~ sqrt(es.var),
                  data = DF.es,
                  random = ~ 1 | id.study / id.es)
  
  # 3a. aggregate dependent effect sizes
  ########################
  DF.agg <- DF.es %>%
    
    # convert to an 'escalc' object so function can run
    escalc(yi = es,
           vi = es.var,
           data = DF.es,
           measure = "SMD") %>%
    
    # delete vestigial: es is now yi; es.var is now vi
    select(-c(es, es.var)) %>% 
    
    # aggregate dependencies
    aggregate(x = .,
              cluster = id.study,
              rho = rho.val)
  
  # 3b. aggregated precision-effect test
  ########################
  pe.a <- rma.uni(yi = yi,
                  vi = vi,
                  mods = ~ sqrt(vi),
                  data = DF.agg,
                  method = "REML")

  # 3c. Weight-function model
  ########################
  weight.funct <- weightfunct(effect = DF.agg$yi,
                              v = DF.agg$vi,
                              mods = NULL,
                              weights= NULL,
                              fe = FALSE,
                              table = TRUE,
                              pval = NULL)
  
  # 4a. funnel plot
  ########################
  par(mfrow=c(1,2))
  
  rma.uni(yi = es,
          vi = es.var,
          data = DF.es,
          method = "REML") %>%   
    metafor::funnel(hlines = "lightgray",
                    xlab = "Cohen's standardized d")
  
  # 4b. funnel plot w/ aggregated dependencies
  ########################
  rma.uni(yi = yi,
          vi = vi,
          data = DF.agg,
          method = "REML") %>%   
    metafor::funnel(hlines = "lightgray",
                    xlab = "Cohen's standardized d (aggregated)")
  
  # save funnel plot as object
  funnel.plot <- recordPlot()
    
  # clear R environment
  plot.new()
  
  # 5. Organize results in list 
  ########################
  list(sens = sens,
       pe.3l = pe.3l,
       DF.agg = DF.agg,
       pe.a = pe.a,
       weight.funct = weight.funct,
       funnel = funnel.plot) %>%  
    return()
}

# for range of rho values, run publication bias analyses
rho.l = seq(from = .1,
            to = .9,
            by = .2)

pub.r <- lapply(X = rho.l,
                FUN = PubBias)

names(pub.r) = paste0("rho_", rho.l) #  name list

# delete vestigial 
rm(rho.l, PubBias)

# look at sensitivity analyses
## general story: often, but not always, find evidence of reverse publication bias (preference for negative effects)
# lapply(pub.r, function(x){x[["pet"]]})
# lapply(pub.r, function(x){x[["peese"]]})
# lapply(pub.r, function(x){x[["weight.funct"]]})
# lapply(pub.r, function(x){x[["funnel"]]})

# plot funnels
# overall %>% 
#   metafor::funnel(x = .,
#                   hlines = "lightgray",
#                   xlab = "Cohen's standardized d")
# 
# pub.r$rho_0.5$pe.3l$b[2]
# 
# pub.r$rho_0.5$weight.funct %>% View()
```

Overall, publication bias analyses were inconclusive. For example, a funnel plot containing all effect sizes appeared to indicate that publication bias favored acquiescence effects (i.e., effects where participants shifted responses to be consistent with the demand characteristics). However, a funnel plot where non-independent effect sizes were aggregated appeared to indicate the opposite: that publication bias favored non-significant or counter-acquiescence effects.

Precision-effect tests with 3LMA provided a non-significant estimate of publication bias that favored acquiescence effects, $\beta$ = `r pub.r$rho_0.5$pe.3l$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[2]`, `r pub.r$rho_0.5$pe.3l$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.3l$pval[2])`. The bias-corrected overall effect size estimate did not significantly differ from zero $d$ = `r pub.r$rho_0.5$pe.3l$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.3l$ci.lb[1]`, `r pub.r$rho_0.5$pe.3l$ci.ub[1]`], $p$ `r printp(pub.r$rho_0.5$pe.3l$pval[1])`. Precision-effect tests with aggregated non-independent effect sizes, however, estimated the opposite: that there was a slightly (but not statistically significant) preference for non-significant or counter-acquiescence effects, $\beta$ = `r pub.r$rho_0.5$pe.a$b[2]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[2]`, `r pub.r$rho_0.5$pe.a$ci.ub[2]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[2])`. The bias-corrected overall effect size estimate was thus slightly adjusted upward, $d$ = `r pub.r$rho_0.5$pe.a$b[1]`, 95% CI [`r pub.r$rho_0.5$pe.a$ci.lb[1]`, `r pub.r$rho_0.5$pe.a$ci.ub[1]`], $p$ = `r printp(pub.r$rho_0.5$pe.a$pval[1])`.

The weight-function model suggested that better fit was achieved with a model indicating that publication bias favored non-significant or counter-acquiescence effects, $\chi^2$(1) = 10.80, $p$ = .001. The bias-corrected overall effect size was thus upward-adjusted, $d$ = 0.41, 95% CI [0.19, 0.62], $p$ \< .001. We did not, however, find evidence that effect sizes differed among unpublished ($d$ = `r mod.r$published$sub$b[1]`, 95% CI [`r mod.r$published$sub$ci.lb[1]`, `r mod.r$published$sub$ci.ub[1]`], $p$ = `r printp(mod.r$published$sub$pval[1])`) and published ($d$ = `r mod.r$published$sub$b[2]`, 95% CI [`r mod.r$published$sub$ci.lb[2]`, `r mod.r$published$sub$ci.ub[2]`], $p$ `r printp(mod.r$published$sub$pval[2])`) studies $F$(`r mod.r$published$mod$QMdf[1]`, `r mod.r$published$mod$QMdf[2]`) = `r mod.r$published$mod$QM`, $p$ = `r printp(mod.r$published$mod$QMp)`.

(Not currently describing Mathur sensitivity analysis because of uncertainty around direction of bias, which must be specified in the analysis.)

```{r funnel, fig.cap = "Funnel plot of non-aggregated and aggregated effect sizes."}
pub.r$rho_0.5$funnel
```

# Discussion

Contrary to classic coneptualizations of the impact of demand characteristics () and frameworks proposed by @rosnow1997people and @coles2022fact, we did not find evidence of two moderators that should underlie response biases: motivation to adjust responses and opportunity to adjust responses. Instead, our results are unexpectedly more consistent with a more parsimonious view: that phenomena typically described by experimental psychologists as "demand effects" are actually *placebo effects*. Applied to classic work by Orne, maybe participants did not exhibit sham symptoms of hypnosis because they were motivated to please the experimenter; instead maybe they intentionally or unintentionally responded in a manner that was consistent with their personal beliefs (a self-fulfilling prophecy or placebo effect).

This still doesn't make sense when explaining why Mummolo didn't find the effect. Had above average belief ratings

```{r}
DF.es %>% 
  filter(name == "Demand effects in survey experiments: An empirical assessment") %>%  View
```

## Implications

Based on the belief that demand characteristics is a response bias moderated by motivation and opportunity to adjust responses, two recommendations have been proposed to combat demand characteristics: lower participants' (1) motivation to adjust responses (e.g., by emphasizing the importance of natural responding), and/or (2) ability to adjust responses (e.g., by using difficult-to-control measures, like implicit or physiological measures). The "demand-as-placebo" account, however, suggests that these approaches are likely to be ineffective. Regardless of whether participants are motivated or able to adjust responses, knowledge of the experimenter's hypothesis can create placebo-induced changes in their responses.

# New figure
```{r eval = F}
theme_set(theme_classic())

#################
# motivation plot
#################
# get predicted and actual values into a single dataset
mot.df <- predict(mod.r$mot$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$mot$mod$yi)

# plot
m <- ggplot(data = mot.df,
            aes(x = X.mot,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .5) +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "Cohen's d", 
       x = "motivation ratings")

rm(mot.df)

#################
# opportunity plot
#################
# get predicted and actual values into a single dataset
opp.df <- predict(mod.r$opp$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$opp$mod$yi)

# plot
o <- ggplot(data = opp.df,
            aes(x = X.opp,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .5) +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "", 
       x = "opportunity ratings")

rm(opp.df)

#################
# belief plot
#################
# get predicted and actual values into a single dataset
bel.df <- predict(mod.r$bel$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$bel$mod$yi)

# plot
b <- ggplot(data = bel.df,
            aes(x = X.bel,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .5) +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "Cohen's d", 
       x = "belief ratings")

rm(bel.df)

#################
# prediction plot
#################
# get predicted and actual values into a single dataset
pre.df <- predict(mod.r$pre$mod,
                  addx = T) %>% 
  as.data.frame() %>% 
  cbind(.,
        yi = mod.r$pre$mod$yi)

# plot
p <- ggplot(data = pre.df,
            aes(x = X.pre,
                y = yi)) +
  
  # jittered raw data
  geom_jitter(alpha = .5) +
  
  # model derived prediction line
  geom_line(aes(y = pred)) +
  
  # model derived CI
  geom_ribbon(aes(ymin = ci.lb, 
                  ymax = ci.ub), 
              alpha = .10,
              fill = "#3366FF") +
  
  # adjust labels
  labs(y = "", 
       x = "prediction ratings")

rm(pre.df)

#################
# merge plots
#################
grid.arrange(m, o, b, p,
             nrow = 2)

rm(m, o, b, p)
```


# Limitations

We had to rely on ratings of motivation, opportunity, and belief that were both (a) ad-hoc (unvalidated), and (b) post-hoc (estimated with a new group of participants).

-   This is largely because researchers rarely have measured these proposed moderators.

-   It's possible that some of these measures may have been more valid than others. For example, participants who review descriptions of studies may be better able to estimate their beliefs about the hypothesized effect than their motivation to confirm the hypothesized effect.

-   Future research can address this by more intentionally measuring and manipulating these constructs in experimental tests of the role of demand characteristics.

Our literature search was non-comprehensive.

-   For our literature review, we focused on identifying a workable conceptual space, focusing on explicit experimental manipulations of the communicated hypothesis.

-   But demand characteristics is a broad--and some might even argue vague--construct. Anything that potentially influences participants beliefs about the experimenter's hypothesis can be considered demand characteristics, including the text in informed consents and study materials, the experimenter's delivery, etc.

-   Furthermore, Orne's writings on demand characteristics often included discussions of even broader (but difficult to measure) aspects of the social situations, including social norms about participant behavior, pacts of ignorance about admitting awareness of the true purpose of the study, etc.

Publication bias analyses are problematic. Evidence of moderation is correlational. There may be confounding.

# thoughts

Compare average effect of demand to average effect in psychology. It suggest that it's plausible that these are driven by demand. When looking at the distribution, it's clear that no effect is too big to rule out concerns about demand.

Might want to plot es distribution and identify proportion of (a) non-negligible acquiescence, (b) non-negligible counter-acquiescence, and (c) non-acquiescence.

### Exploratory stuff

```{r}
df <- DF.es %>% 
               filter(ref.type != "cvz" &
                        ref.type != "pvz")

mod.m <- rma.mv(yi = es,
                  V = es.var,
                  data = df,
                  random = ~ 1 | id.study / id.es,
                  mods = ~ opp * bel,
                  test= "t")

mod.m %>% anova()
```

# To-do

[] Add Figure 4 number

[] Need to add refs

[] F-value looks off here "Instances where a demand characteristic condition was compared to a control group allowed us to additional test whether participants respond more strongly to positive, nil, or negative demand characteristics."

[] Clean up folder structure

[] Have Mike review again?

[] Update pre-registration

[] Have M.W. work on outcome type

[] Have M.W. build codebooks

[] Will have to update figures

# References

::: {#refs custom-style="Bibliography"}
:::
