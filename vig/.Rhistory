# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
#rm(f, f.list, v)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols()
View(v.DF)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols()
View(v)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1)
View(v)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent)
View(v)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num)
View(v)
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
View(v.DF)
f.list
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
#rm(f, f.list, v)
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
#rm(f, f.list, v)
write.csv(v.DF, 'metaware.vig.csv',
row.names = F)
rm(f, f.list, v)
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(f,
col.names = "V1") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
rm(f, f.list, v)
View(v.DF)
v.DF[39, ]
v.DF[39, ] %>% View()
v.DF[39, 2] %>% View()
v.DF[39, 3] %>% View()
v.DF[133, 3] %>% View()
?read.delim
v <-
# open data
read.delim(f,
col.names = "V1",
encoding = "UTF-8")
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(pattern = ".txt")
f = f.list[36]
v <-
# open data
read.delim(f,
col.names = "V1",
encoding = "UTF-8")
View(v)
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(f,
col.names = "V1",
encoding = "UTF-8") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 200) * 60 * 1000) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
rm(f, f.list, v)
write.csv(v.DF, 'metaware.vig.csv',
row.names = F)
