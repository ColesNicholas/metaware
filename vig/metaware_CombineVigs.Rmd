---
title: Combine vignettes into single data file
author: "Nicholas A. Coles and Morgan Wyatt"
editor_options: 
  chunk_output_type: console
---
This code creates a .csv file containing (1) a unique vignette identifier, (2) 3 columns containing extracted lines of text for each vignette, (3) 3 columns containing estimated reading times for each line of text, and (4) four columns containing extracted hypotheses for each vignette.

# Prep environment
```{r setup and load packages, include = FALSE}
# clean environment
rm(list = ls())

# load packages
library("tidyverse")
```

# Extract vignette lines of text and estimate reading time
Get list of all vignette files
```{r}
f.list <- list.files(path = "./vignettes",
                     pattern = ".txt")
```

Create blank dataframe to paste results in
```{r}
v.DF <- data.frame()
```

Open each vignette file, extract relevant information, and append to v.DF
```{r}
for (f in f.list){
 # open and process vignette file
  v <- 
    # open data
    read.delim(file = paste0("./vignettes/", f),
               col.names = "V1",
               encoding = "UTF-8") %>% 
    
    # remove non-vignette rows via slicing
    slice(
      # start at row after Vignette
      (which(. == "Vignette") + 1) : 
        
      # stop at end of file
      nrow(.)
      ) %>% 
    
    # identify vignette number
    mutate(V1 = if_else(grepl("#", .$V1),
                        substr(V1, 1, 9),
                        V1)
           ) %>% 
    
    # split at every new vignette
    split(cumsum(grepl("#", .$V1))
          ) %>% 
    
    # create separate column for each vignette
    map(c) %>% 
    bind_cols() %>% 
    
    # move first row to column names
    set_names(as.character(slice(., 1))) %>%
    slice(-1) %>% 
    
    # add sentence number column
    mutate(sent = seq.int(nrow(.))) %>% 
    
    # pivot long
    pivot_longer(cols = -sent) %>% 
    
    # estimate reading time in milliseconds (400 words per minute)
    mutate(
      # estimate # of words
      word.num = str_count(value, "\\w+"),
      
      # estimate reading time 
      rt = (word.num / 400) * 60 * 1000
      ) %>% 
    
    select(-word.num) %>% 
    
    # pivot wider
    pivot_wider(names_from = sent,
                values_from = c(value, rt)) 
  
  # append results to v.DF
  v.DF <<- rbind(v.DF, v)
}

rm(f, v)
```

# Extract list of hypotheses
Create blank dataframe to paste results in
```{r}
h.DF <- data.frame()
```

Open each vignette file, extract relevant information, and append to v.DF
```{r}
for (f in f.list){
  h <- 
    # open data
    read.delim(file = paste0("./vignettes/", f),
               col.names = "V1",
               encoding = "UTF-8") %>% 
      
    # identify hypotheses using the ~ identifier
    filter(grepl("~", V1)) %>% 
    
    # separate hypothesis from key
    separate(V1,
             into = c("name", "hypothesis"),
             sep = 7)
  
  # append results to h.DF
  h.DF <<- rbind(h.DF, h)
}

rm(f, f.list, h)
```

# Merge dataframe containing hypotheses (h.DF) and vignette text (v.DF)
```{r}
# prep h.DF for merge
h.DF <- h.DF %>% 
  mutate(id = substr(x = name,
                     start = 2,
                     stop = 3),
         dir = substr(x = name,
                      start = 5,
                      stop = 5)) %>% 
  select(-name) %>% 
  pivot_wider(names_from = dir,
              values_from = hypothesis)

# prep v.DF for merge
v.DF <- v.DF %>% 
  mutate(id = substr(x = name,
                     start = 2,
                     stop = 3))

# merge
DF <- full_join(v.DF, h.DF,
                by = "id") %>%
    select(-id)

rm(h.DF, v.DF)
```

# Export data
```{r}
write.csv(DF, 'metaware_VigCombined.csv',
          row.names = F)
```
