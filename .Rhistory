count.2 = DF$count.2[i])
DF$es.var[i] <- EsVarBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
}
# call EsVarBetw on cases with continuous data and a between subject designs
if (DF$es.calc[i] != "or"){
DF$es.var[i] <- EsVarBetw(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
es = DF$es[i])
}
}
}
# Chunk 10: assumed correlation
# define assumed correlation (sensitivity analyses performed later)
corr <- .5
# Chunk 11: EsWitnMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
# formula for imputing sd.diff:
# http://handbook.cochrane.org/chapter_16/16_4_6_1_mean_differences.htm
EsWitnMean <- function(m.1, sd.1, m.2, sd.2, corr){
sd.diff <- sqrt((sd.1^2) + (sd.2^2) -
(2 * corr * sd.1 * sd.2));
es <- ((m.1 - m.2) / sd.diff) * sqrt(2 * (1- corr));
return(es)
}
# Chunk 12: EsWitnTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnTval <- function(n.1, tval, corr){
es <- tval * sqrt((2 * (1 - corr)) / n.1);
return(es)
}
# Chunk 13: EsWitnFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnFval <- function(n.1, fval, corr){
es <- sqrt((2 * fval * (1- corr)) / n.1);
return(es)
}
# Chunk 14: EsVarWitn
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsVarWitn <- function(n.1, es){
es.var <- ((1 / n.1) +
((es^2) / (2 * n.1))) *
2 * (1 - corr);
return(es.var)
}
# Chunk 15: w: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "within"){
# call EsWitnMean on cases with within-subject designs and *means*
if(DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsWitnMean(m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i],
corr = corr)
}
# call EsWitnTval on cases with within-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsWitnTval(n.1 = DF$n.1[i],
tval = DF$tval[i],
corr = corr)
}
# call EsWitnFval on cases with within-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsWitnFval(n.1 = DF$n.1[i],
fval = DF$fval[i],
corr = corr)
}
# call EsVarWitn on cases with within subject designs
DF$es.var[i] <- EsVarWitn(n.1 = DF$n.1[i],
es = DF$es[i])
}
}
# Chunk 16: del var
# delete unnecessary variables
rm(corr, i,
EsBetwFval, EsBetwMean, EsBetwTval,
EsBetwPval, EsVarBetw, EsVarWitn,
EsWitnFval, EsWitnMean, EsWitnTval,
EsBetwCount, EsVarBetwCount)
# Chunk 17
DF <- DF %>%
filter(id != 18)
# Chunk 18: specify es direction
DF <- DF %>%
# specify direction of the effect size
rowwise() %>%
mutate(es = if_else(condition = direction == "positive",
true = abs(es),
false = abs(es) * -1)
) %>%
ungroup()
# Chunk 19
write.csv(DF %>%
filter(!is.na(es)),
'metaware.data_clean.csv',
row.names = FALSE)
# Chunk 20
# overall d= 0.28, 95% CI = [0.16, 0.39], T = 0.28
overall <-
robu(formula = es ~ 1,
data = DF,
studynum = id,
var.eff.size = es.var,
modelweights = "CORR",
small = FALSE)
# get imperfect estimate of I2: 88.50%
## RVE approach doesn't give an I2
rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML")
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
'readxl', 'robumeta', 'MAd', 'weightr',
'gtools')
ipak(packages)
rm(packages, ipak)
# turn scientific notation off
options(scipen = 999)
# do we need Type 3 SS?
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
# delete unnecessary variables
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note)) %>%
# convert factors to factors
mutate(id = as.factor(id),
published = as.factor(published),
student = as.factor(student),
paid = as.factor(paid),
online = as.factor(online),
ref.type = as.factor(ref.type))
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# record by year
tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 32 studies
DF$name %>% unique() %>% length()
# 228 effect sizes
nrow(DF)
?robu
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
'readxl', 'robumeta', 'MAd', 'weightr',
'gtools')
ipak(packages)
rm(packages, ipak)
# turn scientific notation off
options(scipen = 999)
# do we need Type 3 SS?
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
# delete unnecessary variables
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note)) %>%
# convert factors to factors
mutate(id = as.factor(id),
published = as.factor(published),
student = as.factor(student),
paid = as.factor(paid),
online = as.factor(online),
ref.type = as.factor(ref.type))
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# record by year
tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 31 studies
DF$name %>% unique() %>% length()
# 222 effect sizes
nrow(DF)
# Chunk 3: define function EsBetwMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 226
EsBetwMean <- function(n.1, m.1, sd.1,
n.2, m.2, sd.2){
sd.within <- sqrt((((n.1 - 1) * (sd.1^2)) +
((n.2 - 1) * (sd.2^2))) /
(n.1 + n.2 - 2));
es <- (m.1 - m.2) / sd.within;
return(es)
}
# Chunk 4: define function EsBetwTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwTval <- function(n.1, n.2, tval){
es <- tval * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 5: define function EsBetwFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwFval <- function(n.1, n.2, fval){
es <- sqrt((fval * (n.1 + n.2)) /
(n.1 * n.2));
return(es)
}
# Chunk 6: define function EsBetwPval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwPval <- function(n.1, n.2, pval){
# calculate the inverse of the cumulative distribution function of t
t.inv <- qt(p = (pval / 2),
df = (n.1 + n.2 - 2),
lower.tail = FALSE);
es <- t.inv * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 7: define function EsVarBetw
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsVarBetw <- function(n.1, n.2, es){
es.var <- ((n.1 + n.2) / (n.1 * n.2)) +
((es^2) / (2 * (n.1 + n.2)));
return(es.var)
}
# Chunk 8: define functions EsBetwCount and EsVarBetwCount
# cohen's d
EsBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.8
or <- (count.1 * (n.2 - count.2)) /
((n.1 - count.1) * count.2);
# calculate log odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.9
log.or = log(or);
# convert log odds ratio to Cohen's d
# formula: Borenstein et al. 2011; p . 47 Equation 7.1
es <- log.or * (sqrt(3) / pi);
return(es)
}
# variance of cohen's d
EsVarBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate log odds ratio variance
# formula: Borenstein et al. 2011; p. 36; Equation 5.10
log.or.var = (1 / count.1) +
(1 / (n.2 - count.2)) +
(1 / (n.1 - count.1)) +
(1 / count.2);
# convert log odds ratio variance to variance of Cohen's d
# formula: Borenstein et al. 2011; p. 47 Equation 7.2
es.var = log.or.var * (3 / pi^2);
return(es.var)
}
# Chunk 9: b: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "between"){
# call EsBetwMean on cases with between-subject designs and *means*
if (DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsBetwMean(n.1 = DF$n.1[i],
m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
n.2 = DF$n.2[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i])
}
# call EsBetwTval on cases with between-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsBetwTval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
tval = DF$tval[i])
}
# call EsBetwFval on cases with between-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsBetwFval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
fval = DF$fval[i])
}
# call EsBetwPval on cases with between-subject designs and *p-values*
if (DF$es.calc[i] == "p") {
DF$es[i] <- EsBetwPval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
pval = DF$pval[i])
}
# call EsBetwCount and EsVarBetwCount on cases with between-subject designs and *count data*
if (DF$es.calc[i] == "or") {
DF$es[i] <- EsBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
DF$es.var[i] <- EsVarBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
}
# call EsVarBetw on cases with continuous data and a between subject designs
if (DF$es.calc[i] != "or"){
DF$es.var[i] <- EsVarBetw(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
es = DF$es[i])
}
}
}
# Chunk 10: assumed correlation
# define assumed correlation (sensitivity analyses performed later)
corr <- .5
# Chunk 11: EsWitnMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
# formula for imputing sd.diff:
# http://handbook.cochrane.org/chapter_16/16_4_6_1_mean_differences.htm
EsWitnMean <- function(m.1, sd.1, m.2, sd.2, corr){
sd.diff <- sqrt((sd.1^2) + (sd.2^2) -
(2 * corr * sd.1 * sd.2));
es <- ((m.1 - m.2) / sd.diff) * sqrt(2 * (1- corr));
return(es)
}
# Chunk 12: EsWitnTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnTval <- function(n.1, tval, corr){
es <- tval * sqrt((2 * (1 - corr)) / n.1);
return(es)
}
# Chunk 13: EsWitnFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnFval <- function(n.1, fval, corr){
es <- sqrt((2 * fval * (1- corr)) / n.1);
return(es)
}
# Chunk 14: EsVarWitn
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsVarWitn <- function(n.1, es){
es.var <- ((1 / n.1) +
((es^2) / (2 * n.1))) *
2 * (1 - corr);
return(es.var)
}
# Chunk 15: w: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "within"){
# call EsWitnMean on cases with within-subject designs and *means*
if(DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsWitnMean(m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i],
corr = corr)
}
# call EsWitnTval on cases with within-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsWitnTval(n.1 = DF$n.1[i],
tval = DF$tval[i],
corr = corr)
}
# call EsWitnFval on cases with within-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsWitnFval(n.1 = DF$n.1[i],
fval = DF$fval[i],
corr = corr)
}
# call EsVarWitn on cases with within subject designs
DF$es.var[i] <- EsVarWitn(n.1 = DF$n.1[i],
es = DF$es[i])
}
}
# Chunk 16: del var
# delete unnecessary variables
rm(corr, i,
EsBetwFval, EsBetwMean, EsBetwTval,
EsBetwPval, EsVarBetw, EsVarWitn,
EsWitnFval, EsWitnMean, EsWitnTval,
EsBetwCount, EsVarBetwCount)
# Chunk 17
DF <- DF %>%
filter(id != 18)
# Chunk 18: specify es direction
DF <- DF %>%
# specify direction of the effect size
rowwise() %>%
mutate(es = if_else(condition = direction == "positive",
true = abs(es),
false = abs(es) * -1)
) %>%
ungroup()
# Chunk 19
write.csv(DF %>%
filter(!is.na(es)),
'metaware.data_clean.csv',
row.names = FALSE)
# Chunk 20
# overall d= 0.28, 95% CI = [0.16, 0.39], T = 0.28
overall <-
robu(formula = es ~ 1,
data = DF,
studynum = id,
var.eff.size = es.var,
modelweights = "CORR",
small = FALSE)
# get imperfect estimate of I2: 88.50%
## RVE approach doesn't give an I2
rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML")
# Chunk 21
# create blank dataframe
mod.r <- data.frame(mod = character(),
level = character(),
s = integer(),
k = integer(),
d = integer(),
f = integer(),
LB = integer(),
UB = integer(),
p = integer())
# create list of moderators
mod.l <- c("student", "paid", "online",
"design", "ref.type", "published")
mod = "student"
m <-
robu(formula = as.formula(paste0("es ~ ", mod)),
data = DF,
studynum = id,
var.eff.size = es.var,
modelweights = "CORR",
small = FALSE)
m
m <- rma.uni(yi = es,
vi = es.var,
data = DF,
mods = as.formula(paste0("~ ", mod)),
method = "REML") %>%
robust(cluster = DF$id)
m
m1 <- rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML") %>%
robust(cluster = DF$id)
m2 <- rma.uni(yi = es,
vi = es.var,
data = DF,
mods = "~  student",
method = "REML") %>%
robust(cluster = DF$id)
m2 <- rma.uni(yi = es,
vi = es.var,
data = DF,
mods = ~  student,
method = "REML") %>%
robust(cluster = DF$id)
anova(m1, m2)
?Wald_test
library(clubSandwich)
?Wald_test
Wald_test(m2)
m2
DF$student %>% unique()
Wald_test(m2,
constraints = constrain_zero(2:3),
vcov = "CR2")
Wald_test(m2,
constraints = constrain_zero(2:3),
vcov = "CR2",
cluster = DF$id)
m
m2
?robust
