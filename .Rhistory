sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: clean.env.2
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
# Chunk 9: overall
# estimate overall effect size
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])
# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
mean = overall$b,
sd = tau,
lower.tail = F) %>%
round(2) * 100
h.i <- pnorm(q = (-.10),
mean = overall$b,
sd = tau,
lower.tail = T) %>%
round(2) * 100
?qnorm
dnorm(x = .5,
sd = 1)
rnorm(n = 1000000,
mean = 0,
sd = 1) %>% hist()
rnorm(n = 1000000,
mean = 0,
sd = 1) %>% min()
rnorm(n = 1000000,
mean = 0,
sd = 1) %>% max()
rnorm(n = 1000000,
mean = overall$b,
sd = tau) %>% max()
rnorm(n = 1000000,
mean = overall$b,
sd = tau) %>% min()
rnorm(n = 100000000000,
mean = overall$b,
sd = tau) %>% min()
rnorm(n = 1000000000,
mean = overall$b,
sd = tau) %>% min()
rnorm(n = 100000000,
mean = overall$b,
sd = tau) %>% min()
# Chunk 1: setup and load packages
# load libraries
library("tidyverse")
library("readxl")
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp (for aesthetics)
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note, outcome)) %>%
# create ref.r var that indicates whether there were one or two demand conditions
mutate(ref.r =
if_else(ref.type == "cvp" |
ref.type == "nvc" |
ref.type == "cvz",
true = "single",
false = "double")) %>%
# create study and es_id columns
rename(id.study = id) %>%
mutate(id.es = 1 : nrow(DF)) %>%
# convert factors to factors
mutate(across(.cols = c(id.study, id.es, published, student,
paid, online, ref.type,
ref.r),
.fns = as.factor)
)
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# Chunk 3: define function EsBetwMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 226
EsBetwMean <- function(n.1, m.1, sd.1,
n.2, m.2, sd.2){
sd.within <- sqrt((((n.1 - 1) * (sd.1^2)) +
((n.2 - 1) * (sd.2^2))) /
(n.1 + n.2 - 2));
es <- (m.1 - m.2) / sd.within;
return(es)
}
# Chunk 4: define function EsBetwTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwTval <- function(n.1, n.2, tval){
es <- tval * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 5: define function EsBetwFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwFval <- function(n.1, n.2, fval){
es <- sqrt((fval * (n.1 + n.2)) /
(n.1 * n.2));
return(es)
}
# Chunk 6: define function EsBetwPval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwPval <- function(n.1, n.2, pval){
# calculate the inverse of the cumulative distribution function of t
t.inv <- qt(p = (pval / 2),
df = (n.1 + n.2 - 2),
lower.tail = FALSE);
es <- t.inv * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 7: define function EsVarBetw
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsVarBetw <- function(n.1, n.2, es){
es.var <- ((n.1 + n.2) / (n.1 * n.2)) +
((es^2) / (2 * (n.1 + n.2)));
return(es.var)
}
# Chunk 8: define functions EsBetwCount and EsVarBetwCount
# cohen's d
EsBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.8
or <- (count.1 * (n.2 - count.2)) /
((n.1 - count.1) * count.2);
# calculate log odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.9
log.or = log(or);
# convert log odds ratio to Cohen's d
# formula: Borenstein et al. 2011; p . 47 Equation 7.1
es <- log.or * (sqrt(3) / pi);
return(es)
}
# variance of cohen's d
EsVarBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate log odds ratio variance
# formula: Borenstein et al. 2011; p. 36; Equation 5.10
log.or.var = (1 / count.1) +
(1 / (n.2 - count.2)) +
(1 / (n.1 - count.1)) +
(1 / count.2);
# convert log odds ratio variance to variance of Cohen's d
# formula: Borenstein et al. 2011; p. 47 Equation 7.2
es.var = log.or.var * (3 / pi^2);
return(es.var)
}
# Chunk 9: b: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "between"){
# call EsBetwMean on cases with between-subject designs and *means*
if (DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsBetwMean(n.1 = DF$n.1[i],
m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
n.2 = DF$n.2[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i])
}
# call EsBetwTval on cases with between-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsBetwTval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
tval = DF$tval[i])
}
# call EsBetwFval on cases with between-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsBetwFval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
fval = DF$fval[i])
}
# call EsBetwPval on cases with between-subject designs and *p-values*
if (DF$es.calc[i] == "p") {
DF$es[i] <- EsBetwPval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
pval = DF$pval[i])
}
# call EsBetwCount and EsVarBetwCount on cases with between-subject designs and *count data*
if (DF$es.calc[i] == "or") {
DF$es[i] <- EsBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
DF$es.var[i] <- EsVarBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
}
# call EsVarBetw on cases with continuous data and a between subject designs
if (DF$es.calc[i] != "or"){
DF$es.var[i] <- EsVarBetw(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
es = DF$es[i])
}
}
}
# Chunk 10: assumed correlation
# if no correlation is defined, set it at .5
if(!exists("corr")){
corr <- .5
}
# Chunk 11: EsWitnMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
# formula for imputing sd.diff:
# http://handbook.cochrane.org/chapter_16/16_4_6_1_mean_differences.htm
EsWitnMean <- function(m.1, sd.1, m.2, sd.2, corr){
sd.diff <- sqrt((sd.1^2) + (sd.2^2) -
(2 * corr * sd.1 * sd.2));
es <- ((m.1 - m.2) / sd.diff) * sqrt(2 * (1- corr));
return(es)
}
# Chunk 12: EsWitnTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnTval <- function(n.1, tval, corr){
es <- tval * sqrt((2 * (1 - corr)) / n.1);
return(es)
}
# Chunk 13: EsWitnFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnFval <- function(n.1, fval, corr){
es <- sqrt((2 * fval * (1- corr)) / n.1);
return(es)
}
# Chunk 14: EsVarWitn
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsVarWitn <- function(n.1, es){
es.var <- ((1 / n.1) +
((es^2) / (2 * n.1))) *
2 * (1 - corr);
return(es.var)
}
# Chunk 15: w: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "within"){
# call EsWitnMean on cases with within-subject designs and *means*
if(DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsWitnMean(m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i],
corr = corr)
}
# call EsWitnTval on cases with within-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsWitnTval(n.1 = DF$n.1[i],
tval = DF$tval[i],
corr = corr)
}
# call EsWitnFval on cases with within-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsWitnFval(n.1 = DF$n.1[i],
fval = DF$fval[i],
corr = corr)
}
# call EsVarWitn on cases with within subject designs
DF$es.var[i] <- EsVarWitn(n.1 = DF$n.1[i],
es = DF$es[i])
}
}
# Chunk 16: del var
# delete unnecessary variables
rm(corr, i,
EsBetwFval, EsBetwMean, EsBetwTval,
EsBetwPval, EsVarBetw, EsVarWitn,
EsWitnFval, EsWitnMean, EsWitnTval,
EsBetwCount, EsVarBetwCount)
# Chunk 17: specify es direction
DF <- DF %>%
# specify direction of the effect size
rowwise() %>%
mutate(es = if_else(condition = direction == "positive",
true = abs(es),
false = abs(es) * -1)
) %>%
ungroup()
# Chunk 18
DF <- DF %>%
select(-c(es.calc : pval))
# Chunk 19
# import data
DF.surv <- read_csv(file = "data/metaware_SurvData_raw.csv") %>%
# remove unnecessary variables
select(-c(StartDate : UserLanguage),
-c(hap1_bl1_hap : survey_order),
-contains("Click"),
-contains("Submit"),
-contains("Count")) %>%
# remove row containing ImportId
filter(!grepl("ImportId", `1_awr`)) %>%
# assign participant ids
mutate(ss = 1 : nrow(.),
ss = as.character(ss)) %>%
# select vignettes columns
select(`1_awr` : `119_opp`, ss) %>%
# extract key from first row
mutate_all(~if_else(condition = grepl("#", .),
true = substr(x = .,
start = 2,
stop = 9),
false = .))
# append first row to column name
## append name
colnames(DF.surv) <- paste(sep = "##",
colnames(DF.surv),
as.character(unlist(DF.surv[1, ]))
)
## remove first row
DF.surv <- DF.surv[-1, ]
## fix ss variable naming
DF.surv <- DF.surv %>%
rename("ss" = "ss##1")
# prep dataframe for summary statistics calculation
DF.surv <- DF.surv %>%
# pivot longer
pivot_longer(cols = contains("##"),
names_to = c("var", "vig"),
names_sep = "##") %>%
# extract variable name
mutate(var = substr(x = var,
start = nchar(var) - 2,
stop = nchar(var))) %>%
# pivot wider
pivot_wider(names_from = var,
values_from = value) %>%
# convert columns to correct class
mutate_at(c("awr", "mot", "opp", "bel", "pre"),
as.numeric)
# Chunk 20
DF.surv <- DF.surv %>%
# identify whether the demand condition was pos, neg, or nil
mutate(dem = substr(x = vig,
start = 4,
stop = 4))
## set att.chk to 0 (false) by default
DF.surv$att.chk = 0
## set att.chk to 1 when hypothesis is correctly identified
DF.surv[DF.surv$dem == "p" &
!is.na(DF.surv$awr) &
DF.surv$awr == 1, ]$att.chk = 1
DF.surv[DF.surv$dem == "n" &
!is.na(DF.surv$awr) &
DF.surv$awr == 2, ]$att.chk = 1
DF.surv[DF.surv$dem == "z" &
!is.na(DF.surv$awr) &
DF.surv$awr == 3, ]$att.chk = 1
## if participants did not specify a  hypothesis, set att.chk to NA
DF.surv[is.na(DF.surv$awr), ]$att.chk = NA
## manually fix 28_p_dis
## there were some oddities in how we created this vignette--but the correct hypothesis would be 2 (not 1)
DF.surv[DF.surv$vig == "28_p_dis" &
!is.na(DF.surv$awr) &
DF.surv$awr == 2, ]$att.chk = 1
## re-examination of 36_n_won suggested it was poorly worded (we meant to convey that it would hinder performance, but many people thought we meant it would improve performance)
# Chunk 21
# overall
DF.surv %>%
summarise(m = mean(att.chk,
na.rm = T))
# split by demand type
DF.surv %>%
group_by(dem) %>%
summarise(m = mean(att.chk,
na.rm = T))
# split by vignette
DF.surv %>%
group_by(vig) %>%
filter(!is.na(att.chk)) %>%
summarise(n = n(),
m = mean(att.chk)) %>%
arrange(m) %>%
View()
# Chunk 22
DF.surv <- DF.surv %>%
filter(att.chk == 1)
View(DF.surv)
library(lme4)
lmer(mot ~ 1 + (1 | ss) + (1 | vig))
lmer(mot ~ 1 + (1 | ss) + (1 | vig),
data = DF.surv)
surv.sum <- DF.surv %>%
group_by(vig) %>%
summarise(m.mot = mean(mot, na.rm = T),
m.opp = mean(opp, na.rm = T),
m.bel = mean(bel, na.rm = T),
m.pre = mean(pre, na.rm = T)
) %>%
ungroup()
surv.sum %>% arrange(desc(m.mot))
surv.sum$m.mot %>% mean()
lmer(mot ~ 1 + (1 | ss) + (1 | vig),
data = DF.surv) %>% summary()
lm(m.mot ~ m.bel,
data = surv.sum) %>% summary()
cor(m.mot, m.bel)
cor(surv.sum$m.mot, surv.sum$m.bel)
lm(m.mot ~ m.bel,
data = surv.sum) %>% summary()
lmer(mot ~ bel + (1 | ss) + (1 | vig),
data = surv.sum) %>% summary()
lmer(mot ~ bel + (1 | ss) + (1 | vig),
data = DF.surv) %>% summary()
plot(DF.surv$mot, DF.surv$bel)
ggplot(aes(x = DF$bel, y = DF.surv$bel)) + gghist
library(ggplot2)
ggplot(aes(x = DF$bel, y = DF.surv$bel)) + geom_jitter()
ggplot(aes(x = DF.surv$bel, y = DF.surv$bel)) + geom_jitter()
ggplot(aes(x = DF.surv$mot, y = DF.surv$bel)) + geom_jitter()
ggplot(aes(x = mot, y = bel), data = DF.surv) + geom_jitter()
lmer(mot ~ 1 + (1 | ss) + (1 | vig),
data = DF.surv) %>% summary()
lmer(mot ~ bel + (1 | ss) + (1 | vig),
data = DF.surv) %>% summary()
# turn scientific notation off
options(scipen = 999)
lmer(mot ~ bel + (1 | ss) + (1 | vig),
data = DF.surv) %>% summary()
