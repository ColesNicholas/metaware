mod.r[["mot"]][["mod"]][["ddf"]][["mot"]]
mod.r$mot[["mod"]][["ddf"]][["mot"]]
mod.r$mot$mod$ddf$[["mot"]]
mod.r$mot$mod$ddf[["mot"]]
mod.r$mot$mod$ddf$mot
mod.r$mot$mod$ddf$"mot"
mod.r$mot$mod$ddf[["mot"]]
mod.r$mot$mod$$zval[2]
mod.r$mot$mod$zval[2]
mod.r$mot$mod$pval[2]
DF.es$bel %>% plot()
DF.es$opp %>% hist()
View(mod.r)
mod.r[["opp"]]
mod.r[["opp"]][["mod"]]
mod.r$opp$mod$b["mot", ]
mod.r$opp$mod$b["opp", ]
mod.r$opp$mod$ci.lb[2]
mod.r$opp$mod$ci.ub[2]
mod.r$opp$mod$ddf[["opp"]]
mod.r$opp$mod$zval[2]
mod.r$opp$mod$pval[2]
mod.r[["bel"]]$mod
mod.r$bel$mod$b["bel", ]
mod.r$bel$mod$ci.lb[2]
mod.r$bel$mod$ci.ub[2]
mod.r$bel$mod$ddf[["bel"]]
mod.r$bel$mod$zval[2]
mod.r$bel$mod$pval[2]
View(mod.r)
mod.r[["pre"]][["mod"]]
cor(DF.es$es, DF.es$pre)
?cor(DF.es$es, DF.es$pre)
?cor(DF.es$es, DF.es$pre, use = "pairwise.complete.obs")
cor(DF.es$es, DF.es$pre, use = "pairwise.complete.obs")
mod.r$pre$mod$b["pre", ]
mod.r$pre$mod$ci.lb[2]
mod.r$pre$mod$ci.ub[2]
mod.r$pre$mod$ddf[["bel"]]
mod.r$pre$mod$ddf[["pre"]]
mod.r$pre$mod$zval[2]
mod.r$pre$mod$pval[2]
#' Calculate I-squared values and variance distribution for multilevel meta-analysis models
#'
#' This function calculates values of \eqn{I^2} and the variance distribution for multilevel meta-analysis
#' models fitted with \code{\link[metafor]{rma.mv}}.
#'
#'
#' @usage mlm.variance.distribution(x)
#'
#' @param x An object of class \code{rma.mv}. Must be a multilevel model with two random effects (three-level meta-analysis model).
#'
#' @details This function estimates the distribution of variance in a three-level meta-analysis
#' model (fitted with the \code{\link[metafor]{rma.mv}} function). The share of variance attributable to
#' sampling error, within and between-cluster heterogeneity is calculated,
#' and an estimate of \eqn{I^2} (total and for Level 2 and Level 3) is provided. The function uses the formula by
#' Cheung (2014) to estimate the variance proportions attributable to each model component and to derive the \eqn{I^2} estimates.
#'
#'
#' @references
#'
#' Harrer, M., Cuijpers, P., Furukawa, T.A, & Ebert, D. D. (2019).
#' \emph{Doing Meta-Analysis in R: A Hands-on Guide}. DOI: 10.5281/zenodo.2551803. \href{https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/mlma.html}{Chapter 12}.
#'
#'Cheung, M. W. L. (2014). Modeling dependent effect sizes with three-level meta-analyses: a structural equation modeling approach. \emph{Psychological Methods, 19}(2), 211.
#'
#' @author Mathias Harrer & David Daniel Ebert
#'
#' @aliases var.comp
#'
#' @import ggplot2
#' @importFrom stats model.matrix
#'
#' @return Returns a data frame containing the results. A plot summarizing the variance distribution and \eqn{I^2} values can be generated using \code{plot}.
#'
#' @export mlm.variance.distribution
#' @export var.comp
#'
#' @examples
#' # Use dat.konstantopoulos2011 from the "metafor" package
#' library(metafor)
#'
#' # Build Multilevel Model (Three Levels)
#' m = rma.mv(yi, vi, random = ~ 1 | district/school, data=dat.konstantopoulos2011)
#'
#' # Calculate Variance Distribution
#' mlm.variance.distribution(m)
#'
#' # Use alias 'var.comp' and 'Chernobyl' data set
#' data("Chernobyl")
#' m2 = rma.mv(yi = z, V = var.z, data = Chernobyl, random = ~ 1 | author/es.id)
#' res = var.comp(m2)
#'
#' # Print results
#' res
#'
#' # Generate plot
#' plot(res)
mlm.variance.distribution = var.comp = function(x){
m = x
# Check class
if (!(class(m)[1] %in% c("rma.mv", "rma"))){
stop("x must be of class 'rma.mv'.")
}
# Check for three level model
if (m$sigma2s != 2){
stop("The model you provided does not seem to be a three-level model. This function can only be used for three-level models.")
}
# Check for right specification (nested model)
if (sum(grepl("/", as.character(m$random[[1]]))) < 1){
stop("Model must contain nested random effects. Did you use the '~ 1 | cluster/effect-within-cluster' notation in 'random'? See ?metafor::rma.mv for more details.")
}
# Get variance diagonal and calculate total variance
n = m$k.eff
vector.inv.var = 1/(diag(m$V))
sum.inv.var = sum(vector.inv.var)
sum.sq.inv.var = (sum.inv.var)^2
vector.inv.var.sq = 1/(diag(m$V)^2)
sum.inv.var.sq = sum(vector.inv.var.sq)
num = (n-1)*sum.inv.var
den = sum.sq.inv.var - sum.inv.var.sq
est.samp.var = num/den
# Calculate variance proportions
level1=((est.samp.var)/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
level2=((m$sigma2[2])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
level3=((m$sigma2[1])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
# Prepare df for return
Level=c("Level 1", "Level 2", "Level 3")
Variance=c(level1, level2, level3)
df.res=data.frame(Variance)
colnames(df.res) = c("% of total variance")
rownames(df.res) = Level
I2 = c("---", round(Variance[2:3], 2))
df.res = as.data.frame(cbind(df.res, I2))
totalI2 = Variance[2] + Variance[3]
# Generate plot
df1 = data.frame("Level" = c("Sampling Error", "Total Heterogeneity"),
"Variance" = c(df.res[1,1], df.res[2,1]+df.res[3,1]),
"Type" = rep(1,2))
df2 = data.frame("Level" = rownames(df.res),
"Variance" = df.res[,1],
"Type" = rep(2,3))
df = as.data.frame(rbind(df1, df2))
g = ggplot(df, aes(fill=Level, y=Variance, x=as.factor(Type))) +
coord_cartesian(ylim = c(0,1), clip = "off") +
geom_bar(stat="identity", position="fill", width = 1, color="black") +
scale_y_continuous(labels = scales::percent)+
theme(axis.title.x=element_blank(),
axis.text.y = element_text(color="black"),
axis.line.y = element_blank(),
axis.title.y=element_blank(),
axis.line.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.y = element_line(lineend = "round"),
legend.position = "none",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
legend.background = element_rect(linetype="solid",
colour ="black"),
legend.title = element_blank(),
legend.key.size = unit(0.75,"cm"),
axis.ticks.length=unit(.25, "cm"),
plot.margin = unit(c(1,3,1,1), "lines")) +
scale_fill_manual(values = c("darkseagreen3", "deepskyblue3", "darkseagreen2",
"deepskyblue1", "deepskyblue2")) +
# Add Annotation
# Total Variance
annotate("text", x = 1.5, y = 1.05,
label = paste("Total Variance:",
round(m$sigma2[1]+m$sigma2[2]+est.samp.var, 3))) +
# Sampling Error
annotate("text", x = 1, y = (df[1,2]/2+df[2,2])/100,
label = paste("Sampling Error Variance: \n", round(est.samp.var, 3)), size = 3) +
# Total I2
annotate("text", x = 1, y = ((df[2,2])/100)/2-0.02,
label = bquote("Total"~italic(I)^2*":"~.(round(df[2,2],2))*"%"), size = 3) +
annotate("text", x = 1, y = ((df[2,2])/100)/2+0.05,
label = paste("Variance not attributable \n to sampling error: \n", round(m$sigma2[1]+m$sigma2[2],3)), size = 3) +
# Level 1
annotate("text", x = 2, y = (df[1,2]/2+df[2,2])/100, label = paste("Level 1: \n",
round(df$Variance[3],2), "%", sep=""), size = 3) +
# Level 2
annotate("text", x = 2, y = (df[5,2]+(df[4,2]/2))/100,
label = bquote(italic(I)[Level2]^2*":"~.(round(df[4,2],2))*"%"), size = 3) +
# Level 3
annotate("text", x = 2, y = (df[5,2]/2)/100,
label = bquote(italic(I)[Level3]^2*":"~.(round(df[5,2],2))*"%"), size = 3)
returnlist = list(results = df.res,
totalI2 = totalI2,
plot = g)
class(returnlist) = c("mlm.variance.distribution", "list")
invisible(returnlist)
returnlist
}
var.comp(mod.r$pre$mod)
mod.r$pre$mod
mod.r$pre$mod %>% anova()
mod.r$pre$mod %>% print.rma()
mod.r$pre$mod %>% anova.rma()
tmp <- mod.r$pre$mod %>% anova.rma()
View(tmp)
tmp <- anova.rma(overall, mod.r$pre$mod)
View(overall)
anova(overall, mod.r$pre$mod)
anova( mod.r$pre$mod, overall)
anova(mod.r$pre$mod, overall)
tmp <-
rma.mv(yi = es,
V = es.var,
data = DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz"),
random = ~ 1 | id.study / id.es)
ampva(tmp, mod.r$pre$mod)
anova(tmp, mod.r$pre$mod
)
tmp2 <-
rma.mv(yi = es,
V = es.var,
data = DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz"),
random = ~ 1 | id.study / id.es,
mods = ~ pre)
tmp2
mod.r$pre$mod
tmp2 <-
rma.mv(yi = es,
V = es.var,
data = DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz"),
random = ~ 1 | id.study / id.es,
mods = ~ pre,
test = "t")
tmp2
rm(list = ls())
# Chunk 1: setup
# load packages writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: vig.desc
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>%
nrow()
# Chunk 9: vig
knitr::include_graphics("images/metaware_vigs.png")
# Chunk 10: survey.n
# identify number of participants who completed the survey
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>%
nrow()
# Chunk 11: mods
knitr::include_graphics("images/metaware_mods.png")
# Chunk 12: clean.env.2
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
# Chunk 13: overall
# estimate overall effect size
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# overall <-
#   rma.uni(yi = es,
#           vi = es.var,
#           data = DF.es,
#           method = "REML") %>%
#   robust(cluster = DF.es$id)
# create moderator analysis function
ModAnalysis = function(m, df = DF.es) {
# set dataset
df <- df
# moderator analysis
mod.m <- rma.mv(yi = es,
V = es.var,
data = df,
random = ~ 1 | id.study / id.es,
mods = as.formula(paste0("~ ", m)),
test= "t")
sub.m <- rma.mv(yi = es,
V = es.var,
data = df,
random = ~ 1 | id.study / id.es,
mods = as.formula(paste0("~ 0 + ", m)),
test= "t")
# return results as list
return(list(mod = mod.m,
sub = sub.m))
}
# conduct moderator and subgroup analyses for moderators assessed with full dataset
mod.l <- c("student", "paid", "online",
"design", "ref.r", "published",
"year")
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis)
rm(mod.l)
# test ref.type moderator in scenarios where there is a control comparison (i.e., ref.r == single)
mod.r[["ref.type"]] <-
ModAnalysis(m = "ref.type",
df = DF.es[DF.es$ref.r == "single", ])
# add motivation, opportunity, belief, and prediction moderators
## Note: comparisons with nil-demand conditions are excluded
mod.r.2 <-
sapply(X = c("mot", "opp", "bel", "pre"),
simplify = F,
FUN = ModAnalysis,
df = DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz"))
## combine results
mod.r = c(mod.r, mod.r.2)
# delete vestigial
rm(ModAnalysis, mod.r.2)
View(mod.r)
mod.r[["mot"]][["mod"]]
mod.r[["opp"]][["mod"]]
mod.r[["bel"]][["mod"]]
mod.r[["pre"]][["mod"]]
DF <- DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz")
mod.1 <- rma.mv(yi = es,
V = es.var,
data = DF,
random = ~ 1 | id.study / id.es,
mods = ~ bel,
test= "t")
mod.0 <- rma.mv(yi = es,
V = es.var,
data = DF,
random = ~ 1 | id.study / id.es,
test= "t")
anova(mod.0, mod.1)
anova.rma(mod.0, mod.1
)
tmp <- anova.rma(mod.0, mod.1
tmp
tmp <- anova.rma(mod.0, mod.1)
View(tmp)
tmp[["R2"]]
tmp <- anova.rma(mod.0, mod.1,
refit = T)
tmp
View(tmp)
?rma.uni
mod.1 <- rma.uni(yi = es,
V = es.var,
data = DF,
mods = ~ bel)
DF <- DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz")
mod.1 <- rma.uni(yi = es,
V = es.var,
data = DF,
mods = ~ bel)
mod.1 <- rma.(yi = es,
V = es.var,
data = DF,
mods = ~ bel)
mod.1 <- rma(yi = es,
V = es.var,
data = DF,
mods = ~ bel)
mod.1 <- rma(yi = es,
vi  = es.var,
data = DF,
mods = ~ bel)
mod.0 <- rma.mv(yi = es,
vi = es.var,
data = DF)
mod.0 <- rma(yi = es,
vi = es.var,
data = DF)
tmp <- anova.rma(mod.0, mod.1)
tmp <- anova.rma(mod.0, mod.1,
refit = T)
tmp
View(tmp)
df <- DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz")
mod.m <- rma.mv(yi = es,
V = es.var,
data = df,
random = ~ 1 | id.study / id.es,
mods = ~ opp * bel,
test= "t")
df <- DF.es %>%
filter(ref.type != "cvz" &
ref.type != "pvz")
mod.m <- rma.mv(yi = es,
V = es.var,
data = df,
random = ~ 1 | id.study / id.es,
mods = ~ opp * bel,
test= "t")
mod.m %>% anova()
mod.m
View(DF.es)
DF.es %>%
filter(name == "Demand effects in survey experiments: An empirical assessment") %>%
select(bel, es) %>% View
DF.es %>%
filter(name == "Demand effects in survey experiments: An empirical assessment") %>%  View
DF.es$bel %>% plot()
DF.es$bel %>% hist()
DF.es$bel %>% mean()
DF.es$bel %>% mean(na.rm = T)
