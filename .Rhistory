data = tmp.DF)  %>%
icc(by_group = T)
# export
list(mot = mot.icc,
opp = opp.icc,
bel = bel.icc,
pre = pre.icc) %>%
saveRDS(.,
"output/vig.survfull.rel.rds")
rm(tmp.DF, mot.icc, opp.icc, pre.icc, bel.icc)
# Chunk 27
DF.surv <- DF.surv %>%
# identify whether the researcher hypothesis was pos, neg, or nil
mutate(dem = substr(x = vig,
start = 4,
stop = 4))
## set att.chk to 0 (failed attention check) by default
DF.surv$att.chk = 0
## set att.chk to 1 when hypothesis is correctly identified
### positive hypothesis (dem == p)
DF.surv[DF.surv$dem == "p" &
!is.na(DF.surv$awr) &
DF.surv$awr == 1, ]$att.chk = 1
### negative hypothesis (dem == n)
DF.surv[DF.surv$dem == "n" &
!is.na(DF.surv$awr) &
DF.surv$awr == 2, ]$att.chk = 1
### nil hypothesis (dem == z)
DF.surv[DF.surv$dem == "z" &
!is.na(DF.surv$awr) &
DF.surv$awr == 3, ]$att.chk = 1
## if participants did not respond to the attention check item, set the att.chk variable to NA
DF.surv[is.na(DF.surv$awr), ]$att.chk = NA
## manually fix 28_p_dis vignette attention check
## there were some oddities in how we created this vignette--but the correct hypothesis would be 2 (not 1)
DF.surv[DF.surv$vig == "28_p_dis" &
!is.na(DF.surv$awr) &
DF.surv$awr == 2, ]$att.chk = 1
# Chunk 29
surv.att <- DF.surv %>%
group_by(vig) %>%
summarise(m.att = mean(att.chk,
na.rm = T))
DF.surv <- DF.surv %>%
filter(att.chk == 1)
# Chunk 30
surv.sum <- DF.surv %>%
group_by(vig) %>%
summarise(m.mot = mean(mot, na.rm = T),
m.opp = mean(opp, na.rm = T),
m.bel = mean(bel, na.rm = T),
m.pre = mean(pre, na.rm = T)
) %>%
ungroup()
# combine with attention check data
surv.sum <- full_join(surv.att, surv.sum)
rm(surv.att)
# remove old survey dataframe (not needed anymore)
rm(DF.surv)
# output this data (for later analyses)
write.csv(x = surv.sum,
file = "output/surv.sum.csv",
row.names = F)
# Chunk 31
# connect summary data to effect size dataframe
DF <- DF %>%
# join vig.1 data
## rename vig.1 to vig to enable join
rename(vig = vig.1) %>%
## connect summary data to vig.1
left_join(x = .,
y = surv.sum,
by = "vig") %>%
## rename vig.1 summary columns
rename(v1.att = m.att,
v1.mot = m.mot,
v1.opp = m.opp,
v1.bel = m.bel,
v1.pre = m.pre) %>%
# join vig.2 data
## rename vig.2 to vig to prep for join
rename(vig.1 = vig,
vig = vig.2) %>%
## connect summary data to vig.2
left_join(x = .,
y = surv.sum,
by = "vig") %>%
## rename vig.2 summary columns
rename(v2.att = m.att,
v2.mot = m.mot,
v2.opp = m.opp,
v2.bel = m.bel,
v2.pre = m.pre) %>%
# sum motivation, opportunity, belief, and prediction scores
# Note: the if_else approach helps ensure that 0 isn't assigned to cases with two NA's
rowwise() %>%
mutate(att =
if_else(is.na(v1.att),
true = NA,
false =
sum(c(v1.att, v2.att),
na.rm = T)
),
mot =
if_else(is.na(v1.mot),
true = NA,
false =
sum(c(v1.mot, v2.mot),
na.rm = T)
),
opp =
if_else(is.na(v1.opp),
true = NA,
false =
sum(c(v1.opp, v2.opp),
na.rm = T)
),
bel =
if_else(is.na(v1.bel),
true = NA,
false =
sum(c(v1.bel, v2.bel),
na.rm = T)
),
pre =
if_else(is.na(v1.pre),
true = NA,
false =
sum(c(v1.pre, v2.pre),
na.rm = T)
)) %>%
ungroup()
# set attention, motivation, opportunity, belief, and prediction scores blank for id 18
# we did not create vignettes for id 18 because it was a *massive* outlier that we removed from all analyses
DF[DF$id.study == 18, ]$att = NA
DF[DF$id.study == 18, ]$mot = NA
DF[DF$id.study == 18, ]$opp = NA
DF[DF$id.study == 18, ]$bel = NA
DF[DF$id.study == 18, ]$pre = NA
# organize ordering of variables
DF <- DF %>%
select(id.study, id.es,
citation : ref.r,
es, es.var, lb, ub,
att : pre,
report : internal)
# Chunk 32
# import data
DF.surv2 <-
read_csv(file = "data/metaware_SurvData_raw.csv")
# remove rows containing unnecessary variable details
DF.surv2 <- DF.surv2[-(1 : 2), ]
# hand recode gender
## list as unknown if they did not answer (or said they'd prefer not to answer)
DF.surv2[is.na(DF.surv2$indiv_gend_var), ]$indiv_gend_var = "unknown"
DF.surv2[DF.surv2$indiv_gend_var == "7", ]$indiv_gend_var = "unknown"
## recode rest
DF.surv2[DF.surv2$indiv_gend_var == "1", ]$indiv_gend_var = "female"
DF.surv2[DF.surv2$indiv_gend_var == "2", ]$indiv_gend_var = "male"
DF.surv2[DF.surv2$indiv_gend_var == "3", ]$indiv_gend_var = "trans_female"
DF.surv2[DF.surv2$indiv_gend_var == "4", ]$indiv_gend_var = "trans_male"
DF.surv2[DF.surv2$indiv_gend_var == "5", ]$indiv_gend_var = "gender_nonconforming"
# hand recode ethnicity
DF.surv2[is.na(DF.surv2$ethnicity), ]$ethnicity = "unknown"
DF.surv2[DF.surv2$ethnicity == "1", ]$ethnicity = "white_caucasian"
DF.surv2[DF.surv2$ethnicity == "2", ]$ethnicity = "black_african_american"
DF.surv2[DF.surv2$ethnicity == "3", ]$ethnicity = "american_indian"
DF.surv2[DF.surv2$ethnicity == "4", ]$ethnicity = "asian"
DF.surv2[DF.surv2$ethnicity == "5", ]$ethnicity = "native_pacific_islander"
DF.surv2[DF.surv2$ethnicity == "6", ]$ethnicity = "native_pacific_islander"
DF.surv2[DF.surv2$ethnicity == "6" |
DF.surv2$ethnicity == "1,6" |
DF.surv2$ethnicity == "1,2" |
DF.surv2$ethnicity == "1,4" |
DF.surv2$ethnicity == "2,6" |
DF.surv2$ethnicity == "7" |
DF.surv2$ethnicity == "1,3", ]$ethnicity = "other"
# process data
DF.surv2 <- DF.surv2 %>%
# identify relevant variables
select(
# block 1 happy pose emotion reports
hap1_bl1_hap : hap1_bl1_enj,
# block 1 neutral pose emotion reports
neu1_bl1_hap : neu1_bl1_enj,
# block 2 happy pose emotion reports
hap2_bl2_hap : hap2_bl2_enj,
# block 2 neutral pose emotion reports
neu2_bl2_hap : neu2_bl2_enj,
# motivation, prediction, belief, and opportunity scores
mot : opp, mot_gen,
# manipulation check items (awareness and attention checks)
hap1_bl1_att, hap2_bl2_att, neu1_bl1_att, neu2_bl2_att,
awr,
# condition and individual difference items
demand,
indiv_gend_var, indiv_agee_var, ethnicity,
# relevant vignette data
`46_awr` : `46_opp`,
`47_awr` : `47_opp`) %>%
# rename vignette variables
rename(pos_awr = `46_awr`,
pos_mot = `46_mot`,
pos_pre = `46_pre`,
pos_bel = `46_bel`,
pos_opp = `46_opp`,
nil_awr = `47_awr`,
nil_mot = `47_mot`,
nil_pre = `47_pre`,
nil_bel = `47_bel`,
nil_opp = `47_opp`) %>%
# add subid variable
mutate(sub = factor(1 : nrow(.))) %>%
# identify whether attention checks were all passed
mutate(att.chk =
if_else(condition =
hap1_bl1_att == "5" &
neu1_bl1_att == "5" &
hap2_bl2_att == "5" &
neu2_bl2_att == "5",
true = 1,
false = 0)
) %>%
# identify whether participants correctly identified the hypothesis
mutate(manip.chk =
if_else(condition =
demand == "pos" & awr == 1,
true = 1,
false =
if_else(condition =
demand == "nil" & awr == 2,
true = 1,
false = 0))) %>%
# create separate rows for each trial
# 1. Gather emotion reports into a single column
gather(key = "dv",
value = "value",
hap1_bl1_hap : neu2_bl2_enj) %>%
# 2. Create separate variables that identify the trial and outcome using "_..._ naming convention separator
separate(col = "dv",
into = c("trial", "outcome"),
sep = "_..._") %>%
# 3. Spread outcomes into individual rows
pivot_wider(names_from = "outcome",
values_from = "value") %>%
# fix variable types
mutate_at(.vars = c("mot", "pre", "bel", "opp",
"mot_gen", "indiv_agee_var",
"hap", "sat", "enj",
"pos_awr", "pos_mot",
"pos_pre", "pos_bel",
"pos_opp",
"nil_awr", "nil_mot",
"nil_pre", "nil_bel",
"nil_opp"),
.funs = as.numeric) %>%
mutate_at(.vars = c("demand", "indiv_gend_var",
"ethnicity", "sub"),
.funs = as.factor) %>%
# Calculate self-reported happiness scores
rowwise() %>%
mutate(happy = mean(c(hap, sat, enj))) %>%
ungroup() %>%
# identify block number
mutate(block.num = if_else(condition = trial == "hap1" |
trial == "neu1",
true = 1,
false = 2),
block.num = factor(block.num)) %>%
# create new trial variable that remove redundant information about block
mutate(trial = substr(x = trial,
start = 1,
stop = 3),
trial = factor(trial)) %>%
# organize dataframe
arrange(sub, block.num, trial) %>%
select(sub, demand, trial, block.num, happy,
att.chk, manip.chk,
mot : mot_gen,
indiv_gend_var : ethnicity) %>%
# recode (flip) motivation for the nil hypothesis condition
rowwise() %>%
mutate(mot = if_else(demand == "nil",
mot * (-1),
mot)) %>%
ungroup()
# center motivation, opportunity, and belief scores
DF.surv2$mot.c = scale(DF.surv2$mot,
scale = F) %>% as.numeric()
DF.surv2$opp.c = scale(DF.surv2$opp,
scale = F) %>% as.numeric()
DF.surv2$bel.c = scale(DF.surv2$bel,
scale = F) %>% as.numeric()
# Chunk 33
if(!exists("sens")){
write.csv(DF,
"data/metaware_meta_clean.csv",
row.names = F)
}
# Chunk 34
write.csv(DF.surv2,
"data/metaware_replication_clean.csv",
row.names = F)
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# set token indicating that this is a sensitivity analysis
sens <- T
# create list of correlations to examine
corr.list <- seq(from = .1,
to = .9,
by = .2)
# Chunk 2
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_DataProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
# set corr
corr <- c
# source script
source(knitr::purl("metaware_DataProcessing.Rmd", quiet=TRUE),
local = knitr::knit_global())
# export dataframe
write.csv(DF,
paste0("data/r_sensitivity/metaware_meta_clean_r",
c,
".csv"),
row.names = F
)
}
# clean up
rm(list = ls())
# Chunk 1: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of lead author's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify
# unpublished dissertations by identifying links that contain the word 'dissertation' AND
# records identified manually by identify links that contain 'NA'
mutate(unpub =
if_else(condition = grepl("dissertation", link) |
link == "NA",
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pub <- DF.s %>%
filter(unpub == 0) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(unpub == 1) %>%
nrow()
# Chunk 4: prisma.info
records.per.search <- DF.s %>%
group_by(database) %>%
summarise(n = n())
# Chunk 5: prisma
knitr::include_graphics("images/metaware_PRISMA.png")
# Chunk 6: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 7: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pub, r.unp, num.s, num.p, outlier.es, records.per.search)
# Chunk 8: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es) %>%
robust(x = .,
cluster = id.study,
clubSandwich = T)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 9: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 10: vig.dem
# identify total number of vignettes
vig.n <- read.csv(file = "admin/vig/metaware_VigCombined.csv") %>%
nrow()
# compile demographic info for the two vignette studies
# import data from first survey
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_1")
vig.surv1.n = nrow(vig.surv1)
vig.surv2 <-
read_csv(file = "data/metaware_SurvData2_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_2")
vig.surv <- rbind(vig.surv1, vig.surv2)
rm(vig.surv1, vig.surv2)
# describe participant demographics
survey.n <- nrow(vig.surv)
survey.gend <-
vig.surv$indiv_gend_var %>%
table() %>%
prop.table() %>%
round(2) * 100
survey.eth <-
vig.surv$ethnicity %>%
table() %>%
prop.table() %>%
round(2) * 100
survey.age.m <-
mean(vig.surv$indiv_agee_var %>% as.numeric(),
na.rm = T) %>%
round(2)
survey.age.sd <-
sd(vig.surv$indiv_agee_var %>% as.numeric(),
na.rm = T) %>%
round(2)
# put demographics into table
survey.dem = list(n = survey.n,
gend = survey.gend,
ethnicity = survey.eth,
age.m = survey.age.m,
age.sd = survey.age.sd)
# remove vestigial
rm(survey.n, survey.gend,
survey.eth, survey.age.m, survey.age.sd)
# Chunk 11: vig.rel
# open reliability data
vig1.rel <- readRDS("output/vig.surv1.rel.rds")
# Chunk 12: vig
knitr::include_graphics("images/metaware_vigs.png")
# Chunk 14: clean.env.2
# delete vestigial
rm(mult.eff.per, sens.range)
DF.es$year %>% sd()
