tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 31 studies
DF$name %>% unique() %>% length()
# 222 effect sizes
nrow(DF)
# Chunk 4: define function EsBetwMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 226
EsBetwMean <- function(n.1, m.1, sd.1,
n.2, m.2, sd.2){
sd.within <- sqrt((((n.1 - 1) * (sd.1^2)) +
((n.2 - 1) * (sd.2^2))) /
(n.1 + n.2 - 2));
es <- (m.1 - m.2) / sd.within;
return(es)
}
# Chunk 5: define function EsBetwTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwTval <- function(n.1, n.2, tval){
es <- tval * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 6: define function EsBetwFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwFval <- function(n.1, n.2, fval){
es <- sqrt((fval * (n.1 + n.2)) /
(n.1 * n.2));
return(es)
}
# Chunk 7: define function EsBetwPval
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwPval <- function(n.1, n.2, pval){
# calculate the inverse of the cumulative distribution function of t
t.inv <- qt(p = (pval / 2),
df = (n.1 + n.2 - 2),
lower.tail = FALSE);
es <- t.inv * sqrt((n.1 + n.2) /
(n.1 * n.2));
return(es)
}
# Chunk 8: define function EsVarBetw
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsVarBetw <- function(n.1, n.2, es){
es.var <- ((n.1 + n.2) / (n.1 * n.2)) +
((es^2) / (2 * (n.1 + n.2)));
return(es.var)
}
# Chunk 9: define functions EsBetwCount and EsVarBetwCount
# cohen's d
EsBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.8
or <- (count.1 * (n.2 - count.2)) /
((n.1 - count.1) * count.2);
# calculate log odds ratio
# formula: Borenstein et al. 2011; p. 36; Equation 5.9
log.or = log(or);
# convert log odds ratio to Cohen's d
# formula: Borenstein et al. 2011; p . 47 Equation 7.1
es <- log.or * (sqrt(3) / pi);
return(es)
}
# variance of cohen's d
EsVarBetwCount <- function(n.1, n.2, count.1, count.2){
# calculate log odds ratio variance
# formula: Borenstein et al. 2011; p. 36; Equation 5.10
log.or.var = (1 / count.1) +
(1 / (n.2 - count.2)) +
(1 / (n.1 - count.1)) +
(1 / count.2);
# convert log odds ratio variance to variance of Cohen's d
# formula: Borenstein et al. 2011; p. 47 Equation 7.2
es.var = log.or.var * (3 / pi^2);
return(es.var)
}
# Chunk 10: b: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "between"){
# call EsBetwMean on cases with between-subject designs and *means*
if (DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsBetwMean(n.1 = DF$n.1[i],
m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
n.2 = DF$n.2[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i])
}
# call EsBetwTval on cases with between-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsBetwTval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
tval = DF$tval[i])
}
# call EsBetwFval on cases with between-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsBetwFval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
fval = DF$fval[i])
}
# call EsBetwPval on cases with between-subject designs and *p-values*
if (DF$es.calc[i] == "p") {
DF$es[i] <- EsBetwPval(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
pval = DF$pval[i])
}
# call EsBetwCount and EsVarBetwCount on cases with between-subject designs and *count data*
if (DF$es.calc[i] == "or") {
DF$es[i] <- EsBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
DF$es.var[i] <- EsVarBetwCount(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
count.1 = DF$count.1[i],
count.2 = DF$count.2[i])
}
# call EsVarBetw on cases with continuous data and a between subject designs
if (DF$es.calc[i] != "or"){
DF$es.var[i] <- EsVarBetw(n.1 = DF$n.1[i],
n.2 = DF$n.2[i],
es = DF$es[i])
}
}
}
# Chunk 11: assumed correlation
# define assumed correlation (sensitivity analyses performed later)
corr <- .5
# Chunk 12: EsWitnMean
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
# formula for imputing sd.diff:
# http://handbook.cochrane.org/chapter_16/16_4_6_1_mean_differences.htm
EsWitnMean <- function(m.1, sd.1, m.2, sd.2, corr){
sd.diff <- sqrt((sd.1^2) + (sd.2^2) -
(2 * corr * sd.1 * sd.2));
es <- ((m.1 - m.2) / sd.diff) * sqrt(2 * (1- corr));
return(es)
}
# Chunk 13: EsWitnTval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnTval <- function(n.1, tval, corr){
es <- tval * sqrt((2 * (1 - corr)) / n.1);
return(es)
}
# Chunk 14: EsWitnFval
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnFval <- function(n.1, fval, corr){
es <- sqrt((2 * fval * (1- corr)) / n.1);
return(es)
}
# Chunk 15: EsVarWitn
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsVarWitn <- function(n.1, es){
es.var <- ((1 / n.1) +
((es^2) / (2 * n.1))) *
2 * (1 - corr);
return(es.var)
}
# Chunk 16: w: call functions to calculate d
for (i in 1:nrow(DF)) {
if (DF$design[i] == "within"){
# call EsWitnMean on cases with within-subject designs and *means*
if(DF$es.calc[i] == "m_sd") {
DF$es[i] <- EsWitnMean(m.1 = DF$m.1[i],
sd.1 = DF$sd.1[i],
m.2 = DF$m.2[i],
sd.2 = DF$sd.2[i],
corr = corr)
}
# call EsWitnTval on cases with within-subject designs and *t-values*
if (DF$es.calc[i] == "t") {
DF$es[i] <- EsWitnTval(n.1 = DF$n.1[i],
tval = DF$tval[i],
corr = corr)
}
# call EsWitnFval on cases with within-subject designs and *F-values*
if (DF$es.calc[i] == "f") {
DF$es[i] <- EsWitnFval(n.1 = DF$n.1[i],
fval = DF$fval[i],
corr = corr)
}
# call EsVarWitn on cases with within subject designs
DF$es.var[i] <- EsVarWitn(n.1 = DF$n.1[i],
es = DF$es[i])
}
}
# Chunk 17: del var
# delete unnecessary variables
rm(corr, i,
EsBetwFval, EsBetwMean, EsBetwTval,
EsBetwPval, EsVarBetw, EsVarWitn,
EsWitnFval, EsWitnMean, EsWitnTval,
EsBetwCount, EsVarBetwCount)
# Chunk 18
DF <- DF %>%
filter(id != 18)
# Chunk 19: specify es direction
DF <- DF %>%
# specify direction of the effect size
rowwise() %>%
mutate(es = if_else(condition = direction == "positive",
true = abs(es),
false = abs(es) * -1)
) %>%
ungroup()
# Chunk 21
write.csv(DF %>%
filter(!is.na(es)),
'metaware.data_clean.csv',
row.names = FALSE)
# Chunk 22
# overall d= 0.28, 95% CI = [0.16, 0.39], T = 0.28
overall <-
robu(formula = es ~ 1,
data = DF,
studynum = id,
var.eff.size = es.var,
modelweights = "HIER",
small = FALSE)
# get imperfect estimate of I2: 88.50%
## RVE approach doesn't give an I2
rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML")
rma.mv(yi = es,
v = es.var,
data = DF)
rma.mv(yi = DF$es,
v = DF$es.var,
data = DF)
rma.mv(yi = es,
V = es.var,
data = DF)
rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML")
rma.mv(yi = es,
V = es.var,
data = DF)
?rma.mv
View(DF)
DF$es.id <- 1 : nrow(DF)
1 : nrow(DF)
rma.mv(yi = es,
V = es.var,
data = DF,
random = ~ 1 | es.id)
rma.uni(yi = es,
vi = es.var,
data = DF,
method = "REML")
rma.mv(yi = es,
V = es.var,
data = DF,
random = ~ 1 | id / es.id)
overall <- rma.mv(yi = es,
V = es.var,
data = DF,
random = ~ 1 | id / es.id)
overall %>% var.comp() %>% summary()
var.comp(overall) %>% summary()
install.packages('dmetar')
library('dmetar')
#' Calculate I-squared values and variance distribution for multilevel meta-analysis models
#'
#' This function calculates values of \eqn{I^2} and the variance distribution for multilevel meta-analysis
#' models fitted with \code{\link[metafor]{rma.mv}}.
#'
#'
#' @usage mlm.variance.distribution(x)
#'
#' @param x An object of class \code{rma.mv}. Must be a multilevel model with two random effects (three-level meta-analysis model).
#'
#' @details This function estimates the distribution of variance in a three-level meta-analysis
#' model (fitted with the \code{\link[metafor]{rma.mv}} function). The share of variance attributable to
#' sampling error, within and between-cluster heterogeneity is calculated,
#' and an estimate of \eqn{I^2} (total and for Level 2 and Level 3) is provided. The function uses the formula by
#' Cheung (2014) to estimate the variance proportions attributable to each model component and to derive the \eqn{I^2} estimates.
#'
#'
#' @references
#'
#' Harrer, M., Cuijpers, P., Furukawa, T.A, & Ebert, D. D. (2019).
#' \emph{Doing Meta-Analysis in R: A Hands-on Guide}. DOI: 10.5281/zenodo.2551803. \href{https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/mlma.html}{Chapter 12}.
#'
#'Cheung, M. W. L. (2014). Modeling dependent effect sizes with three-level meta-analyses: a structural equation modeling approach. \emph{Psychological Methods, 19}(2), 211.
#'
#' @author Mathias Harrer & David Daniel Ebert
#'
#' @aliases var.comp
#'
#' @import ggplot2
#' @importFrom stats model.matrix
#'
#' @return Returns a data frame containing the results. A plot summarizing the variance distribution and \eqn{I^2} values can be generated using \code{plot}.
#'
#' @export mlm.variance.distribution
#' @export var.comp
#'
#' @examples
#' # Use dat.konstantopoulos2011 from the "metafor" package
#' library(metafor)
#'
#' # Build Multilevel Model (Three Levels)
#' m = rma.mv(yi, vi, random = ~ 1 | district/school, data=dat.konstantopoulos2011)
#'
#' # Calculate Variance Distribution
#' mlm.variance.distribution(m)
#'
#' # Use alias 'var.comp' and 'Chernobyl' data set
#' data("Chernobyl")
#' m2 = rma.mv(yi = z, V = var.z, data = Chernobyl, random = ~ 1 | author/es.id)
#' res = var.comp(m2)
#'
#' # Print results
#' res
#'
#' # Generate plot
#' plot(res)
mlm.variance.distribution = var.comp = function(x){
m = x
# Check class
if (!(class(m)[1] %in% c("rma.mv", "rma"))){
stop("x must be of class 'rma.mv'.")
}
# Check for three level model
if (m$sigma2s != 2){
stop("The model you provided does not seem to be a three-level model. This function can only be used for three-level models.")
}
# Check for right specification (nested model)
if (sum(grepl("/", as.character(m$random[[1]]))) < 1){
stop("Model must contain nested random effects. Did you use the '~ 1 | cluster/effect-within-cluster' notation in 'random'? See ?metafor::rma.mv for more details.")
}
# Get variance diagonal and calculate total variance
n = m$k.eff
vector.inv.var = 1/(diag(m$V))
sum.inv.var = sum(vector.inv.var)
sum.sq.inv.var = (sum.inv.var)^2
vector.inv.var.sq = 1/(diag(m$V)^2)
sum.inv.var.sq = sum(vector.inv.var.sq)
num = (n-1)*sum.inv.var
den = sum.sq.inv.var - sum.inv.var.sq
est.samp.var = num/den
# Calculate variance proportions
level1=((est.samp.var)/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
level2=((m$sigma2[2])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
level3=((m$sigma2[1])/(m$sigma2[1]+m$sigma2[2]+est.samp.var)*100)
# Prepare df for return
Level=c("Level 1", "Level 2", "Level 3")
Variance=c(level1, level2, level3)
df.res=data.frame(Variance)
colnames(df.res) = c("% of total variance")
rownames(df.res) = Level
I2 = c("---", round(Variance[2:3], 2))
df.res = as.data.frame(cbind(df.res, I2))
totalI2 = Variance[2] + Variance[3]
# Generate plot
df1 = data.frame("Level" = c("Sampling Error", "Total Heterogeneity"),
"Variance" = c(df.res[1,1], df.res[2,1]+df.res[3,1]),
"Type" = rep(1,2))
df2 = data.frame("Level" = rownames(df.res),
"Variance" = df.res[,1],
"Type" = rep(2,3))
df = as.data.frame(rbind(df1, df2))
g = ggplot(df, aes(fill=Level, y=Variance, x=as.factor(Type))) +
coord_cartesian(ylim = c(0,1), clip = "off") +
geom_bar(stat="identity", position="fill", width = 1, color="black") +
scale_y_continuous(labels = scales::percent)+
theme(axis.title.x=element_blank(),
axis.text.y = element_text(color="black"),
axis.line.y = element_blank(),
axis.title.y=element_blank(),
axis.line.x = element_blank(),
axis.ticks.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.y = element_line(lineend = "round"),
legend.position = "none",
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
legend.background = element_rect(linetype="solid",
colour ="black"),
legend.title = element_blank(),
legend.key.size = unit(0.75,"cm"),
axis.ticks.length=unit(.25, "cm"),
plot.margin = unit(c(1,3,1,1), "lines")) +
scale_fill_manual(values = c("darkseagreen3", "deepskyblue3", "darkseagreen2",
"deepskyblue1", "deepskyblue2")) +
# Add Annotation
# Total Variance
annotate("text", x = 1.5, y = 1.05,
label = paste("Total Variance:",
round(m$sigma2[1]+m$sigma2[2]+est.samp.var, 3))) +
# Sampling Error
annotate("text", x = 1, y = (df[1,2]/2+df[2,2])/100,
label = paste("Sampling Error Variance: \n", round(est.samp.var, 3)), size = 3) +
# Total I2
annotate("text", x = 1, y = ((df[2,2])/100)/2-0.02,
label = bquote("Total"~italic(I)^2*":"~.(round(df[2,2],2))*"%"), size = 3) +
annotate("text", x = 1, y = ((df[2,2])/100)/2+0.05,
label = paste("Variance not attributable \n to sampling error: \n", round(m$sigma2[1]+m$sigma2[2],3)), size = 3) +
# Level 1
annotate("text", x = 2, y = (df[1,2]/2+df[2,2])/100, label = paste("Level 1: \n",
round(df$Variance[3],2), "%", sep=""), size = 3) +
# Level 2
annotate("text", x = 2, y = (df[5,2]+(df[4,2]/2))/100,
label = bquote(italic(I)[Level2]^2*":"~.(round(df[4,2],2))*"%"), size = 3) +
# Level 3
annotate("text", x = 2, y = (df[5,2]/2)/100,
label = bquote(italic(I)[Level3]^2*":"~.(round(df[5,2],2))*"%"), size = 3)
returnlist = list(results = df.res,
totalI2 = totalI2,
plot = g)
class(returnlist) = c("mlm.variance.distribution", "list")
invisible(returnlist)
returnlist
}
overall %>% var.comp() %>% summary()
overall %>% var.comp() %>% summary()
i2 <- var.comp(overall)
i2
i2 %>% summary()
summary(i2)
rma.mv(yi = es,
V = es.var,
data = DF,
mods = ~ student,
random = ~ 1 | id / es.id)
DF$online %>% class()
DF$online %>% levels()
rma.mv(yi = es,
V = es.var,
data = DF,
mods = ~ online,
random = ~ 1 | id / es.id)
DF <- DF %>%
mutate(ref.r = if_else(ref.type == "cvp" |
ref.type == "nvc" |
ref.type == "cvz",
"single",
"double"),
ref.r = factor(ref.r))
rma.mv(yi = es,
V = es.var,
data = DF,
mods = ~ ref.r,
random = ~ 1 | id / es.id)
V <- with(DF,
impute_covariance_matrix(vi = es.var,
cluster = id,
r = .6))
library(clubSandwich)
V <- with(DF,
impute_covariance_matrix(vi = es.var,
cluster = id,
r = .6))
overall <- rma.mv(yi = es,
V = V,
data = DF,
random = ~ 1 | id / es.id,
sparse = T)
overall
i2 <- var.comp(overall)
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ student,
random = ~ 1 | id / es.id)
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ online,
random = ~ 1 | id / es.id)
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ ref.r,
random = ~ 1 | id / es.id)
?impute_covariance_matrix
V <- with(DF,
impute_covariance_matrix(vi = es.var,
cluster = id,
r = .1))
overall <- rma.mv(yi = es,
V = V,
data = DF,
random = ~ 1 | id / es.id,
sparse = T)
overall
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ student,
random = ~ 1 | id / es.id)
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ online,
random = ~ 1 | id / es.id)
rma.mv(yi = es,
V = V,
data = DF,
mods = ~ ref.r,
random = ~ 1 | id / es.id)
