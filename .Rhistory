data = DF.es,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = DF.es$id)
# return as list
return(list(mod = mod.m,
sub = sub.m))
}
)
View(mod.r)
lapply
?lapply
mod.r <-
sapply(X = mod.l,
FUN = function(m) {
# moderator analysis
mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = as.formula(paste0("~ ", m)),
method = "REML") %>%
robust(cluster = DF.es$id)
# subgroup analysis
sub.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = DF.es$id)
# return as list
return(list(mod = mod.m,
sub = sub.m))
}
)
View(mod.r)
View(mod.r)
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = function(m) {
# moderator analysis
mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = as.formula(paste0("~ ", m)),
method = "REML") %>%
robust(cluster = DF.es$id)
# subgroup analysis
sub.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = DF.es$id)
# return as list
return(list(mod = mod.m,
sub = sub.m))
}
)
View(mod.r)
mod.r$year$sub
mod.r$year$mod
mod.r$design$sub
mod.r$design$sub["designbetween"]
mod.r$design$sub["designbetween", ]
mod.r$design$sub$b["designbetween", ]
mod.r[["year"]][["sub"]][["ci.lb"]]
mod.r[["design"]][["sub"]][["ci.lb"]]
mod.r[["design"]][["sub"]]
mod.r[["design"]][["sub"]][["ci.lb"]]
mod.r[["design"]][["sub"]][["ci.lb"]][1]
mod.r$design$sub$b[1]
mod.r$design$sub$ci.lb[1]
mod.r[["design"]][["sub"]][["pval"]]
mod.r$design$mod
mod.r[["design"]][["mod"]][["QM"]]
mod.r$design$mod$QM
mod.r[["design"]][["mod"]][["QMdf"]]
mod.r[["design"]][["mod"]][["QMdf"]][1]
mod.r$published
mod.r$published$sub$b[1]
mod.r$published$sub$ci.lb[1]
mod.r$published$sub$ci.ub[1]
printp(mod.r$published$sub$pval[1])
printp(mod.r$published$sub$pval[2])
mod.r$published$mod$QMdf[1]
mod.r$published$mod
mod.r$published$mod$QMdf[2]
mod.r$published$mod$QM
printp(mod.r$published$mod$QMp)
mod.l
mod.r$published
mod.r$year
mod.r$year %>% View()
.[["mod"]][["b"]][2]
mod.r$year$mod$b
mod.r$year$mod$b["year", ]
mod.r$year$mod$tval
.[["mod"]][["zval"]]
mod.r$year$mod$zval
mod.r$year$mod$zval[2]
mod.r$year$mod$QMdf
mod.r$year$mod$QMdf %>% sum
mod.r$year$mod$pval
mod.r$year$mod
View(mod.r)
mod.r$year$ci.lb[2]
mod.r$year$mod$ci.lb[2]
mod.r$year$mod$pval
mod.r$student
mod.r$student$mod$QMdf[1]
r mod.r$design$mod$QMdf[2]
mod.r$design$mod$QMdf[2]
mod.r$design$mod
mod.r$design$mod$QMdf[2]
mod.r$student$mod
mod.r$student$mod$QMdf[2]
mod.r$student$mod$QM
mod.r$student$mod$QMp
mod.r$student$sub
mod.r$student$sub$b[1]
mod.r$student$sub$ci.lb[1]
mod.r$student$sub$ci.ub[1]
mod.r$student$sub$pval[1]
mod.r$online
mod.r$online$sub$b[1]
mod.r$paid
mod.r$ref.r
mod.r$ref.r$sub
mod.r$ref.r$sub$b[1]
DF.es$ref.type
DF.tmp <- DF.es %>%
filter(ref.r == "single") %>%
mutate(ref.type = factor(as.character(ref.type)))
DF.es$ref.r %>% table()
mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = ~ ref.type),
method = "REML")
mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
mods = ~ ref.type,
method = "REML")
mod.m
DF.tmp$ref.type %>% table
mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.tmp,
mods = ~ ref.type,
method = "REML")
mod.m
sub.m <- mod.m <- rma.uni(yi = es,
vi = es.var,
data = DF.tmp,
mods = ~ 0 + ref.type,
method = "REML")
sub.m
source("~/.active-rstudio-document", encoding = 'UTF-8', echo=TRUE)
rm(list = ls())
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation = if_else(grepl("dissertation", link),
1,
0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 4: final.df
# open effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# specify number of studies (denoted by id column)
num.s <- DF.es$id %>%
unique() %>%
length()
# specify number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# identify outlier es
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
# Chunk 5: clean.env.1
# remove outlier
DF.es <- DF.es %>%
filter(id != 18)
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# create list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es as a number
mod$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as numeric and turn into percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: vig.desc
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>%
nrow()
# Chunk 9: vig
knitr::include_graphics("images/metaware_vigs.png")
# Chunk 10: survey.n
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>%
nrow()
# Chunk 11: mods
knitr::include_graphics("images/metaware_mods.png")
# Chunk 12: clean.env.2
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
# Chunk 13: overall
# estimate overall effect size
overall <-
rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# create list of moderators
mod.l <- c("student", "paid", "online",
"design", "ref.type", "ref.r", "published",
"year")
# create moderator analysis function
ModAnalysis = function(df = DF.es, m) {
# set dataset
df = df
# moderator analysis
mod.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ ", m)),
method = "REML") %>%
robust(cluster = df$id)
# subgroup analysis
sub.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = df$id)
# return results as list
return(list(mod = mod.m,
sub = sub.m))
}
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis)
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis(m = x))
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis(m = X))
ModAnalysis = function(m, df = DF.es) {
# set dataset
df = df
# moderator analysis
mod.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ ", m)),
method = "REML") %>%
robust(cluster = df$id)
# subgroup analysis
sub.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = df$id)
# return results as list
return(list(mod = mod.m,
sub = sub.m))
}
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis)
# create list of moderators
mod.l <- c("student", "paid", "online",
"design", "ref.r", "published",
"year")
# create moderator analysis function
ModAnalysis = function(m, df = DF.es) {
# set dataset
df = df
# moderator analysis
mod.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ ", m)),
method = "REML") %>%
robust(cluster = df$id)
# subgroup analysis
sub.m <- rma.uni(yi = es,
vi = es.var,
data = df,
mods = as.formula(paste0("~ 0 + ", m)),
method = "REML") %>%
robust(cluster = df$id)
# return results as list
return(list(mod = mod.m,
sub = sub.m))
}
# conduct moderator and subgroup analyses for moderators assessed with full dataset
mod.r <-
sapply(X = mod.l,
simplify = F,
FUN = ModAnalysis)
DF[ref.r == "single", ]
DF.es[ref.r == "single", ]
DF.es[DF.es$ref.r == "single", ]
DF.es["ref.r" == "single", ]
DF.es[[ref.r == "single", ]]
DF.es[DF.es$ref.r == "single", ]
ref.type <- ModAnalysis(m = "ref.type",
df = DF.es[DF.es$ref.r == "single", ])
View(ref.type)
mod.r[ref.type] <- ModAnalysis(m = "ref.type",
df = DF.es[DF.es$ref.r == "single", ])
mod.r[[ref.type]] <- ModAnalysis(m = "ref.type",
df = DF.es[DF.es$ref.r == "single", ])
mod.r[["ref.type"]] <- ModAnalysis(m = "ref.type",
df = DF.es[DF.es$ref.r == "single", ])
View(mod.r)
mod.r$ref.type
mod.r$ref.type$mod
mod.r$ref.type$sub
mod.l
DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(test = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
getwd()
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(test = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening")
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(test = grepl("dissertation", link),
true = 1,
false = 0)
)
?if_else
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# identify total number of studies (denoted by id column)
num.s <- DF.es$id %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
outlier.es
