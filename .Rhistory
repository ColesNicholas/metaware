# Chunk 1
title             : "A quantitative review of demand characteristics and their underlying mechanisms"
shorttitle        : "Demand characteristics quantitative review"
author:
- name          : "Nicholas A. Coles"
affiliation   : "1"
corresponding : yes
address       : "Cordura Hall, 210 Panama St, Stanford, CA 94305"
email         : "ncoles@stanford.edu"
- name          : "Michael C. Frank"
affiliation   : "2"
affiliation:
- id            : "1"
institution   : "Center for the Study of Language and Information, Stanford University"
- id            : "2"
institution   : "Department of Psychology, Stanford University"
authornote: |
All materials, data, code, and pre-registrations (for Studies 1 and 2) are openly available at https://osf.io/3hkre/. This work was supported by the John Templeton Foundation (grant # 62295). The funder had no role in the preparation of the manuscript or decision to publish. We thank Morgan H. Wyatt for his assistance with the meta-analysis and code review. We also thank Anjie Cao for assistance with code review.
abstract: |
Demand characteristics are a fundamental methodological concern in experimental psychology. Yet, little is known about the direction, magnitude, consistency, and mechanisms underlying their effects. We conducted a meta-analysis of 195 effect sizes from 40 studies that provided strict experimental tests of demand effects by manipulating the hypothesis communicated to participants. Results indicated that demand characteristics tend to produce small increases in hypothesis-consistent responding (*d* = 0.22, 95% CI [0.11, 0.33]). However, these effects were extremely heterogeneous (between-study $\tau$ = 0.30; within-study $\sigma$ = 0.20), with the estimated distribution of true effects ranging from *d* = 2.17 (a large increase in hypothesis-consistent responding) to *d* = -1.44 (a large increase in hypothesis-*in*consistent responding). Contra prior frameworks, we did not find evidence that demand effects were driven by participants’ motivation or opportunity to adjust their responses. We did, however, find robust evidence that such effects are driven by participants’ beliefs, as in placebo effects. Similar findings emerged in a direct replication of a study included in the meta-analysis. Taken together, results challenge conventional distinctions between demand characteristics and placebo effects. Furthermore, they underscore the importance of controlling for both response bias and placebo effects when estimating causal relationships.
keywords          : "demand characteristics, placebo, mindsets, research methods, meta-analysis"
bibliography      : "r-references.bib"
floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options:
chunk_output_type: console
# Chunk 2: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of Nick's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 4: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 5: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 6: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 7: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 8: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 9: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 10: clean.env.2
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
overall
dist.min <- rnorm(n = 1000000,
mean = overall$b,
sd = tau)
# estimate overall effect size
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])
# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
mean = overall$b,
sd = tau,
lower.tail = F) %>%
round(digits = 2) * 100
h.i <- pnorm(q = (-.10),
mean = overall$b,
sd = tau,
lower.tail = T) %>%
round(digits = 2) * 100
# estimate lower and upper bound of effect size distribution
dist <- rnorm(n = 1000000,
mean = overall$b,
sd = tau)
dist %>%
min()
dist <- rnorm(n = 1000000,
mean = overall$b,
sd = tau)
dist %>%
min() %>%
round(digits = 2)
rnorm(n = 1000000,
mean = overall$b,
sd = tau)
dist %>%
min() %>%
round(digits = 2)
# Chunk 1
title             : "A quantitative review of demand characteristics and their underlying mechanisms"
shorttitle        : "Demand characteristics quantitative review"
author:
- name          : "Nicholas A. Coles"
affiliation   : "1"
corresponding : yes
address       : "Cordura Hall, 210 Panama St, Stanford, CA 94305"
email         : "ncoles@stanford.edu"
- name          : "Michael C. Frank"
affiliation   : "2"
affiliation:
- id            : "1"
institution   : "Center for the Study of Language and Information, Stanford University"
- id            : "2"
institution   : "Department of Psychology, Stanford University"
authornote: |
All materials, data, code, and pre-registrations (for Studies 1 and 2) are openly available at https://osf.io/3hkre/. This work was supported by the John Templeton Foundation (grant # 62295). The funder had no role in the preparation of the manuscript or decision to publish. We thank Morgan H. Wyatt for his assistance with the meta-analysis and code review. We also thank Anjie Cao for assistance with code review.
abstract: |
Demand characteristics are a fundamental methodological concern in experimental psychology. Yet, little is known about the direction, magnitude, consistency, and mechanisms underlying their effects. We conducted a meta-analysis of 195 effect sizes from 40 studies that provided strict experimental tests of demand effects by manipulating the hypothesis communicated to participants. Results indicated that demand characteristics tend to produce small increases in hypothesis-consistent responding (*d* = 0.22, 95% CI [0.11, 0.33]). However, these effects were extremely heterogeneous (between-study $\tau$ = 0.30; within-study $\sigma$ = 0.20), with the estimated distribution of true effects ranging from *d* = 2.17 (a large increase in hypothesis-consistent responding) to *d* = -1.44 (a large increase in hypothesis-*in*consistent responding). Contra prior frameworks, we did not find evidence that demand effects were driven by participants’ motivation or opportunity to adjust their responses. We did, however, find robust evidence that such effects are driven by participants’ beliefs, as in placebo effects. Similar findings emerged in a direct replication of a study included in the meta-analysis. Taken together, results challenge conventional distinctions between demand characteristics and placebo effects. Furthermore, they underscore the importance of controlling for both response bias and placebo effects when estimating causal relationships.
keywords          : "demand characteristics, placebo, mindsets, research methods, meta-analysis"
bibliography      : "r-references.bib"
floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
classoption       : "man"
output            : papaja::apa6_docx
editor_options:
chunk_output_type: console
# Chunk 2: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of Nick's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 4: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 5: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 6: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 7: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 8: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 9: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 10: clean.env.2
# delete vestigial
rm(mult.eff.per, sens.range)
# estimate overall effect size
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])
# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
mean = overall$b,
sd = tau,
lower.tail = F) %>%
round(digits = 2) * 100
h.i <- pnorm(q = (-.10),
mean = overall$b,
sd = tau,
lower.tail = T) %>%
round(digits = 2) * 100
# estimate lower and upper bound of effect size distribution
dist <- rnorm(n = 1000000,
mean = overall$b,
sd = tau)
dist.min <- dist %>%
min() %>%
round(digits = 2)
dist.max <- dist %>%
max() %>%
round(digits = 2)
rm(dist)
dist.min
dist.max
1.97-.22
1.44+.22
dist.min
