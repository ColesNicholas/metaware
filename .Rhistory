mods = ~ sqrt(vi),
data = DF.agg,
method = "REML")
# 3c. Weight-function model
########################
weight.funct <- weightfunct(effect = DF.agg$yi,
v = DF.agg$vi,
mods = NULL,
weights= NULL,
fe = FALSE,
table = TRUE,
pval = NULL)
# 4a. funnel plot
########################
par(mfrow=c(1,2))
rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
metafor::funnel(hlines = "lightgray",
xlab = "Cohen's standardized d")
# 4b. funnel plot w/ aggregated dependencies
########################
rma.uni(yi = yi,
vi = vi,
data = DF.agg,
method = "REML") %>%
metafor::funnel(hlines = "lightgray",
xlab = "Cohen's standardized d (aggregated)")
# save funnel plot as object
funnel.plot <- recordPlot()
# clear R environment
plot.new()
# 5. Organize results in list
########################
list(sens = sens,
pe.3l = pe.3l,
DF.agg = DF.agg,
pe.a = pe.a,
weight.funct = weight.funct,
funnel = funnel.plot) %>%
return()
}
# for range of rho values, run publication bias analyses
rho.l = seq(from = .1,
to = .9,
by = .2)
pub.r <- lapply(X = rho.l,
FUN = PubBias)
names(pub.r) = paste0("rho_", rho.l) #  name list
# delete vestigial
rm(rho.l, PubBias)
# look at sensitivity analyses
## general story: often, but not always, find evidence of reverse publication bias (preference for negative effects)
# lapply(pub.r, function(x){x[["pe.a"]]})
# lapply(pub.r, function(x){x[["peese"]]})
# lapply(pub.r, function(x){x[["weight.funct"]]})
# lapply(pub.r, function(x){x[["funnel"]]})
# plot funnels
# overall %>%
#   metafor::funnel(x = .,
#                   hlines = "lightgray",
#                   xlab = "Cohen's standardized d")
#
# pub.r$rho_0.5$pe.3l$b[2]
#
# pub.r$rho_0.5$weight.funct %>% View()
# Chunk 14: funnel
##########
# Funnel plot with non-aggregated dependencies
##########
# create a temporary dataset with standard error (se) values
tmp <- DF.es %>%
rowwise() %>%
mutate(se = sqrt(es.var)) %>%
ungroup()
# create temporary sequence of ses
se.seq = seq(0, max(tmp$se),
length.out = nrow(DF.es))
ll95 = overall$b[1] - (1.96 * se.seq)
ul95 = overall$b[1] + (1.96 * se.seq)
# create coordinates for polygon
t.coord <- rbind(cbind(x = overall$b[1],
y = 0),
cbind(x = min(ll95),
y = max(tmp$se)),
cbind(x = max(ul95),
y = max(tmp$se))
) %>%
as.data.frame()
# plot
a <- ggplot(data = tmp,
aes(x = es,
y = se)) +
geom_polygon(data = t.coord,
aes(x = x,
y = y),
fill = "#3366FF",
alpha = .1) +
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
scale_y_reverse() +
geom_vline(xintercept = overall$b[1],
linetype = "dotted") +
labs(x = "Cohen's d",
y = "Standard error")
# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord)
##########
# Funnel plot with aggregated dependencies
##########
# create a temporary dataset with standard error (se) values
tmp <- pub.r$rho_0.5$DF.agg %>%
rowwise() %>%
mutate(es = yi,
se = sqrt(vi)) %>%
ungroup()
# calculate overall effect size
tmp.meta <- rma.uni(yi = yi,
vi = vi,
data = tmp,
method = "REML")
# create temporary sequence of ses
se.seq = seq(0, max(tmp$se),
length.out = nrow(tmp))
ll95 = tmp.meta$b[1] - (1.96 * se.seq)
ul95 = tmp.meta$b[1] + (1.96 * se.seq)
# create coordinates for polygon
t.coord <- rbind(cbind(x = tmp.meta$b[1],
y = 0),
cbind(x = min(ll95),
y = max(tmp$se)),
cbind(x = max(ul95),
y = max(tmp$se))
) %>%
as.data.frame()
b <- ggplot(data = tmp,
aes(x = es,
y = se)) +
geom_polygon(data = t.coord,
aes(x = x,
y = y),
fill = "#3366FF",
alpha = .1) +
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
scale_y_reverse() +
geom_vline(xintercept = tmp.meta$b[1],
linetype = "dotted") +
labs(x = "Cohen's d",
y = "Standard error")
# delete vestigial
rm(tmp, ll95, ul95, se.seq, t.coord)
##########
# Plot funnels next to each other plot with aggregated dependencies
##########
plot_grid(a, b,
labels = c("A", "B"))
rm(a, b)
# Chunk 15: vig.desc
# identify total number of vignettes
vig.n <- read.csv(file = "vig/metaware_VigCombined.csv") %>%
nrow()
# Chunk 16: vig
knitr::include_graphics("images/metaware_vigs.png")
# Chunk 17: survey.n
# identify number of participants who completed the survey
survey.n <- read.csv("data/metaware_SurvData_raw.csv") %>%
nrow()
# Chunk 18: mods
knitr::include_graphics("images/metaware_mods.png")
# Chunk 19: mod.fig
#################
# motivation plot
#################
# get predicted and actual values into a single dataset
mot.df <- predict(mod.r$mot$mod,
addx = T) %>%
as.data.frame() %>%
cbind(.,
yi = mod.r$mot$mod$yi)
# plot
m <- ggplot(data = mot.df,
aes(x = X.mot,
y = yi)) +
# jittered raw data
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
# model derived prediction line
geom_line(aes(y = pred)) +
# model derived CI
geom_ribbon(aes(ymin = ci.lb,
ymax = ci.ub),
alpha = .10,
fill = "#3366FF") +
# adjust labels
labs(y = "Cohen's d",
x = "motivation ratings")
rm(mot.df)
#################
# opportunity plot
#################
# get predicted and actual values into a single dataset
opp.df <- predict(mod.r$opp$mod,
addx = T) %>%
as.data.frame() %>%
cbind(.,
yi = mod.r$opp$mod$yi)
# plot
o <- ggplot(data = opp.df,
aes(x = X.opp,
y = yi)) +
# jittered raw data
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
# model derived prediction line
geom_line(aes(y = pred)) +
# model derived CI
geom_ribbon(aes(ymin = ci.lb,
ymax = ci.ub),
alpha = .10,
fill = "#3366FF") +
# adjust labels
labs(y = "",
x = "opportunity ratings")
rm(opp.df)
#################
# belief plot
#################
# get predicted and actual values into a single dataset
bel.df <- predict(mod.r$bel$mod,
addx = T) %>%
as.data.frame() %>%
cbind(.,
yi = mod.r$bel$mod$yi)
# plot
b <- ggplot(data = bel.df,
aes(x = X.bel,
y = yi)) +
# jittered raw data
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
# model derived prediction line
geom_line(aes(y = pred)) +
# model derived CI
geom_ribbon(aes(ymin = ci.lb,
ymax = ci.ub),
alpha = .10,
fill = "#3366FF") +
# adjust labels
labs(y = "Cohen's d",
x = "belief ratings")
rm(bel.df)
#################
# prediction plot
#################
# get predicted and actual values into a single dataset
pre.df <- predict(mod.r$pre$mod,
addx = T) %>%
as.data.frame() %>%
cbind(.,
yi = mod.r$pre$mod$yi)
# plot
p <- ggplot(data = pre.df,
aes(x = X.pre,
y = yi)) +
# jittered raw data
geom_jitter(alpha = .8,
fill = "dark grey",
color = "dark grey") +
# model derived prediction line
geom_line(aes(y = pred)) +
# model derived CI
geom_ribbon(aes(ymin = ci.lb,
ymax = ci.ub),
alpha = .10,
fill = "#3366FF") +
# adjust labels
labs(y = "",
x = "prediction ratings")
rm(pre.df)
#################
# merge plots
#################
plot_grid(m, o, b, p,
labels = c("A", "B", "C", "D"))
rm(m, o, b, p)
rm(tmp, ll95, ul95, se.seq, t.coord, tmp.meta)
DF.es$year
DF.recent <- DF.es %>%
filter(year > 2012)
View(mod.r)
mod.year.sens <-
sapply(X = c("mot", "opp", "bel"),
simplify = F,
FUN = ModAnalysis,
df = DF.es %>%
filter(year > 2012 &
ref.type != "cvz" &
ref.type != "pvz"))
mod.year.sens$mot$mod
mod.year.sens$opp$mod
mod.year.sens$bel$mod
DF.recent <- DF.es %>%
filter(year > 2012)
mod.year.sens <-
sapply(X = c("mot", "opp", "bel"),
simplify = F,
FUN = ModAnalysis,
df = DF.es %>%
filter(year > 2013 &
ref.type != "cvz" &
ref.type != "pvz"))
mod.year.sens$mot$mod
mod.year.sens$opp$mod
mod.year.sens$bel$mod
rm(DF.recent)
mod.year.sens$mot$mod
mod.year.sens$opp$mod
mod.year.sens$bel$mod
# Chunk 1: setup
# load packages writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library(lme4)
library(lmerTest)
library(emmeans)
# identify refs
r_refs("r-references.bib")
# set theme
theme_set(theme_classic())
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation =
if_else(condition = grepl("dissertation", link),
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: clean.env.2
# delete vestigial
rm(mult.eff.per, vig.n, survey.n, sens.range)
# Chunk 9: overall
# estimate overall effect size
overall <-
rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# estimate standard deviation of effect size distribution (i.e., Tau)
# to do so, combine both sources of estimated variability in the model
tau <- sqrt(overall$sigma2[1] + overall$sigma2[2])
# estimate proportion of hypothesis-consistent and inconsistent responding
# -0.10 < d > 0.10 is the arbitrary threshold for saying it's neither consistent or inconsistent
h.c <- pnorm(q = .10,
mean = overall$b,
sd = tau,
lower.tail = F) %>%
round(2) * 100
h.i <- pnorm(q = (-.10),
mean = overall$b,
sd = tau,
lower.tail = T) %>%
round(2) * 100
# create plot of effect size distribution
ggplot(NULL,
aes(c(-1.2, 1.5))) +
# hypothesis inconsistent effects
## area
geom_area(stat = "function",
fun = dnorm,
args = list(mean = overall$b,
sd = tau),
fill = "#F8766D",
alpha = .5,
xlim = c(-1.2, -.10)) +
## label
geom_text(aes(label = paste0(h.i, "%")),
x = -.3,
y = .075,
size = 4) +
# negligible effects
## area
geom_area(stat = "function",
fun = dnorm,
args = list(mean = overall$b,
sd = tau),
fill = "grey80",
alpha = .5,
xlim = c(-.10, .10)) +
## label
geom_text(aes(label = paste0(100 - h.i - h.c, "%")),
x = -0,
y = .075,
size = 4) +
# hypothesis consistent effects
## area
geom_area(stat = "function",
fun = dnorm,
args = list(mean = overall$b,
sd = tau),
fill = "#00998a",
alpha = .5,
xlim = c(.10, 1.5)) +
## label
geom_text(aes(label = paste0(h.c, "%")),
x = .3,
y = .075,
size = 4) +
# aesthetics
labs(x = "Cohen's d") +
scale_y_continuous(expand = c(0, 0)) +
theme(axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank(),
axis.line.y = element_blank(),
)
