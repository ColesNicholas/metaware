install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
'readxl', 'robumeta', 'MAd', 'weightr',
'gtools')
ipak(packages)
rm(packages, ipak)
# turn scientific notation off
options(scipen = 999)
# do we need Type 3 SS?
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
# delete unnecessary variables
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note)) %>%
# convert factors to factors
mutate(id = as.factor(id),
published = as.factor(published),
student = as.factor(student),
paid = as.factor(paid),
online = as.factor(online),
ref.type = as.factor(ref.type))
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# record by year
tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 31 studies
DF$name %>% unique() %>% length()
# 222 effect sizes
nrow(DF)
DF %>%
filter(grepl("c", ref.type))
DF %>%
filter(grepl("c", ref.type)) %>%
#arrange(substr(vig.1, 4, 5)) %>%
View()
DF <- DF %>%
select(id, name,
ref.type, vig.1, vig.2)
# inspect
DF %>%
filter(grepl("c", ref.type)) %>%
#arrange(substr(vig.1, 4, 5)) %>%
View()
DF %>%
filter(grepl("c", ref.type)) %>%
arrange(substr(vig.1, 4, 5)) %>%
View()
DF %>%
filter(grepl("c", ref.type)) %>%
arrange(ref.type,
substr(vig.1, 4, 5)) %>%
View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1.1 = substr(vig.1, 4, 5)) %>%
arrange(ref.type,
vig.1.1) %>%
View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1.1 = substr(vig.1, 4, 4)) %>%
arrange(ref.type,
vig.1.1) %>%
View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1.1 = substr(vig.1, 4, 4)) %>%
arrange(ref.type,
vig.1.1) %>%
group_by(ref.type) %>%
count(vig.1.1)
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1.1 = substr(vig.1, 4, 4)) %>%
arrange(ref.type,
vig.1.1) %>%
group_by(ref.type) %>%
count(vig.1.1) %>% View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.1, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type) %>%
count(vig.1.b, vig.2b)
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.1, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type) %>%
count(vig.1b, vig.2b) %>% View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type) %>%
count(vig.1b, vig.2b) %>% View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type) %>%
table(vig.1b, vig.2b)
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type)
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally() %>% View()
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4))
DF %>%
filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>% View()
DF %>%
#filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally() %>% View()
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
'readxl', 'robumeta', 'MAd', 'weightr',
'gtools')
ipak(packages)
rm(packages, ipak)
# turn scientific notation off
options(scipen = 999)
# do we need Type 3 SS?
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
# delete unnecessary variables
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note)) %>%
# convert factors to factors
mutate(id = as.factor(id),
published = as.factor(published),
student = as.factor(student),
paid = as.factor(paid),
online = as.factor(online),
ref.type = as.factor(ref.type))
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# record by year
tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 31 studies
DF$name %>% unique() %>% length()
# 222 effect sizes
nrow(DF)
DF <- DF %>%
select(id, name,
ref.type, vig.1, vig.2)
# inspect
DF %>%
#filter(grepl("c", ref.type)) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4)) %>%
#arrange(ref.type,
#        vig.1b) %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally() %>% View()
DF %>%
filter(ref.type = 'nvc', vig.1 == 'p')
DF %>%
filter(ref.type == 'nvc', vig.1 == 'p')
DF %>%
filter(ref.type == 'nvc', vig.1b == 'p')
DF <- DF %>%
select(id, name,
ref.type, vig.1, vig.2) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4))
DF %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally() %>% View()
DF %>%
filter(ref.type == 'nvc', vig.1b == 'p')
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
'readxl', 'robumeta', 'MAd', 'weightr',
'gtools')
ipak(packages)
rm(packages, ipak)
# turn scientific notation off
options(scipen = 999)
# do we need Type 3 SS?
# Chunk 2: open/clean data
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
sheet = "coding",
na = c("NA"))
# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"
# delete unnecessary variables
DF <- DF %>%
# remove reference columns
select(-ends_with("ref")) %>%
# remove variables we won't include in any analyses
select(-c(dv, note)) %>%
# convert factors to factors
mutate(id = as.factor(id),
published = as.factor(published),
student = as.factor(student),
paid = as.factor(paid),
online = as.factor(online),
ref.type = as.factor(ref.type))
# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA
# record by year
tmp <- DF %>%
distinct(id,
.keep_all = T)
tmp$year %>%
hist(breaks = seq(from = 1960,
to = 2022,
by = 2))
rm(tmp)
# 31 studies
DF$name %>% unique() %>% length()
# 222 effect sizes
nrow(DF)
DF <- DF %>%
select(id, name,
ref.type, vig.1, vig.2) %>%
mutate(vig.1b = substr(vig.1, 4, 4),
vig.2b = substr(vig.2, 4, 4))
# inspect
DF %>%
group_by(ref.type, vig.1b, vig.2b) %>%
tally() %>% View()
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(path = "./vignettes",
pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(file = paste0("./vignettes/", f),
col.names = "V1",
encoding = "UTF-8") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 250) * 60 * 1000 # MW, you can adjust the 200
) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
rm(f, v)
# Chunk 5
h.DF <- data.frame()
# Chunk 6
for (f in f.list){
h <-
# open data
read.delim(file = paste0("./vignettes/", f),
col.names = "V1",
encoding = "UTF-8") %>%
# identify hypotheses using the ~ identifier
filter(grepl("~", V1)) %>%
# separate hypothesis from key
separate(V1,
into = c("name", "hypothesis"),
sep = 7)
# append results to h.DF
h.DF <<- rbind(h.DF, h)
}
rm(f, f.list, h)
# prep h.DF for merge
h.DF <- h.DF %>%
mutate(id = substr(x = name,
start = 2,
stop = 3),
dir = substr(x = name,
start = 5,
stop = 5)) %>%
select(-name) %>%
pivot_wider(names_from = dir,
values_from = hypothesis)
# prep v.DF for merge
v.DF <- v.DF %>%
mutate(id = substr(x = name,
start = 2,
stop = 3))
# merge
DF <- full_join(v.DF, h.DF,
by = "id")
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# load packages
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
# call package function
packages <- c('tidyverse')
ipak(packages)
rm(packages, ipak)
# Chunk 2
f.list <- list.files(path = "./vignettes",
pattern = ".txt")
# Chunk 3
v.DF <- data.frame()
# Chunk 4
for (f in f.list){
# open and process vignette file
v <-
# open data
read.delim(file = paste0("./vignettes/", f),
col.names = "V1",
encoding = "UTF-8") %>%
# remove non-vignette rows
slice(
# start at row after Vignette
(which(. == "Vignette") + 1) :
# stop at end of dataframe
nrow(.)
) %>%
# identify vignette number
mutate(V1 = if_else(grepl("#", .$V1),
substr(V1, 1, 9),
V1)
) %>%
# split at every new vignette
split(cumsum(grepl("#", .$V1))
) %>%
# create separate column for each vignette
map(c) %>%
bind_cols() %>%
# move first row to column names
set_names(as.character(slice(., 1))) %>%
slice(-1) %>%
# add sentence number column
mutate(sent = seq.int(nrow(.))) %>%
# pivot long
pivot_longer(cols = -sent) %>%
# estimate reading time in milliseconds (200 words per minute)
mutate(
# estimate # of words
word.num = str_count(value, "\\w+"),
# estimate reading time
rt = (word.num / 250) * 60 * 1000 # MW, you can adjust the 200
) %>%
select(-word.num) %>%
# pivot wider
pivot_wider(names_from = sent,
values_from = c(value, rt))
# append results to v.DF
v.DF <<- rbind(v.DF, v)
}
rm(f, v)
# Chunk 5
h.DF <- data.frame()
# Chunk 6
for (f in f.list){
h <-
# open data
read.delim(file = paste0("./vignettes/", f),
col.names = "V1",
encoding = "UTF-8") %>%
# identify hypotheses using the ~ identifier
filter(grepl("~", V1)) %>%
# separate hypothesis from key
separate(V1,
into = c("name", "hypothesis"),
sep = 7)
# append results to h.DF
h.DF <<- rbind(h.DF, h)
}
rm(f, f.list, h)
