mutate_all(~if_else(condition = grepl("#", .),
true = substr(x = .,
start = 2,
stop = 9),
false = .))
# append first row to column name
## append name
colnames(DF.surv) <- paste(sep = "##",
colnames(DF.surv),
as.character(unlist(DF.surv[1, ]))
)
## remove first row
DF.surv <- DF.surv[-1, ]
## fix ss variable naming
DF.surv <- DF.surv %>%
rename("ss" = "ss##1")
# prep dataframe for summary statistics calculation
DF.surv <- DF.surv %>%
# pivot longer
pivot_longer(cols = contains("##"),
names_to = c("var", "vig"),
names_sep = "##") %>%
# extract variable name
mutate(var = substr(x = var,
start = nchar(var) - 2,
stop = nchar(var))) %>%
# pivot wider
pivot_wider(names_from = var,
values_from = value) %>%
# convert columns to correct class
mutate_at(c("awr", "mot", "opp", "bel", "pre"),
as.numeric)
# Chunk 20
surv.sum <- DF.surv %>%
group_by(vig) %>%
summarise(m.mot = mean(mot, na.rm = T),
m.opp = mean(opp, na.rm = T),
m.bel = mean(bel, na.rm = T),
m.pre = mean(pre, na.rm = T)
) %>%
ungroup()
rm(DF.surv)
# Chunk 21
# connect summary data to ES dataframe
DF <- DF %>%
# rename vig.1 to vig to enable join
rename(vig = vig.1) %>%
# connect summary data to vig.1
left_join(x = .,
y = surv.sum,
by = "vig") %>%
# rename vig.1 summary columns
rename(v1.mot = m.mot,
v1.opp = m.opp,
v1.bel = m.bel,
v1.pre = m.pre) %>%
# rename vig.2 to vig to prep for join
rename(vig.1 = vig,
vig = vig.2) %>%
# connect summary data to vig.2
left_join(x = .,
y = surv.sum,
by = "vig") %>%
# rename vig.2 summary columns
rename(v2.mot = m.mot,
v2.opp = m.opp,
v2.bel = m.bel,
v2.pre = m.pre) %>%
ungroup() %>%
# remove vestigial
select(-c(vig.1, vig, v1.mot : v2.pre))
rm(surv.sum)
if(!exists("sens")){
write.csv(DF,
"data/metaware_data_clean.csv",
row.names = FALSE)
}
sens <- TRUE
sens <- T
<- ?seq
?seq
corr.list <- seq(from = .1,
to = .9,
by = .2)
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# set token indicating that this is a sensitivity analysis
sens <- T
# create list of correlations to examine
corr.list <- seq(from = .1,
to = .9,
by = .2)
# Chunk 2
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_EsProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
corr <- c
source("metaware_EsProcessing.Rmd",
local = knitr::knit_global())
}
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_EsProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
corr <- c
source(knitr::purl("metaware_EsProcessing.Rmd", quiet=TRUE),
local = knitr::knit_global())
}
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# set token indicating that this is a sensitivity analysis
sens <- T
# create list of correlations to examine
corr.list <- seq(from = .1,
to = .9,
by = .2)
# Chunk 2
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_EsProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
# set corr
corr <- c
# source script
source(knitr::purl("metaware_EsProcessing.Rmd", quiet=TRUE),
local = knitr::knit_global())
# export dataframe
write.csv(DF,
paste0("data/r_sensitivity/metaware_data_clean_r", c)
)
}
# Chunk 1: setup and load packages
# clean environment
rm(list = ls())
# set token indicating that this is a sensitivity analysis
sens <- T
# create list of correlations to examine
corr.list <- seq(from = .1,
to = .9,
by = .2)
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_EsProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
# set corr
corr <- c
# source script
source(knitr::purl("metaware_EsProcessing.Rmd", quiet=TRUE),
local = knitr::knit_global())
# export dataframe
write.csv(DF,
paste0("data/r_sensitivity/metaware_data_clean_r", c),
row.names = F
)
}
# for each correlation,
# (1) compute effect sizes by sourcing 'metaware_EsProcessing.Rmd'
# (2) save results in data/r_sensitivity
for (c in corr.list){
# set corr
corr <- c
# source script
source(knitr::purl("metaware_EsProcessing.Rmd", quiet=TRUE),
local = knitr::knit_global())
# export dataframe
write.csv(DF,
paste0("data/r_sensitivity/metaware_data_clean_r", c, ".csv"),
row.names = F
)
}
sens.df.list <- list.files(path = "./data/r_sensitivity")
?lapply
rm(list = ls())
# create list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("/data/r_sensitivity",
i)
}     )
)
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("/data/r_sensitivity",
i)
)
}
)
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity",
i)
)
}
)
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
}
)
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es
return(mod$b)
}
)
# examine how assumed repeated measures correlation impacts general pattern of results
# create list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model
sens.res <-
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es
return(mod$b)
}
)
max(sens.res)
sens.res
sens.res <-
lapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es as a number
mod$b %>%
as.numeric() %>%
return()
}
)
View(sens.res)
sens.range <- max(sens.res)
sens.res %>% unlist()
sens.res %>% unlist() %>% max()
sens.res %>% unlist %>% range
?lapply
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es as a number
mod$b %>%
as.numeric() %>%
return()
}
)
max(sens.res) - min(sens.res)
rm(sens.df.list, sens.res)
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation = if_else(grepl("dissertation", link),
1,
0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
# Chunk 4: final.df
# open effect size data
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
# specify number of studies (denoted by id column)
num.s <- DF.es$id %>%
unique() %>%
length()
# specify number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# identify outlier es
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es))
# Chunk 5: clean.env.1
# remove outlier
DF.es <- DF.es %>%
filter(id != 18)
# clean environment
rm(DF.s, r.pi, r.unp, num.s, num.p, outlier.es)
# examine how assumed repeated measures correlation impacts general pattern of results
# create list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
DF.es <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
mod <- rma.uni(yi = es,
vi = es.var,
data = DF.es,
method = "REML") %>%
robust(cluster = DF.es$id)
# return overall es as a number
mod$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
sens.range
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation = if_else(grepl("dissertation", link),
1,
0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
outlier.es
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es))
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation = if_else(grepl("dissertation", link),
1,
0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
F.es <-
read_csv(file = "data/metaware_data_clean.csv")
# specify number of studies (denoted by id column)
num.s <- DF.es$id %>%
unique() %>%
length()
# specify number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# identify outlier es
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es))
outlier.es
DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es))
DF.es %>%
filter(id == 18)
# Chunk 1: setup
# load necessary packages
library("papaja")
library("tidyverse")
library("readxl")
library("metafor")
#library("robumeta")
# identify refs
r_refs("r-references.bib")
# Chunk 2: framework
knitr::include_graphics("images/metaware_framework.png")
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify unpublished dissertations by identifying links that contain the word 'dissertation'
mutate(dissertation = if_else(grepl("dissertation", link),
1,
0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pi <- DF.s %>%
filter(!is.na(Database)) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(dissertation == 1) %>%
nrow()
DF.es <-
read_csv(file = "data/metaware_data_clean.csv")
num.s <- DF.es$id %>%
unique() %>%
length()
num.s
DF.es %>%
filter(id == 18)
DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es))
DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
outlier.es <- DF.es %>%
filter(id == 18) %>%
summarise(min.es = min(es)) %>%
round(2)
