write.csv(DF.surv2,
"data/metaware_replication_clean.csv",
row.names = F)
vig.dem <-
read_csv(file = "data/metaware_SurvData_raw.csv")
View(vig.dem)
?read.csv
DF.surv <- read_csv(file = "data/metaware_SurvData_raw.csv") %>%
# remove unnecessary variables
select(-c(StartDate : UserLanguage),
-c(hap1_bl1_hap : survey_order),
-contains("Click"),
-contains("Submit"),
-contains("Count")) %>%
# remove row containing ImportId
filter(!grepl("ImportId", `1_awr`)) %>%
# assign participant ids
mutate(ss = 1 : nrow(.),
ss = as.character(ss)) %>%
# select columns containing information from the vignettes
select(`1_awr` : `119_opp`, ss) %>%
# extract vignette identifier from the first row of data
# note: I warned you the data structure is awkward
mutate_all(~if_else(condition = grepl("#", .),
true = substr(x = .,
start = 2,
stop = 9),
false = .))
# append first row (containing vignette identifiers) to the column name
## append name
colnames(DF.surv) <- paste(sep = "##",
colnames(DF.surv),
as.character(unlist(DF.surv[1, ]))
)
## remove the first row
### now that the vignette identifier has been added to the column name, we don't need this row anymore
DF.surv <- DF.surv[-1, ]
## fix ss (subject identifier) variable naming
DF.surv <- DF.surv %>%
rename("ss" = "ss##1")
View(DF.surv)
# Chunk 1: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of lead author's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify
# unpublished dissertations by identifying links that contain the word 'dissertation' AND
# records identified manually by identify links that contain 'NA'
mutate(unpub =
if_else(condition = grepl("dissertation", link) |
link == "NA",
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pub <- DF.s %>%
filter(unpub == 0) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(unpub == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pub, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
rm(list = ls())
# Chunk 1: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of lead author's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify
# unpublished dissertations by identifying links that contain the word 'dissertation' AND
# records identified manually by identify links that contain 'NA'
mutate(unpub =
if_else(condition = grepl("dissertation", link) |
link == "NA",
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pub <- DF.s %>%
filter(unpub == 0) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(unpub == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pub, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: vig.desc
# identify total number of vignettes
vig.n <- read.csv(file = "admin/vig/metaware_VigCombined.csv") %>%
nrow()
# Chunk 9: vig.dem
vig.surv <-
read_csv(file = "data/metaware_SurvData_raw.csv")
vig.surv$Progress %>% unique
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv")
# remove rows containing unnecessary variable details
vig.surv1 <- vig.surv1[-(1 : 2), ]
vig.surv1 %>% head
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1)
View(vig.surv1)
vig.surv1 %>% names
View(vig.surv1)
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity)
vig.surv2 <-
read_csv(file = "data/metaware_SurvData2_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity)
vig.surv <- rbind(vig.surv1, vig.surv2)
```{r survey.dem, include = F}
```{r survey.dem, include = F}
# Chunk 1: setup
# load writing and data processing packages
library("papaja")
library("tidyverse")
library("readxl")
library("cowplot")
# load meta-analyses packages
library("metafor")
library("weightr")
library("PublicationBias")
# load mixed-effect regression packages
library("lme4")
library("lmerTest")
library("emmeans")
# identify paper references
r_refs("r-references.bib")
# turn scientific notation off
options(scipen = 999)
# set seed to year of lead author's favorite [unfinished] album, SMiLE
set.seed(1967)
# set theme
theme_set(theme_classic())
# Chunk 3: literature search
# open and process literature search data
DF.s <-
# open data
read_xlsx(path = "data/metaware_EsData_raw.xlsx",
sheet = "records.screening") %>%
# identify
# unpublished dissertations by identifying links that contain the word 'dissertation' AND
# records identified manually by identify links that contain 'NA'
mutate(unpub =
if_else(condition = grepl("dissertation", link) |
link == "NA",
true = 1,
false = 0)
)
# calculate number of records from PsycInfo by removing all records with no known database (i.e., ones that were personally found)
r.pub <- DF.s %>%
filter(unpub == 0) %>%
nrow()
# calculate number of unpublished records (i.e., dissertations)
r.unp <- DF.s %>%
filter(unpub == 1) %>%
nrow()
# Chunk 4: final.df
# open clean effect size data
DF.es <-
read_csv(file = "data/metaware_meta_clean.csv")
# identify total number of studies (denoted by id.study column)
num.s <- DF.es$id.study %>%
unique() %>%
length()
# identify total number of papers (denoted by name column)
num.p <- DF.es$name %>%
unique() %>%
length()
# for the known outlier (id = 18), give an example of the largest effect size
outlier.es <- DF.es %>%
filter(id.study == 18) %>%
summarise(max.es = min(es)) %>% #  using min because largest value is neg
round(2)
# Chunk 5: clean.env.1
# remove outlier and re-initialize id factors
DF.es <- DF.es %>%
filter(id.study != 18) %>%
mutate(id.study = factor(id.study),
id.es = factor(id.es))
# clean environment
rm(DF.s, r.pub, r.unp, num.s, num.p, outlier.es)
# Chunk 6: corr.sens
# examine how assumed repeated measures correlation impacts general pattern of results
# get list of sensitivity dataframes
sens.df.list <- list.files(path = "./data/r_sensitivity")
# (1) open dataframe, (2) compute intercept-only model, (3) extract overall es
sens.res <-
sapply(X = sens.df.list,
FUN = function(i){
# open data
df <- read.csv(paste0("data/r_sensitivity/",
i)
)
# fit model
m <- rma.mv(yi = es,
V = es.var,
data = DF.es,
random = ~ 1 | id.study / id.es)
# return overall es as a number
m$b %>%
as.numeric() %>%
return()
}
)
# compute range of es values
sens.range <- max(sens.res) - min(sens.res)
# delete vestigial
rm(sens.df.list, sens.res)
# Chunk 7: mult.eff
# calculate percentage of studies with multiple effect sizes
mult.eff.per <- DF.es %>%
# identify number of effect sizes for each study (id)
group_by(id.study) %>%
count() %>%
# code whether each study has more than one effect size
mutate(dep = if_else(condition = n > 1,
true = 1,
false = 0)
) %>%
# calculate proportion of studies with more than one effect size
ungroup() %>%
summarise(mult.eff = mean(dep)) %>%
# export as percentage
as.numeric() %>%
round(digits = 2) * 100
# Chunk 8: vig.desc
# identify total number of vignettes
vig.n <- read.csv(file = "admin/vig/metaware_VigCombined.csv") %>%
nrow()
# import data from first survey
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity)
vig.surv2 <-
read_csv(file = "data/metaware_SurvData2_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity)
vig.surv <- rbind(vig.surv1, vig.surv2)
rm(vig.surv1, vig.surv2)
# import data from first survey
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_1")
vig.surv2 <-
read_csv(file = "data/metaware_SurvData2_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_2")
vig.surv <- rbind(vig.surv1, vig.surv2)
rm(vig.surv1, vig.surv2)
vig.surv %>% filter(study == "study_1) %>% nrow()
""
)
# import data from first survey
vig.surv1 <-
read_csv(file = "data/metaware_SurvData_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_1")
vig.surv1.n = nrow(vig.surv1)
vig.surv2 <-
read_csv(file = "data/metaware_SurvData2_raw.csv") %>%
filter(Finished == 1) %>%
select(indiv_gend_var : ethnicity) %>%
mutate(survey = "survey_2")
vig.surv <- rbind(vig.surv1, vig.surv2)
rm(vig.surv1, vig.surv2)
vig.surv1.n
vig1.rel <- readRDS("output/vig.surv1.rel.rds
")
get.wd()
getwd()
vig1.rel <- readRDS("/output/vig.surv1.rel.rds
")
vig1.rel <- readRDS("/output/vig.surv1.rel.rds")
readRDS("/output/vig.surv1.rel.rds")
readRDS
?readRDS
readRDS("output/vig.surv1.rel.rds")
vig1.rel <- readRDS("output/vig.surv1.rel.rds
")
vig1.rel <- readRDS("output/vig.surv1.rel.rds")
View(vig1.rel)
vig1.rel$mot$ICC
vig1.rel$mot$ICC[1]
vig1.rel$bel
vig.surv %>% View()
View(vig.surv)
survey.n <- nrow(vig.surv)
survey.n
survey.gend <-
vig.surv$indiv_gend_var %>%
table() %>%
prop.table() %>%
round(2) * 100
survey.gend
survey.eth <-
vig.surv$ethnicity %>%
table() %>%
prop.table() %>%
round(2) * 100
survey.eth
survey.age.m <-
mean(survey.DF$indiv_agee_var,
na.rm = T) %>%
round(2)
survey.age.m <-
mean(vig.surv$indiv_agee_var,
na.rm = T) %>%
round(2)
survey.age.m
vig.surv$indiv_agee_var
vig.surv$indiv_agee_var %>% as.numeric()
survey.age.m <-
mean(vig.surv$indiv_agee_var %>% as.numeric(),
na.rm = T) %>%
round(2)
survey.age.m
survey.age.sd <-
sd(survey.DF$indiv_agee_var %>% as.numeric(),
na.rm = T) %>%
round(2)
survey.age.sd <-
sd(vig.surv$indiv_agee_var %>% as.numeric(),
na.rm = T) %>%
round(2)
survey.age.sd
survey.dem = list(n = survey.n,
gend = survey.gend,
ethnicity = survey.eth,
age.m = survey.age.m,
age.sd = survey.age.sd)
# remove vestigial
rm(survey.DF, survey.n, survey.gend,
survey.eth, survey.age.m, survey.age.sd)
rm(vig.surv, survey.n, survey.gend,
survey.eth, survey.age.m, survey.age.sd)
rm(vig.surv1, vig.surv2, vig.n)
View(survey.dem)
rm(survey.dem, vig.1.rel)
rm(survey.dem, vig1.rel)
