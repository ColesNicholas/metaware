---
title: Data analysis for "A meta-analysis of the effects of demand characteristics in non-clinical research"
author: "Nicholas A. Coles and Morgan Wyatt"
editor_options: 
  chunk_output_type: console
---
The code was written in `r R.version$version.string` using R Markdown (http://rmarkdown.rstudio.com/).

# Section 1: Data Prep
```{r setup and load packages, include = FALSE}
# clean environment
rm(list = ls())

# load packages
ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    
    sapply(pkg, require, character.only = TRUE)
}

# call package function
packages <- c('metafor', 'tidyverse', 'PublicationBias',
              'readxl', 'robumeta', 'MAd', 'weightr',
              'gtools')
ipak(packages)
rm(packages, ipak)

# turn scientific notation off
options(scipen = 999)

# do we need Type 3 SS?
```

## Open and clean data
```{r open/clean data, message = FALSE, warning = FALSE}
# import data
DF <- read_xlsx(path = "metaware_data_raw.xlsx",
                sheet = "coding",
                na = c("NA"))

# change ref.type pvc to cvp
DF[DF$ref.type == "pvc", ]$ref.type = "cvp"

# delete unnecessary variables
DF <- DF %>% 
  
  # remove reference columns
  select(-ends_with("ref")) %>% 
  
  # remove variables we won't include in any analyses
  select(-c(prop.woman, age, dv, note)) %>% 
  
  # convert factors to factors
  mutate(id = as.factor(id),
         published = as.factor(published),
         student = as.factor(student),
         paid = as.factor(paid),
         online = as.factor(online),
         ref.type = as.factor(ref.type))

# create blank columns for effect size and effect size variance
# Note: these pre-exisiting columns are necessary for the 
# Cohen's d functions (defined later) to work
DF$es <- NA
DF$es.var <- NA

# record by year
tmp <- DF %>% 
  distinct(id,
           .keep_all = T)

tmp$year %>% 
  hist(breaks = seq(from = 1960, 
                  to = 2022,
                  by = 2))

rm(tmp)

# 32 studies
DF$name %>% unique() %>% length()

# 228 effect sizes
nrow(DF)
```

# Calculate Cohen's *d* and variance
## For between-subjects data
### Functions for calculating *d* 
#### When *M*'s and *SD*'s are provided
```{r define function EsBetwMean}
# formula: Cooper, Hedges, & Valentine, 2009; p. 226
EsBetwMean <- function(n.1, m.1, sd.1, 
                       n.2, m.2, sd.2){
    sd.within <- sqrt((((n.1 - 1) * (sd.1^2)) +
                       ((n.2 - 1) * (sd.2^2))) /
                        (n.1 + n.2 - 2));
    
    es <- (m.1 - m.2) / sd.within;
    return(es)
}
```

#### When t-values are provided
```{r define function EsBetwTval}
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwTval <- function(n.1, n.2, tval){
    es <- tval * sqrt((n.1 + n.2) / 
                      (n.1 * n.2));
    return(es)
}
```

#### When F-values are provided
```{r define function EsBetwFval}
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwFval <- function(n.1, n.2, fval){
  es <- sqrt((fval * (n.1 + n.2)) / 
             (n.1 * n.2)); 
    return(es)
  }
```

#### When p-values are provided
```{r define function EsBetwPval}
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsBetwPval <- function(n.1, n.2, pval){
  # calculate the inverse of the cumulative distribution function of t
  t.inv <- qt(p = (pval / 2), 
              df = (n.1 + n.2 - 2),
              lower.tail = FALSE);
  
  es <- t.inv * sqrt((n.1 + n.2) /
                     (n.1 * n.2));
  return(es)
}
```

### Function for calculating variance of *d* with continuous data
```{r define function EsVarBetw}
# formula: Cooper, Hedges, & Valentine, 2009; p. 228
EsVarBetw <- function(n.1, n.2, es){       
   es.var <- ((n.1 + n.2) / (n.1 * n.2)) +
             ((es^2) / (2 * (n.1 + n.2)));
   return(es.var)
}
```

### Functions for calculating *d* and variance of *d* with count data
```{r define functions EsBetwCount and EsVarBetwCount}
# cohen's d
EsBetwCount <- function(n.1, n.2, count.1, count.2){
  # calculate odds ratio
  # formula: Borenstein et al. 2011; p. 36; Equation 5.8
  or <- (count.1 * (n.2 - count.2)) / 
        ((n.1 - count.1) * count.2);
  
  # calculate log odds ratio
  # formula: Borenstein et al. 2011; p. 36; Equation 5.9
  log.or = log(or);
  
  # convert log odds ratio to Cohen's d
  # formula: Borenstein et al. 2011; p . 47 Equation 7.1
  es <- log.or * (sqrt(3) / pi);
  return(es)
}

# variance of cohen's d
EsVarBetwCount <- function(n.1, n.2, count.1, count.2){
  # calculate log odds ratio variance
  # formula: Borenstein et al. 2011; p. 36; Equation 5.10
  log.or.var = (1 / count.1) +
               (1 / (n.2 - count.2)) +
               (1 / (n.1 - count.1)) +
               (1 / count.2);
  
  # convert log odds ratio variance to variance of Cohen's d
  # formula: Borenstein et al. 2011; p. 47 Equation 7.2
  es.var = log.or.var * (3 / pi^2);
  return(es.var)
}
```

### Call functions to calculate d and variance of *d*
```{r b: call functions to calculate d}
for (i in 1:nrow(DF)) {
  if (DF$design[i] == "between"){
    
    # call EsBetwMean on cases with between-subject designs and *means*
    if (DF$es.calc[i] == "m_sd") {
      DF$es[i] <- EsBetwMean(n.1 = DF$n.1[i],
                             m.1 = DF$m.1[i],
                             sd.1 = DF$sd.1[i],
                             n.2 = DF$n.2[i],
                             m.2 = DF$m.2[i],
                             sd.2 = DF$sd.2[i]) 
      } 
    # call EsBetwTval on cases with between-subject designs and *t-values*
    if (DF$es.calc[i] == "t") {
      DF$es[i] <- EsBetwTval(n.1 = DF$n.1[i],
                             n.2 = DF$n.2[i],
                             tval = DF$tval[i])
      }
    
    # call EsBetwFval on cases with between-subject designs and *F-values*
    if (DF$es.calc[i] == "f") {
      DF$es[i] <- EsBetwFval(n.1 = DF$n.1[i],
                             n.2 = DF$n.2[i],
                             fval = DF$fval[i])
    }
    
    # call EsBetwPval on cases with between-subject designs and *p-values*
    if (DF$es.calc[i] == "p") {
      DF$es[i] <- EsBetwPval(n.1 = DF$n.1[i],
                             n.2 = DF$n.2[i],
                             pval = DF$pval[i])
    }
    
    # call EsBetwCount and EsVarBetwCount on cases with between-subject designs and *count data*
    if (DF$es.calc[i] == "or") {
      DF$es[i] <- EsBetwCount(n.1 = DF$n.1[i],
                              n.2 = DF$n.2[i],
                              count.1 = DF$count.1[i],
                              count.2 = DF$count.2[i])
      
      DF$es.var[i] <- EsVarBetwCount(n.1 = DF$n.1[i],
                                     n.2 = DF$n.2[i],
                                     count.1 = DF$count.1[i],
                                     count.2 = DF$count.2[i])
    }
    
    # call EsVarBetw on cases with continuous data and a between subject designs
    if (DF$es.calc[i] != "or"){
      DF$es.var[i] <- EsVarBetw(n.1 = DF$n.1[i],
                                n.2 = DF$n.2[i],
                                es = DF$es[i])
    }
    }
}
```

## For within-subjects data
```{r assumed correlation}
# define assumed correlation (sensitivity analyses performed later)
corr <- .5
```

### Functions for calculating d
#### When M's and SD's are provided 
```{r EsWitnMean}
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
# formula for imputing sd.diff:
# http://handbook.cochrane.org/chapter_16/16_4_6_1_mean_differences.htm
EsWitnMean <- function(m.1, sd.1, m.2, sd.2, corr){
  sd.diff <- sqrt((sd.1^2) + (sd.2^2) -
                  (2 * corr * sd.1 * sd.2));
        
  es <- ((m.1 - m.2) / sd.diff) * sqrt(2 * (1- corr));
  return(es)
}
```

#### When t-values are provided
```{r EsWitnTval}
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnTval <- function(n.1, tval, corr){
  es <- tval * sqrt((2 * (1 - corr)) / n.1);
  return(es)
}
```

#### When F-values are provided
```{r EsWitnFval}
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsWitnFval <- function(n.1, fval, corr){
  es <- sqrt((2 * fval * (1- corr)) / n.1);
  return(es)
}
```

### Function for calculating variance of d
```{r EsVarWitn}
# formula: Cooper, Hedges, & Valentine, 2009; p. 229
EsVarWitn <- function(n.1, es){
  es.var <- ((1 / n.1) + 
             ((es^2) / (2 * n.1))) *
            2 * (1 - corr);
  return(es.var)
}
```

### Call functions to calculate d and variance of d
```{r w: call functions to calculate d}
for (i in 1:nrow(DF)) {
  if (DF$design[i] == "within"){
    
    # call EsWitnMean on cases with within-subject designs and *means*
    if(DF$es.calc[i] == "m_sd") {
      DF$es[i] <- EsWitnMean(m.1 = DF$m.1[i],
                             sd.1 = DF$sd.1[i],
                             m.2 = DF$m.2[i],
                             sd.2 = DF$sd.2[i],
                             corr = corr)
    }
    
    # call EsWitnTval on cases with within-subject designs and *t-values*
    if (DF$es.calc[i] == "t") {
      DF$es[i] <- EsWitnTval(n.1 = DF$n.1[i],
                             tval = DF$tval[i],
                             corr = corr)
  }  
    
    # call EsWitnFval on cases with within-subject designs and *F-values*
    if (DF$es.calc[i] == "f") {
      DF$es[i] <- EsWitnFval(n.1 = DF$n.1[i],
                             fval = DF$fval[i],
                             corr = corr)
    }
    
    # call EsVarWitn on cases with within subject designs
    DF$es.var[i] <- EsVarWitn(n.1 = DF$n.1[i],
                              es = DF$es[i])
    }
  }
  
```

```{r del var}
# delete unnecessary variables
rm(corr, i,
   EsBetwFval, EsBetwMean, EsBetwTval,
   EsBetwPval, EsVarBetw, EsVarWitn,
   EsWitnFval, EsWitnMean, EsWitnTval,
   EsBetwCount, EsVarBetwCount)
```

## Delete id 18 because it's a MASSIVE outlier
```{r}
DF <- DF %>% 
  filter(id != 18)
```

## Specify direction
This direction of the effect was coded for in the raw database, and this code ensures that all effects are in the correct direction.
```{r specify es direction}
DF <- DF %>% 
  # specify direction of the effect size
  rowwise() %>% 
  mutate(es = if_else(condition = direction == "positive",
                      true = abs(es),
                      false = abs(es) * -1)
         ) %>% 
  ungroup()
```

# Export
```{r}
write.csv(DF %>% 
            filter(!is.na(es)), 
          'metaware.data_clean.csv', 
          row.names = FALSE)
```

# Data analysis
## Overall effect
```{r}
# overall d= 0.28, 95% CI = [0.16, 0.39], T = 0.28
overall <- 
  robu(formula = es ~ 1,
       data = DF, 
       studynum = id,
       var.eff.size = es.var,
       modelweights = "HIER",
       small = FALSE)

# get imperfect estimate of I2: 88.50%
## RVE approach doesn't give an I2
rma.uni(yi = es,
        vi = es.var,
        data = DF,
        method = "REML")
```

## Moderator analyses
Prep analyses
```{r}
# create blank dataframe
mod.r <- data.frame(mod = character(),
                    level = character(),
                    s = integer(),
                    k = integer(),
                    d = integer(),
                    f = integer(),
                    LB = integer(),
                    UB = integer(),
                    p = integer())

# create list of moderators
mod.l <- c("student", "paid", "online", 
           "design", "ref.type", "published")
```

Create function for moderator analysis
```{r}
ModAnalysis <- function(mod){
  # fit model
  m <- rma.uni(yi = es,
               vi = es.var,
               data = DF,
               mods = as.formula(paste0("~ ", mod)),
               method = "REML") %>%
    robust(cluster = DF$id)
  
  # append to moderator analysis results dataframe
  mod.r <<- 
    rbind(mod.r,
          cbind(mod = mod,
                level = NA,
                s = m$n,
                k = m$k,
                d = NA,
                f = round(m$QM, 
                          2),
                LB = NA,
                UB = NA,
                p = round(m$QMp,
                          4))
          )
}
```

Create subgroup analysis function
```{r}
SubAnalysis <- function(df, level, mod){
  # fit subgroup model
  m <- rma.uni(yi = es,
               vi = es.var,
               data = df,
               method = "REML")
  
  # run cluster-robust analysis if there are enough observations
   if(nrow(df) > 1){ 
     m <- robust(m, cluster = df$id)
     }
  
  # append to moderator analysis results dataframe
  mod.r <<- 
    rbind(mod.r,
          cbind(mod = mod,
                level = level,
                s = ifelse(nrow(df) > 1,
                           m$n,
                           1),
                k = m$k,
                d = round(m$b[1],
                          2),
                f = NA,
                LB = round(m$ci.lb, 2),
                UB = round(m$ci.ub, 2),
                p = round(m$pval,
                          4))
          )
}
```

Run moderator and subgroup analyses
```{r}
# DF <- DF %>% 
#   filter(name != "Demand effects in survey experiments: An empirical assessment")

for (mod in mod.l){
  ModAnalysis(mod = mod)
  
  mapply(FUN = SubAnalysis,
         df = split(DF, DF[, mod]),
         level = names(split(DF, DF[, mod])),
         mod = mod)
}
```

## Publication bias analyses
Funnel plot
```{r}
rma.uni(yi = es,
        vi = es.var,
        data = DF,
        method = "REML") %>% 
  robust(cluster = DF$id) %>% 
  metafor::funnel(hlines = "lightgray",
                  xlab = "Cohen's standardized d")
```

PET-PEESE and 3PSM
```{r}
# aggregate dependent effect sizes
DF.AGG <- agg(id = id,
              es = es,
              var = es.var,
              method = "BHHR",
              cor = .5, 
              data = DF)

# aggregated funnel plot
rma.uni(yi = es,
        vi = var,
        data = DF.AGG,
        method = "REML") %>%
  metafor::funnel()
  
# PET-PEESE models
## syntax provided by Carter & McCullough, 2014
##  first compute standard error of es
DF.AGG$se <- sqrt(DF.AGG$var)

### create models
pet <-   lm(DF.AGG$es ~ DF.AGG$se, 
             weights = 1 / DF.AGG$var) %>% summary() #  B1 = 1.19, p = .04; B0 = 0.19, p = .75
peese <- lm(DF.AGG$es ~ DF.AGG$var, 
             weights = 1 / DF.AGG$var) %>% summary() #  B1 = 1.86, p = .31; B0 = 0.08, p = .10
  
# Vevea and Hedges (1995) Weight-Function Model 
## test of significant X2(1) = 2.54, p = 0.11
## bias corrected intercept = 0.38
weight.funct <- weightfunct(effect = DF.AGG$es,
                            v = DF.AGG$var,
                            mods = NULL,
                            weights= NULL,
                            fe = FALSE,
                            table = TRUE,
                            pval = NULL)

# Mathur and VanderWeele 2020 sensitivity analyses
# If eta = 14, effect size is no longer significant 
corrected_meta(yi = DF$es,
               vi = DF$es.var,
               eta = 49,
               clustervar = DF$id,
               model = "robust",
               favor.positive = T,
               )

significance_funnel(yi = DF$es,
                    vi = DF$es.var,
                    favor.positive = T,
                    xlab = "Cohen's standardized d")
```


# Subgroup analysis forest plot'
Prep dataframe
```{r}
# reorder moderator factor
mod.r <- mod.r %>% 
  
  # remove publication status moderator
  filter(mod != "published") %>% 
  
  # re-order moderators
  mutate(mod = factor(mod,
                      levels = c("student",
                                 "online",
                                 "paid",
                                 "design",
                                 "ref.type"))) %>% 
  
  # remove p-values for the levels
  mutate(p = if_else(is.na(level),
                     p,
                     ""))

# update labels
mod.r[!is.na(mod.r$level) &
        mod.r$level == "cvz", ]$level = "control v. nil"
mod.r[!is.na(mod.r$level) &
        mod.r$level == "nvc", ]$level = "control v. negative"
mod.r[!is.na(mod.r$level) &
        mod.r$level == "nvz", ]$level = "nil v. negative"
mod.r[!is.na(mod.r$level) &
        mod.r$level == "cvp", ]$level = "control v. positive"
mod.r[!is.na(mod.r$level) &
        mod.r$level == "pvn", ]$level = "positive v. negative"
mod.r[!is.na(mod.r$level) &
        mod.r$level == "pvz", ]$level = "positive v. nil"

tmp <- mod.r %>% 
  # add blank rows after each moderator
  group_split(mod) %>% 
  map_dfr(~ add_row(.x, .after = Inf)) %>% 
  ungroup() %>% 
  
  # add tab to the levels of the moderator
  mutate(Moderator = 
           if_else(is.na(level),
                   as.character(mod),
                   paste0("  ",
                          level))
         ) %>% 
  relocate(Moderator, .before = mod) %>%
  
  # place fake values (99s) for the moderator analysis rows
  mutate(d = if_else(is.na(d),
                     99, 
                     as.numeric(d)),
         LB = if_else(is.na(LB),
                      99,
                      as.numeric(LB)),
         UB = if_else(is.na(UB),
                      99,
                      as.numeric(UB)),
         p = if_else(as.numeric(p) > .05,
                     round(as.numeric(p), 2) %>% 
                       as.character(),
                     stars.pval(as.numeric(p))
                     )
         ) %>%
  # select relevant variables
  select(Moderator, s, k, d, LB, UB, p) %>% 
  
  # create row of column names
  add_row(Moderator = "Moderator",
          s = "s",
          k = "k",
          d = 99,
          LB = 99,
          UB = 99,
          p = "p-value",
          .before = 1) %>% 
  add_row(Moderator = NA,
          s = NA,
          k = NA,
          d = 99,
          LB = 99,
          UB = 99,
          p = NA,
          .after = 1)

# change name of ref.type
tmp[!is.na(tmp$Moderator) &
      tmp$Moderator == "ref.type", ]$Moderator = "demand comparison"

# remove extra row
tmp <- tmp[1 : (nrow(tmp) - 1), ]
```

Plot figure
```{r}
ggplot(data= tmp, 
       aes(y = rev(1: nrow(tmp)), 
           x = d,
           xmin = LB,
           xmax = UB)) +
  
  # add points and error bars
  geom_point(shape = "diamond",
             size = 3,
             colour = '#FFC000') +
  geom_errorbarh(height = .5,
                 colour = '#FFC000') +
  
  # add moderator label
  geom_text(aes(label = Moderator),
            x = -1.8,
            hjust = 0,
            fontface = c("bold",
                         rep(x= "plain",
                             times = nrow(tmp) - 1)
                         ),
                   ) +
  
  # add s label
  geom_text(aes(label = s),
            x = -1,
            hjust = 0,
            fontface = c("bold",
                         rep(x= "plain",
                             times = nrow(tmp) - 1)
                         ),
                   ) +
  
  # add k label
  geom_text(aes(label = k),
            x = -.75,
            hjust = 0,
            fontface = c("bold",
                         rep(x= "plain",
                             times = nrow(tmp) - 1)
                         ),
                   ) +
  
  # add p-value label
  geom_text(aes(label = p),
            x = 1.3,
            hjust = 0,
            fontface = c("bold",
                         rep(x= "plain",
                             times = nrow(tmp) - 1)
                         ),
                   ) +
  
  # limit view of plot
  coord_cartesian(xlim=c(-2.4, 2)) +
  
  # create line at 0
  geom_vline(xintercept = 0, 
             color = "black", 
             linetype = "dashed", 
             alpha = .5) +
  
  
  # clean up aesthetics 
  theme_classic() +
  scale_x_continuous(limits = c(-2, 2),
                     n.breaks = 10, 
                     labels = c(rep("", 4), 
                                seq(from = -.5, 
                                    to = 1,
                                    by = .5),
                                rep("", 2))) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.line.y = element_blank(),
        axis.ticks.x = element_blank()) +
  labs(x = "                                                  Cohen's standardized d")
```

